{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, ConcatDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trial1RegularRawSweep = pd.read_csv('./data/planar10regular_1-sweep.csv')\n",
    "trial1RegularRawAngle = pd.read_csv('./data/planar10regular_1-angles.csv')\n",
    "\n",
    "trial4RegularRawSweep = pd.read_csv('./data/planar10regular_4-sweep.csv')\n",
    "trial4RegularRawAngle = pd.read_csv('./data/planar10regular_4-angles.csv')\n",
    "\n",
    "trial7RegularRawSweep = pd.read_csv('./data/planar10regular_7-sweep.csv')\n",
    "trial7RegularRawAngle = pd.read_csv('./data/planar10regular_7-angles.csv')\n",
    "\n",
    "trial6SlowRawSweep = pd.read_csv('./data/10cmplanarSlow6-sweep.csv')\n",
    "trial6SlowRawAngle = pd.read_csv('./data/10cmplanarSlow6-angles.csv')\n",
    "\n",
    "trial8SlowRawSweep = pd.read_csv('./data/10cmplanarSlow8-sweep.csv')\n",
    "trial8SlowRawAngle = pd.read_csv('./data/10cmplanarSlow8-angles.csv')\n",
    "\n",
    "trial5SlowRawSweep = pd.read_csv('./data/10cmplanarSlow5-sweep.csv')\n",
    "trial5SlowRawAngle = pd.read_csv('./data/10cmplanarSlow5-angles.csv')\n",
    "\n",
    "trial3FastRawSweep = pd.read_csv('./data/planar10fast_3-sweep.csv')\n",
    "trial3FastRawAngle = pd.read_csv('./data/planar10fast_3-angles.csv')\n",
    "\n",
    "trial4FastRawSweep = pd.read_csv('./data/planar10fast_4-sweep.csv')\n",
    "trial4FastRawAngle = pd.read_csv('./data/planar10fast_4-angles.csv')\n",
    "\n",
    "trial1FastRawSweep = pd.read_csv('./data/planar10fast_1-sweep.csv')\n",
    "trial1FastRawAngle = pd.read_csv('./data/planar10fast_1-angles.csv')\n",
    "\n",
    "timeS21trial3fast = trial3FastRawSweep['time'].to_numpy()\n",
    "s21rawtrial3fast = trial3FastRawSweep['s21'].to_numpy()\n",
    "timeCamtrial3fast = trial3FastRawAngle['time'].to_numpy()\n",
    "angleCamtrial3fast = trial3FastRawAngle['angle_3d'].to_numpy()\n",
    "\n",
    "timeS21trial4fast = trial4FastRawSweep['time'].to_numpy()\n",
    "s21rawtrial4fast = trial4FastRawSweep['s21'].to_numpy()\n",
    "timeCamtrial4fast = trial4FastRawAngle['time'].to_numpy()\n",
    "angleCamtrial4fast = trial4FastRawAngle['angle_3d'].to_numpy()\n",
    "\n",
    "timeS21trial1fast = trial1FastRawSweep['time'].to_numpy()\n",
    "s21rawtrial1fast = trial1FastRawSweep['s21'].to_numpy()\n",
    "timeCamtrial1fast = trial1FastRawAngle['time'].to_numpy()\n",
    "angleCamtrial1fast = trial1FastRawAngle['angle_3d'].to_numpy()\n",
    "\n",
    "timeS21trial7regular = trial7RegularRawSweep['time'].to_numpy()\n",
    "s21rawtrial7regular = trial7RegularRawSweep['s21'].to_numpy()\n",
    "timeCamtrial7regular = trial7RegularRawAngle['time'].to_numpy()\n",
    "angleCamtrial7regular = trial7RegularRawAngle['angle_3d'].to_numpy()\n",
    "\n",
    "timeS21trial4regular = trial4RegularRawSweep['time'].to_numpy()\n",
    "s21rawtrial4regular = trial4RegularRawSweep['s21'].to_numpy()\n",
    "timeCamtrial4regular = trial4RegularRawAngle['time'].to_numpy()\n",
    "angleCamtrial4regular = trial4RegularRawAngle['angle_3d'].to_numpy()\n",
    "\n",
    "timeS21trial6Slow = trial6SlowRawSweep['time'].to_numpy()\n",
    "s21rawtrial6Slow = trial6SlowRawSweep['s21'].to_numpy()\n",
    "timeCamtrial6Slow = trial6SlowRawAngle['time'].to_numpy()\n",
    "angleCamtrial6Slow = trial6SlowRawAngle['angle_3d'].to_numpy()\n",
    "\n",
    "timeS21trial8Slow = trial8SlowRawSweep['time'].to_numpy()\n",
    "s21rawtrial8Slow = trial8SlowRawSweep['s21'].to_numpy()\n",
    "timeCamtrial8Slow = trial8SlowRawAngle['time'].to_numpy()\n",
    "angleCamtrial8Slow = trial8SlowRawAngle['angle_3d'].to_numpy()\n",
    "\n",
    "timeS21trial5Slow = trial5SlowRawSweep['time'].to_numpy()\n",
    "s21rawtrial5Slow = trial5SlowRawSweep['s21'].to_numpy()\n",
    "timeCamtrial5Slow = trial5SlowRawAngle['time'].to_numpy()\n",
    "angleCamtrial5Slow = trial5SlowRawAngle['angle_3d'].to_numpy()\n",
    "\n",
    "timeS21trial1regular = trial1RegularRawSweep['time'].to_numpy()\n",
    "s21rawtrial1regular = trial1RegularRawSweep['s21'].to_numpy()\n",
    "timeCamtrial1regular = trial1RegularRawAngle['time'].to_numpy()\n",
    "angleCamtrial1regular = trial1RegularRawAngle['angle_3d'].to_numpy()\n",
    "\n",
    "\n",
    "timeS21trial3fast = np.linspace(timeS21trial3fast[0] + .03333333333, timeS21trial3fast[-1] + .03333333333, len(timeS21trial3fast), endpoint=True)\n",
    "timeS21trial4fast = np.linspace(timeS21trial4fast[0] + .03333333333, timeS21trial4fast[-1] + .03333333333, len(timeS21trial4fast), endpoint=True)\n",
    "timeS21trial1fast = np.linspace(timeS21trial1fast[0] + .03333333333, timeS21trial1fast[-1] + .03333333333, len(timeS21trial1fast), endpoint=True)\n",
    "timeS21trial7regular = np.linspace(timeS21trial7regular[0] + .03333333333, timeS21trial7regular[-1] + .03333333333, len(timeS21trial7regular), endpoint=True)\n",
    "timeS21trial4regular = np.linspace(timeS21trial4regular[0] + .03333333333, timeS21trial4regular[-1] + .03333333333, len(timeS21trial4regular), endpoint=True)\n",
    "timeS21trial1regular = np.linspace(timeS21trial1regular[0] + .03333333333, timeS21trial1regular[-1] + .03333333333, len(timeS21trial1regular), endpoint=True)\n",
    "timeS21trial5Slow = np.linspace(timeS21trial5Slow[0] + .03333333333, timeS21trial5Slow[-1] + .03333333333, len(timeS21trial5Slow), endpoint=True)\n",
    "timeS21trial6Slow = np.linspace(timeS21trial6Slow[0] + .03333333333, timeS21trial6Slow[-1] + .03333333333, len(timeS21trial6Slow), endpoint=True)\n",
    "timeS21trial8Slow = np.linspace(timeS21trial8Slow[0] + .03333333333, timeS21trial8Slow[-1] + .03333333333, len(timeS21trial8Slow), endpoint=True)\n",
    "\n",
    "\n",
    "interp_calibration_angle_trial1regular = np.interp(timeS21trial1regular, timeCamtrial1regular, angleCamtrial1regular)\n",
    "interp_calibration_angle_trial4regular = np.interp(timeS21trial4regular, timeCamtrial4regular, angleCamtrial4regular)\n",
    "interp_calibration_angle_trial7regular = np.interp(timeS21trial7regular, timeCamtrial7regular, angleCamtrial7regular)\n",
    "interp_calibration_angle_trial8Slow = np.interp(timeS21trial8Slow, timeCamtrial8Slow, angleCamtrial8Slow)\n",
    "interp_calibration_angle_trial5Slow = np.interp(timeS21trial5Slow, timeCamtrial5Slow, angleCamtrial5Slow)\n",
    "interp_calibration_angle_trial6Slow = np.interp(timeS21trial6Slow, timeCamtrial6Slow, angleCamtrial6Slow)\n",
    "interp_calibration_angle_trial3Fast = np.interp(timeS21trial3fast, timeCamtrial3fast, angleCamtrial3fast)\n",
    "interp_calibration_angle_trial1Fast = np.interp(timeS21trial1fast, timeCamtrial1fast, angleCamtrial1fast)\n",
    "interp_calibration_angle_trial4Fast = np.interp(timeS21trial4fast, timeCamtrial4fast, angleCamtrial4fast)\n",
    "\n",
    "trial1regular = pd.DataFrame(columns = ['Time', 'S21', 'Angle'])\n",
    "trial1regular['Time'] = timeS21trial1regular\n",
    "trial1regular['S21'] = s21rawtrial1regular\n",
    "trial1regular['Angle'] = interp_calibration_angle_trial1regular\n",
    "\n",
    "trial4regular = pd.DataFrame(columns = ['Time', 'S21', 'Angle'])\n",
    "trial4regular['Time'] = timeS21trial4regular\n",
    "trial4regular['S21'] = s21rawtrial4regular\n",
    "trial4regular['Angle'] = interp_calibration_angle_trial4regular\n",
    "\n",
    "trial7regular = pd.DataFrame(columns = ['Time', 'S21', 'Angle'])\n",
    "trial7regular['Time'] = timeS21trial7regular\n",
    "trial7regular['S21'] = s21rawtrial7regular\n",
    "trial7regular['Angle'] = interp_calibration_angle_trial7regular\n",
    "\n",
    "trial8Slow = pd.DataFrame(columns = ['Time', 'S21', 'Angle'])\n",
    "trial8Slow['Time'] = timeS21trial8Slow\n",
    "trial8Slow['S21'] = s21rawtrial8Slow\n",
    "trial8Slow['Angle'] = interp_calibration_angle_trial8Slow\n",
    "\n",
    "trial5Slow = pd.DataFrame(columns = ['Time', 'S21', 'Angle'])\n",
    "trial5Slow['Time'] = timeS21trial5Slow\n",
    "trial5Slow['S21'] = s21rawtrial5Slow\n",
    "trial5Slow['Angle'] = interp_calibration_angle_trial5Slow\n",
    "\n",
    "trial6Slow = pd.DataFrame(columns = ['Time', 'S21', 'Angle'])\n",
    "trial6Slow['Time'] = timeS21trial6Slow\n",
    "trial6Slow['S21'] = s21rawtrial6Slow\n",
    "trial6Slow['Angle'] = interp_calibration_angle_trial6Slow\n",
    "\n",
    "trial3Fast = pd.DataFrame(columns = ['Time', 'S21', 'Angle'])\n",
    "trial3Fast['Time'] = timeS21trial3fast\n",
    "trial3Fast['S21'] = s21rawtrial3fast\n",
    "trial3Fast['Angle'] = interp_calibration_angle_trial3Fast\n",
    "\n",
    "trial1Fast = pd.DataFrame(columns = ['Time', 'S21', 'Angle'])\n",
    "trial1Fast['Time'] = timeS21trial1fast\n",
    "trial1Fast['S21'] = s21rawtrial1fast\n",
    "trial1Fast['Angle'] = interp_calibration_angle_trial1Fast\n",
    "\n",
    "trial4Fast = pd.DataFrame(columns = ['Time', 'S21', 'Angle'])\n",
    "trial4Fast['Time'] = timeS21trial4fast\n",
    "trial4Fast['S21'] = s21rawtrial4fast\n",
    "trial4Fast['Angle'] = interp_calibration_angle_trial4Fast\n",
    "\n",
    "trial7regular = trial7regular[trial7regular.Time < 57]\n",
    "trial4regular = trial4regular[trial4regular.Time < 57]\n",
    "trial8Slow = trial8Slow[trial8Slow.Time < 55]\n",
    "trial5Slow = trial5Slow[trial5Slow.Time < 55]\n",
    "trial1regular = trial1regular[trial1regular.Time < 57]\n",
    "\n",
    "trial6Slow = trial6Slow[trial6Slow.Time < 57]\n",
    "trial6Slow = trial6Slow[trial6Slow.Time > 22]\n",
    "\n",
    "trial3Fast = trial3Fast[trial3Fast.Time < 57]\n",
    "trial1Fast = trial1Fast[trial1Fast.Time < 57]\n",
    "trial4Fast = trial4Fast[trial4Fast.Time < 57]\n",
    "\n",
    "\n",
    "trialBank1 = [trial7regular, trial1regular, trial4regular, trial8Slow, trial5Slow, trial6Slow, trial3Fast, trial1Fast, trial4Fast]  \n",
    "\n",
    "\n",
    "plt.plot(trial3Fast['Time'], trial3Fast['S21'], label = 'Trial 3 Fast')\n",
    "plt.plot(trial1Fast['Time'], trial1Fast['S21'], label = 'Trial 1 Fast')\n",
    "plt.plot(trial4Fast['Time'], trial4Fast['S21'], label = 'Trial 4 Fast')\n",
    "plt.plot(trial1regular['Time'], trial1regular['S21'], label = 'Trial 1 Regular')\n",
    "plt.plot(trial4regular['Time'], trial4regular['S21'], label = 'Trial 4 Regular')\n",
    "plt.plot(trial7regular['Time'], trial7regular['S21'], label = 'Trial 7 Regular')\n",
    "plt.plot(trial8Slow['Time'], trial8Slow['S21'], label = 'Trial 8 Slow')\n",
    "plt.plot(trial5Slow['Time'], trial5Slow['S21'], label = 'Trial 5 Slow')\n",
    "plt.plot(trial6Slow['Time'], trial6Slow['S21'], label = 'Trial 6 Slow')\n",
    "plt.grid()\n",
    "plt.xlabel('Time (from camera)')\n",
    "plt.ylabel('S21')\n",
    "plt.title('S21 VS Time')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(trial3Fast['Time'], trial3Fast['Angle'], label = 'Trial 3 Fast')\n",
    "plt.plot(trial1Fast['Time'], trial1Fast['Angle'], label = 'Trial 1 Fast')\n",
    "plt.plot(trial4Fast['Time'], trial4Fast['Angle'], label = 'Trial 4 Fast')\n",
    "plt.plot(trial1regular['Time'], trial1regular['Angle'], label = 'Trial 1 Regular')\n",
    "plt.plot(trial4regular['Time'], trial4regular['Angle'], label = 'Trial 4 Regular')\n",
    "plt.plot(trial7regular['Time'], trial7regular['Angle'], label = 'Trial 7 Regular')\n",
    "plt.plot(trial8Slow['Time'], trial8Slow['Angle'], label = 'Trial 8 Slow')\n",
    "plt.plot(trial5Slow['Time'], trial5Slow['Angle'], label = 'Trial 5 Slow')\n",
    "plt.plot(trial6Slow['Time'], trial6Slow['Angle'], label = 'Trial 6 Slow')\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel('Time (from camera)')\n",
    "plt.ylabel('Flexion Angle')\n",
    "plt.title('Flexion Angle VS Time')\n",
    "plt.show()\n",
    "plt.plot(trial3Fast['S21'], trial3Fast['Angle'], label = 'Trial 3 Fast')\n",
    "plt.plot(trial1Fast['S21'], trial1Fast['Angle'], label = 'Trial 1 Fast')\n",
    "plt.plot(trial4Fast['S21'], trial4Fast['Angle'], label = 'Trial 4 Fast')\n",
    "plt.plot(trial1regular['S21'], trial1regular['Angle'], label = 'Trial 1 Regular')\n",
    "plt.plot(trial4regular['S21'], trial4regular['Angle'], label = 'Trial 4 Regular')\n",
    "plt.plot(trial7regular['S21'], trial7regular['Angle'], label = 'Trial 7 Regular')\n",
    "plt.plot(trial8Slow['S21'], trial8Slow['Angle'], label = 'Trial 8 Slow')\n",
    "plt.plot(trial5Slow['S21'], trial5Slow['Angle'], label = 'Trial 5 Slow')\n",
    "plt.plot(trial6Slow['S21'], trial6Slow['Angle'], label = 'Trial 6 Slow')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('S21 (dB)')\n",
    "plt.ylabel('Flexion Angle')\n",
    "plt.title('Flexion Angle VS S21')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21230905 0.21748127 0.22251129 0.2267249 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.22470726 0.22677684 0.2324798  0.23660289]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.2395345  0.24024905 0.24018742 0.24000561]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.23982543 0.23965697 0.23769636 0.24004897]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.24219196 0.24216631 0.23759528 0.23442103]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.23293746 0.23467086 0.24047613 0.24445964]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.24671425 0.24940746 0.25133708 0.25265127]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.25296077 0.25187886 0.2526911  0.25351438]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.25200063 0.24967586 0.24685754 0.24389452]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.24310951 0.24354747 0.24231826 0.23986924]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.2362998  0.23330928 0.23172389 0.23002921]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.22847897 0.2281998  0.23008543 0.23304518]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.23285443 0.23280223 0.23390174 0.23737122]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.23785314 0.23459582 0.23262876 0.23483007]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.24072511 0.24836592 0.26226342 0.28231168]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.30889758 0.34315485 0.3874625  0.44186077]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.5026692  0.5684025  0.64026475 0.72451776]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.8134519  0.90540093 1.001117   1.1041168 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.2129631 1.3264663 1.4470004 1.5717436]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.6946404 1.818274  1.9466892 2.076665 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.2056308 2.336013  2.4706237 2.610701 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.7567084 2.9075375 3.0649624 3.2286036]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.3976724 3.5720959 3.7473435 3.9289777]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.117283  4.3121552 4.5319705 4.756214 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.983485  5.222847  5.4666924 5.7141542]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.9632034 6.21535   6.470878  6.730754 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.008665  7.300778  7.6082716 7.934846 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.27796  8.630019 8.989794 9.355784]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 9.72507  10.093444 10.463002 10.834685]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.211959  11.5940275 11.980351  12.370413 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.7677555 13.169748  13.575624  13.991277 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.418084 14.850552 15.288619 15.739487]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.201036 16.670166 17.145323 17.627846]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.120913 18.618162 19.111103 19.602852]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.096498 20.58545  21.069876 21.549856]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.026958 22.501701 22.968935 23.434595]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.8993   24.36299  24.824806 25.294907]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.780691 26.272615 26.765415 27.272295]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.791834 28.320684 28.863422 29.423445]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.996746 30.580992 31.169615 31.762245]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.364563 32.973    33.58548  34.20112 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.818867 35.44338  36.07748  36.708458]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.34015  37.95967  38.58528  39.213272]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.831776 40.457462 41.089523 41.72579 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.36788  43.016243 43.66941  44.313435]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.949318 45.57612  46.192307 46.790443]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.379997 47.969055 48.55897  49.149162]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.745434 50.348755 50.959415 51.56772 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.16964  52.771343 53.38045  53.990158]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.600796 55.203598 55.796215 56.385197]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.976135 57.56331  58.140938 58.718296]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.29241  59.862507 60.42475  60.977165]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.523617 62.071056 62.616104 63.158497]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.70426 64.25428 64.80698 65.36026]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.91012  66.45647  67.000244 67.54258 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.08373 68.62187 69.15296 69.66862]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.17309  70.672585 71.15709  71.62683 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.085434 72.53335  72.96403  73.37814 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.77825 74.17117 74.55813 74.93446]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.30118  75.66997  76.03125  76.385765]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.72838  77.054504 77.37842  77.69908 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.008385 78.30537  78.595276 78.8786  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.151924 79.41724  79.67613  79.92677 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.15837  80.38353  80.60206  80.812195]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.01393 81.20605 81.39693 81.58152]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.755325 81.921524 82.085815 82.24896 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.404816 82.55079  82.69139  82.83053 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.96384  83.082245 83.18791  83.28335 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.374794 83.459816 83.53312  83.59707 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.654144 83.703415 83.73636  83.75115 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.755905 83.75305  83.73788  83.71137 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.674355 83.62424  83.55775  83.47724 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.37872  83.259964 83.13417  83.00258 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.85785  82.69835  82.523254 82.33554 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.13971 81.92482 81.69027 81.44912]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.201614 80.9427   80.66759  80.382095]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.08408 79.76715 79.44035 79.10265]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.74636  78.37909  78.00318  77.614075]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.19674 76.76651 76.3234  75.86617]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.39489  74.90642  74.40312  73.886604]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.35838  72.816666 72.26067  71.69214 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.11158  70.52459  69.931725 69.34338 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.73978  68.12298  67.500206 66.868195]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.22552  65.570274 64.90269  64.215805]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.51961  62.81269  62.08894  61.352192]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.604343 59.84597  59.078175 58.305077]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.528114 56.74669  55.9601   55.168217]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.36654  53.555122 52.73833  51.91909 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.0951   50.268658 49.435215 48.593235]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.748    46.90119  46.047573 45.194923]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.342606 43.48831  42.633286 41.783115]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.935226 40.077976 39.213837 38.3511  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.491226 36.635937 35.782547 34.92941 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.083134 33.24661  32.409893 31.573143]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.744913 29.929564 29.11997  28.317274]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.526657 26.73656  25.953873 25.184969]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.424887 23.671555 22.929485 22.204473]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.492575 20.794157 20.114159 19.465797]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.840816 18.234196 17.644869 17.073391]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.517675 15.976722 15.4479   14.930107]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.418463  13.912981  13.4140835 12.930723 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.460836 11.994588 11.535172 11.088047]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.652644  10.231585   9.8221245  9.410239 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.000007 8.601314 8.23054  7.875692]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.5374217 7.223035  6.9262233 6.643011 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.3756332 6.125614  5.896752  5.6962624]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.5210776 5.366021  5.2331047 5.1210613]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.028396  4.9631357 4.918902  4.894924 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.8931456 4.913419  4.946989  5.001668 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.08118   5.18724   5.3053184 5.44218  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.606462  5.7951226 6.0064106 6.2409453]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.498808 6.780077 7.081862 7.402278]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.741249 8.098571 8.471108 8.857553]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 9.260741  9.673802 10.093129 10.523791]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.971588 11.429685 11.896043 12.369084]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.850763 13.339506 13.835064 14.342073]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.855427  15.370325  15.8770485 16.389194 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.905872 17.426525 17.953085 18.487217]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.030136 19.57917  20.134138 20.698286]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.273073 21.85999  22.455568 23.058271]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.672224 24.299107 24.937302 25.589178]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.2542   26.939363 27.649591 28.378847]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.120314 29.85478  30.592804 31.338238]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.08159  32.821556 33.56266  34.30362 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.043327 35.781437 36.514576 37.23909 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.954468 38.662678 39.37447  40.08438 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.795895 41.509693 42.22083  42.928333]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.642548 44.361378 45.073597 45.77751 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.469906 47.164192 47.858837 48.5452  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.22521  49.90022  50.566483 51.225452]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.874905 52.512417 53.141117 53.763477]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.377895 54.986824 55.588524 56.182026]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.776764 57.373375 57.968838 58.559853]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.158688 59.758087 60.356876 60.95237 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.541523 62.123917 62.69513  63.263966]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.82685  64.382576 64.9366   65.487434]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.03197 66.57257 67.11247 67.64394]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.17098  68.69508  69.212814 69.7201  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.2141  70.69893 71.17742 71.64711]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.1062   72.553665 72.983505 73.40384 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.814674 74.211716 74.59625  74.96864 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.32912  75.677765 76.01281  76.33653 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.649796 76.95038  77.238686 77.514145]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.77617  78.02755  78.269966 78.49881 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.710815 78.90134  79.078255 79.2418  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.392204 79.53104  79.650795 79.743576]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.81412 79.87308 79.92181 79.95823]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.968025 79.96437  79.947136 79.91297 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.860344 79.79368  79.70781  79.59801 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.47292  79.334404 79.18266  79.0191  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.8429  78.65408 78.44582 78.21898]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.9793   77.731766 77.47638  77.19865 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.888405 76.567566 76.23592  75.87035 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.49597  75.11243  74.70053  74.271065]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.83025 73.36836 72.88438 72.3823 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.859215 71.31233  70.75238  70.18125 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.59531  68.99281  68.37703  67.747246]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.09908 66.43396 65.75648 65.06263]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.35092  63.63174  62.899174 62.15807 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.408653 60.636795 59.858204 59.077103]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.291786 57.489056 56.678623 55.86338 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.037014 54.198914 53.35402  52.500126]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.63562  50.760975 49.880478 48.99714 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.109646 47.217846 46.32845  45.44108 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.56108  43.695183 42.83905  41.99524 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.16327  40.345955 39.545143 38.77544 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.049488 37.35252  36.683517 36.068703]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.47455  34.90371  34.367928 33.86388 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.37847  32.909527 32.458946 32.026497]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.613537 31.225018 30.853941 30.496693]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.166323 29.863686 29.582249 29.33805 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.11551  28.916128 28.76202  28.626663]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.506954 28.418379 28.360737 28.325428]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.311388 28.323915 28.362371 28.42371 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.50816  28.616669 28.744326 28.889816]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.05487  29.259058 29.484268 29.733934]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.02087  30.328611 30.662563 31.027916]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.4176   31.826506 32.24978  32.69455 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.17341  33.66803  34.170864 34.684414]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.22029 35.76905 36.33442 36.91996]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.52539  38.141453 38.77052  39.41487 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.074768 40.75052  41.437435 42.13246 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.83485  43.537567 44.240067 44.920776]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.592598 46.257935 46.91139  47.55    ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.169548 48.76797  49.34526  49.872734]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.36304  50.825516 51.234413 51.613937]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.97323  52.30691  52.6067   52.889244]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.155064 53.39853  53.614407 53.811985]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.990963 54.151245 54.293118 54.401386]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.479126 54.53522  54.572166 54.584335]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.571407 54.539528 54.491154 54.430164]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.357124 54.26671  54.170456 54.06443 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.946587 53.817116 53.68292  53.544086]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.396862 53.244434 53.091167 52.936928]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.760326 52.583065 52.40775  52.21883 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.028347 51.83296  51.628952 51.43215 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.241024 51.053734 50.861523 50.66685 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.47217  50.280983 50.091805 49.90402 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.72119  49.541668 49.367584 49.198242]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.037983 48.886272 48.743607 48.616955]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.499203 48.3912   48.29447  48.210476]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.140533 48.083286 48.053635 48.038715]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.039227 48.053993 48.08214  48.12374 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.202755 48.296223 48.40835  48.549805]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.707775 48.881634 49.083714 49.307087]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.54624  49.806217 50.076317 50.361576]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.665817 50.99363  51.336296 51.695423]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.071556 52.464024 52.874825 53.300358]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.736156 54.183132 54.64058  55.117153]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.61169  56.113567 56.622536 57.139748]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.663834 58.184616 58.70998  59.240593]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.77603  60.31543  60.852093 61.385925]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.92904  62.472496 63.015415 63.56038 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.10518  64.64246  65.173225 65.70595 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.238106 66.766556 67.27976  67.78311 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.27356  68.75341  69.227356 69.696175]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.16163 70.62528 71.08689 71.54053]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.98393 72.4177  72.84268 73.25985]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.669945 74.071625 74.45521  74.8258  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.18681  75.53884  75.881256 76.2118  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.53112 76.84242 77.14712 77.44134]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.72737 78.0051  78.27907 78.5258 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.7632   78.990364 79.20186  79.40511 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.59805 79.77572 79.93229 80.07685]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.211296 80.3381   80.448425 80.546074]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.63326  80.698784 80.751976 80.793625]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.82306  80.836624 80.839806 80.83637 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.81554  80.78013  80.722496 80.6436  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.554054 80.453354 80.33385  80.19692 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.04392  79.87163  79.68173  79.475975]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.25438  79.023636 78.78388  78.52233 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.240875 77.942764 77.62916  77.293755]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.94521 76.58588 76.20701 75.8086 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.40247 74.98737 74.55576 74.11247]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.65872  73.187706 72.69931  72.20024 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.69034  71.16937  70.638626 70.09704 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.53366  68.96021  68.37594  67.767944]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.15237 66.53147 65.89878 65.25019]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.59238  63.92364  63.242554 62.548447]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.838722 61.113064 60.376366 59.62902 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.873875 58.107025 57.327625 56.540657]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.74679  54.94454  54.134903 53.32131 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.50356  51.68488  50.86433  50.042767]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.221703 48.393917 47.550648 46.702274]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.86152  45.0219   44.182816 43.345573]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.510864 41.668724 40.824547 39.987236]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.15321  38.321712 37.491936 36.66234 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.83526  35.017265 34.206192 33.398945]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.595833 31.800528 31.014408 30.235254]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.46331  28.6984   27.940752 27.197287]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.465763 25.743149 25.03599  24.341055]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.655737 22.98542  22.336279 21.704695]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.091845 20.506317 19.95157  19.422659]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.92107  18.44757  17.995531 17.565813]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.15511  16.760866 16.384342 16.051298]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.747177 15.464251 15.209495 14.967266]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.742551 14.549477 14.382867 14.235138]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.108219  14.0047455 13.923831  13.8609705]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.816847 13.791794 13.791736 13.817531]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.867054 13.939618 14.034446 14.150733]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.286411  14.438557  14.607631  14.7961235]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.006769 15.24615  15.496275 15.756075]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.035471 16.343065 16.669575 17.018488]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.390652 17.786509 18.197206 18.623932]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.082237 19.563032 20.064734 20.585127]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.122568 21.677221 22.252533 22.856155]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.473093 24.1055   24.76281  25.447302]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.154797 26.881466 27.625505 28.380735]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.144632 29.91493  30.691261 31.474339]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.26488  33.0654   33.876137 34.694515]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.515087 36.33744  37.16292  37.986248]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.80341  39.61305  40.42033  41.221886]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.01693  42.813725 43.60214  44.38225 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.160095 45.93912  46.70046  47.446766]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.18524  48.92466  49.665977 50.399864]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.119194 51.83166  52.54455  53.249516]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.9488   54.644863 55.343674 56.045475]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.749454 57.44947  58.13482  58.807858]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.47317  60.128685 60.774963 61.41015 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.037113 62.65885  63.271126 63.870514]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.45163  65.032845 65.616516 66.203354]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.7903  67.37083 67.93636 68.48185]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.01847  69.54604  70.064575 70.561646]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.04446  71.517746 71.98433  72.43785 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.88196 73.3146  73.72619 74.13212]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.53374  74.92464  75.303345 75.6656  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.012215 76.34453  76.66996  76.99289 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.30939 77.6165  77.91908 78.21749]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.50725 78.78742 79.06243 79.33283]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.59767 79.8516  80.10167 80.34556]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.58124  80.806915 81.02694  81.24183 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.443344 81.641594 81.838875 82.03246 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.221275 82.407814 82.58995  82.77209 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.95333  83.133484 83.314674 83.493004]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.66713 83.83072 83.99226 84.15164]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.3082   84.45841  84.599945 84.73252 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.859085 84.98486  85.10895  85.22766 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.34477 85.46313 85.58326 85.69843]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.80851  85.918564 86.02968  86.13577 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.235245 86.32721  86.40741  86.47624 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.54027 86.60316 86.66489 86.71444]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.75212  86.78579  86.81608  86.839806]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.85922  86.87524  86.890686 86.8984  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.90158  86.899376 86.88849  86.86976 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.84627 86.81391 86.77131 86.72253]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.67083  86.608055 86.538055 86.46679 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.39722 86.31862 86.23352 86.14329]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.04653  85.93888  85.821846 85.70091 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.57515 85.44078 85.3014  85.14937]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.9871  84.81553 84.63593 84.44784]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.24887  84.04066  83.824455 83.5942  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.349014 83.09708  82.83495  82.5576  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.26216 81.94373 81.61344 81.27313]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.919334 80.543976 80.14988  79.740524]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.31492  78.869125 78.407875 77.92801 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.43039  76.91822  76.39345  75.858604]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.312614 74.75595  74.193474 73.613846]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.01164  72.384445 71.73908  71.07541 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.393906 69.69981  68.992744 68.27328 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.54488  66.81125  66.07236  65.337326]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.600006 63.862026 63.123585 62.38009 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.624607 60.86318  60.104797 59.34607 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.587524 57.830086 57.069912 56.303387]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.534237 54.763714 53.987614 53.202194]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.415527 51.622627 50.816303 49.997208]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.172054 48.34534  47.517773 46.685196]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.846416 45.009125 44.168736 43.32472 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.47789  41.615242 40.75663  39.901096]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.044327 38.182896 37.32309  36.466705]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.616974 34.76765  33.922413 33.0878  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.258476 31.43002  30.604984 29.78665 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.982079 28.184317 27.395573 26.619238]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.862366 25.12129  24.393316 23.680216]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.988083 22.313528 21.656683 21.018782]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.396553 19.784372 19.177244 18.564419]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.961666 17.370031 16.797777 16.24804 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.723574 15.220001 14.736592 14.270837]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.82099  13.38763  12.968371 12.558551]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.156747 11.764088 11.378141 10.997181]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.620202 10.248279  9.883866  9.533689]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.199643 8.876559 8.58521  8.306772]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.042556  7.8103166 7.603423  7.4185224]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.264151  7.1466856 7.0552263 6.988664 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.948695  6.942637  6.970148  7.0358787]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.1347556 7.2591896 7.413892  7.6000266]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.812584 8.053631 8.315242 8.59628 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 8.91102   9.255275  9.624228 10.016405]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.446487 10.894625 11.355782 11.829242]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.325046 12.843663 13.375036 13.915204]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.471862 15.04906  15.645824 16.261396]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.891705 17.532255 18.182604 18.842876]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.51135  20.184338 20.863705 21.54932 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.24086  22.938477 23.641935 24.356815]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.091112 25.835686 26.60447  27.389486]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.182076 28.982363 29.791746 30.608217]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.430391 32.25665  33.088833 33.929634]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.776875 35.629837 36.491215 37.36531 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.25149  39.151608 40.061024 40.97741 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.89821  42.825436 43.759624 44.688946]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.6134   46.512592 47.408333 48.30364 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.18478  50.054222 50.917667 51.770294]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.603424 53.42908  54.247906 55.05664 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.85376  56.638172 57.403473 58.147427]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.87746  59.5943   60.295536 60.977203]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.636616 62.28235  62.917328 63.541306]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.14518  64.73172  65.305244 65.852356]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.39062 66.92295 67.44558 67.95266]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.442665 68.92266  69.395966 69.85856 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.30761 70.74306 71.16544 71.57832]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.98171 72.37579 72.76019 73.13551]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.50495 73.86736 74.22267 74.56973]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.906685 75.233635 75.543724 75.83326 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.11328 76.37832 76.62161 76.85297]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.07269  77.27447  77.449844 77.60612 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.74521  77.863716 77.95839  78.029945]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.08097  78.110794 78.11166  78.085106]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.03634 77.97002 77.88543 77.76473]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.61469  77.439064 77.25029  77.051796]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.842834 76.61922  76.35592  76.05501 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.72968  75.38375  75.01886  74.637344]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.22974  73.795975 73.33938  72.86081 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.36194  71.84495  71.3088   70.753265]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.18118 69.59748 69.0016  68.37936]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.74135  67.087036 66.417725 65.737816]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.049126 64.353935 63.65223  62.936165]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.198635 61.452457 60.690666 59.91649 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.13916  58.358803 57.57464  56.78392 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.99074  55.199154 54.408722 53.62143 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.838295 52.05694  51.276344 50.49743 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.726284 48.963085 48.218605 47.49307 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.778557 46.078728 45.397972 44.735718]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.09281  43.47427  42.876114 42.291714]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.72515  41.208305 40.71423  40.237156]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.779224 39.338634 38.92037  38.523266]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.151993 37.804615 37.47653  37.171577]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.901783 36.65749  36.458244 36.294388]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.16347  36.06276  35.987984 35.938538]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.911213 35.92711  35.978863 36.05811 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.178604 36.32097  36.482822 36.67149 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.88602  37.129066 37.402245 37.705463]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.035446 38.391697 38.774624 39.181267]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.610065 40.062695 40.550194 41.06506 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.607    42.170666 42.752026 43.351833]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.968758 44.60318  45.254765 45.930878]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.619972 47.32932  48.05943  48.815285]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.58029  50.35549  51.15394  51.956802]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.760284 53.565548 54.3737   55.183945]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.99607  56.80919  57.622864 58.43226 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.234505 60.020504 60.79519  61.565224]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.32729  63.07733  63.816303 64.54416 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.25617 65.95531 66.64692 67.32949]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.98127  68.624886 69.26433  69.90066 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.533966 71.1596   71.781944 72.39608 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.00596 73.61405 74.21183 74.78587]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.34653 75.89096 76.42336 76.94386]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.44406 77.92512 78.39022 78.84322]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.284256 79.71777  80.14455  80.545586]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.937294 81.31982  81.69408  82.058914]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.41014  82.7472   83.069595 83.37997 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.678185 83.96414  84.23458  84.48959 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.726074 84.95445  85.171394 85.37433 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.56764 85.75243 85.9264  86.08881]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.23385  86.36728  86.490814 86.604324]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.704605 86.790085 86.848206 86.894554]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.92862 86.94032 86.94597 86.9477 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.94507 86.92333 86.8886  86.8445 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.78892 86.71856 86.63661 86.5439 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.43243  86.30514  86.166145 86.01162 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.8421   85.66137  85.469574 85.25654 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.0317  84.79931 84.5626  84.32182]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.074684 83.81495  83.540504 83.25367 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.95444 82.64157 82.31524 81.96697]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.599396 81.22054  80.83148  80.43126 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.0233   79.60349  79.166466 78.71559 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.25559  77.783745 77.29932  76.80298 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.293106 75.76784  75.22832  74.67752 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.11867 73.55269 72.98074 72.39716]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.79615  71.168655 70.514244 69.84354 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.15879  68.45986  67.72871  66.994865]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.25186  65.495186 64.72758  63.955208]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.176    62.387943 61.592377 60.79485 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.994263 59.189884 58.38065  57.571022]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.760914 55.948074 55.131092 54.317043]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.50552  52.694347 51.882355 51.07155 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.273613 49.47976  48.69087  47.910793]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.143803 46.386864 45.64185  44.91729 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.208817 43.51244  42.832985 42.186455]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.567497 40.974438 40.420807 39.90419 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.409687 38.9368   38.492146 38.073685]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.685165 37.326717 36.99724  36.696953]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.442867 36.211613 36.010887 35.855587]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.730324 35.6255   35.55457  35.52157 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.511997 35.52895  35.577103 35.648525]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.739048 35.85043  35.9866   36.14579 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.326157 36.52809  36.75352  36.997593]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.25677  37.526756 37.8123   38.137928]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.495663 38.884586 39.301758 39.73996 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.199978 40.703644 41.23107  41.77289 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.333656 42.918247 43.517117 44.12777 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.752228 45.39703  46.056473 46.728477]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.41424  48.11565  48.829933 49.555824]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.29539  51.062714 51.84351  52.631035]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.425743 54.228165 55.03694  55.849777]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.66856  57.49104  58.31582  59.141422]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.966824 60.792007 61.616337 62.436436]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.25356  64.067726 64.86082  65.63234 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.382904 67.10726  67.79858  68.46761 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.111465 69.72689  70.31774  70.879555]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.41694  71.93521  72.43412  72.889015]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.317375 73.73094  74.11226  74.46364 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.79323  75.09179  75.350784 75.58593 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.80063  75.992325 76.161835 76.31278 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.44449  76.557976 76.65175  76.724266]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.779335 76.815346 76.83366  76.83543 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.82271 76.795   76.73244 76.65593]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.56883 76.46421 76.33819 76.19665]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.03813  75.85511  75.65618  75.441055]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.20344  74.944244 74.667915 74.37454 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.058365 73.72617  73.37406  72.99994 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.604805 72.18807  71.75007  71.29019 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.79799  70.281906 69.7452   69.18719 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.60653 68.0077  67.3972  66.77583]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.12625 65.4612  64.78656 64.10217]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.405724 62.697044 61.9779   61.235622]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.48628  59.730865 58.96386  58.18654 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.40512  56.617573 55.821182 55.02288 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.220894 53.410763 52.59674  51.77441 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.948345 50.122833 49.302353 48.491207]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.674118 46.849342 46.02642  45.207695]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.394005 43.588985 42.786846 41.98536 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.18787  40.395687 39.60905  38.829147]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.051105 37.277267 36.51205  35.748863]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.98246  34.21409  33.442772 32.673958]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.911999 31.161083 30.417133 29.675514]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.936228 28.208502 27.492268 26.785946]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.08801  25.399231 24.722149 24.058084]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.407045 22.766657 22.142178 21.534883]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.949986 20.396967 19.886133 19.412601]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.977396 18.58038  18.220993 17.89784 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.611183 17.361917 17.143446 16.958565]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.807775 16.67823  16.567177 16.492393]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.462019 16.47698  16.525028 16.603603]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.71098  16.846037 17.022665 17.226694]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.44829  17.694399 17.974098 18.274097]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.591427 18.9329   19.297148 19.679817]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.07952  20.498075 20.941751 21.40797 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.888718 22.383522 22.896454 23.428354]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.979261 24.544788 25.130487 25.74522 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.383318 27.038671 27.709946 28.39628 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.098442 29.820969 30.556263 31.301317]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.062157 32.841393 33.63335  34.4401  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.262886 36.10414  36.960697 37.828026]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.7024   39.5845   40.4782   41.386444]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.302624 43.2228   44.14745  45.07955 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.021896 46.977776 47.937107 48.902214]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.88086  50.86558  51.851154 52.828754]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.792263 54.731697 55.650352 56.552895]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.435734 58.298046 59.14096  59.964214]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.77023  61.566143 62.348824 63.109367]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.856236 64.595085 65.325905 66.02853 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.69458  67.336494 67.95621  68.55567 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.13875  69.704216 70.248985 70.78171 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.300735 71.796295 72.27903  72.751205]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.209564 73.6534   74.08449  74.506   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.91942  75.318214 75.695274 76.05417 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.3959  76.71725 77.01886 77.3021 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.56556  77.80296  78.005516 78.18138 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.34076  78.47531  78.591545 78.69001 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.76661 78.82698 78.87055 78.89197]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.88661  78.86351  78.824066 78.772255]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.71162  78.63665  78.547226 78.45078 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.341484 78.21196  78.06608  77.911514]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.74634  77.569855 77.38181  77.17718 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.94713  76.705635 76.45379  76.18594 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.89735 75.58826 75.26005 74.91372]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.55544 74.18605 73.79906 73.39715]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.98482 72.56033 72.12017 71.66453]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.19375  70.711586 70.22075  69.71345 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.188   68.64611 68.08606 67.50877]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.9176   66.314865 65.69643  65.06064 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.4103  63.74662 63.05725 62.36206]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.661484 60.951653 60.23185  59.497524]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.75382  58.004887 57.248325 56.48076 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.702435 54.913757 54.1152   53.306427]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.488018 51.661747 50.82788  49.98835 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.143246 48.290432 47.431656 46.571934]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.719475 44.863907 44.005203 43.147793]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.29508  41.44929  40.610344 39.78107 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.96325  38.150967 37.340282 36.52283 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.703556 34.88451  34.058437 33.228172]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.39849  31.562578 30.723988 29.886   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.049767 28.216515 27.388994 26.57187 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.760357 24.957985 24.16267  23.380089]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.61064  21.853086 21.108774 20.375565]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.653318 18.945549 18.253195 17.571999]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.900343 16.240993 15.593178 14.959331]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.33577   13.722538  13.126611  12.5455885]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.9777975 11.427012  10.906526  10.413744 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.951806 9.521821 9.116243 8.726218]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.351746  8.006344  7.6805735 7.3723187]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.0791926 6.8171473 6.590717  6.386704 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.203867  6.049581  5.9259295 5.834538 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.780853 5.766625 5.794214 5.854746]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.946826  6.0744896 6.236776  6.433039 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.662302  6.9190245 7.1997657 7.5063577]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.843557 8.203746 8.57522  8.955455]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 9.359992  9.78207  10.215642 10.661048]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.126634  11.6024275 12.093222  12.599971 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.12167   13.659744  14.2164755 14.805713 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.413315 16.034166 16.667253 17.311693]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.968283 18.636555 19.31656  20.00822 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.710728 21.426743 22.153893 22.892435]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.644165 24.408508 25.186022 25.976978]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.776176 27.583614 28.402964 29.234539]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.075495 30.928133 31.79542  32.67052 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.555584 34.453938 35.364273 36.286545]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.218437 38.144684 39.0634   39.97388 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.878765 41.78504  42.699497 43.623924]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.558075 45.49281  46.42709  47.3636  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.28951  49.211864 50.134907 51.05067 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.9527   52.84185  53.71741  54.574425]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.412178 56.228947 57.031338 57.819447]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.59152  59.34939  60.091652 60.818317]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.5278   62.218517 62.896755 63.55817 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.20701  64.8519   65.493195 66.115974]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.72662  67.328445 67.91677  68.490135]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.05997 69.62479 70.18373 70.7357 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.2805   71.818016 72.335464 72.84552 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.34425  73.835396 74.32112  74.79649 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.26143 75.71266 76.14849 76.5697 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.9758  77.37035 77.75702 78.1342 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.49979 78.8505  79.18458 79.50625]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.81561 80.11218 80.40234 80.68581]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.96071 81.21454 81.45769 81.68958]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.909004 82.11875  82.31958  82.51011 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.68851 82.86101 83.02762 83.17692]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.309326 83.4287   83.533424 83.623886]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.70046 83.76003 83.80657 83.84202]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.85579 83.84088 83.8131  83.77535]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.71975  83.651344 83.5709   83.4765  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.36779  83.24719  83.115845 82.9707  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.810356 82.630684 82.43608  82.229546]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.00997  81.76593  81.497284 81.211716]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.909355 80.59401  80.266464 79.92576 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.563515 79.18882  78.79948  78.377556]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.92226  77.44147  76.941185 76.4203  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.87898  75.3109   74.70563  74.089134]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.45822  72.79851  72.123825 71.437965]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.739944 70.0305   69.306366 68.56797 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.81682  67.04799  66.260284 65.45743 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.63969  63.80881  62.965405 62.11354 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.25092  60.372368 59.480328 58.573425]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.657894 56.729923 55.791603 54.84412 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.88789  52.92502  51.958485 50.98646 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.002914 49.009308 48.01384  47.01189 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.005913 44.997334 43.985382 42.970642]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.96381  40.96284  39.96412  38.968437]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.98407  37.004513 36.02976  35.06599 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.09868  33.127155 32.160187 31.197859]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.243608 29.298683 28.365448 27.444567]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.542025 25.652412 24.771488 23.905582]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.060198 22.235994 21.42859  20.637733]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.86324  19.109474 18.386576 17.702007]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.038113 16.394762 15.791651 15.24657 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.74077  14.269107 13.834319 13.43113 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.061847 12.746704 12.464588 12.21048 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.998726 11.824198 11.677058 11.559123]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.480529 11.442017 11.445888 11.485012]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.559552 11.669804 11.814123 12.003419]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.25196  12.541022 12.851589 13.182434]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.534415 13.918291 14.345329 14.80663 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.293097 15.800268 16.337172 16.90412 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.490517 18.094759 18.7229   19.375776]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.050095 20.748495 21.47732  22.239002]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.018677 23.815119 24.623693 25.43959 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.26236  27.091038 27.927494 28.769922]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.61552  30.467495 31.325115 32.198235]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.089    33.993942 34.912178 35.838894]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.776455 37.723354 38.693424 39.67976 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.675846 41.6909   42.72296  43.767727]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.825615 45.895565 46.972633 48.048096]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.11485  50.16262  51.197052 52.225197]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.243153 54.251568 55.23729  56.208637]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.165607 58.10559  59.03611  59.961105]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.871075 61.76443  62.64807  63.521458]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.37394  65.2088   66.03079  66.831116]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.60901 68.35923 69.08667 69.79269]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.48075 71.15231 71.79295 72.38514]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.954796 73.517166 74.06784  74.60629 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.12129  75.60485  76.06775  76.519424]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.951645 77.361755 77.748726 78.11784 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.46627  78.79297  79.097984 79.37771 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.63961  79.8815   80.08629  80.266266]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.43194 80.58294 80.71703 80.83355]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.93569  81.026276 81.10399  81.1644  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.20719  81.23698  81.250565 81.24607 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.227585 81.195435 81.13105  81.04694 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.94525 80.81112 80.64525 80.46014]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.25139  80.01377  79.746605 79.4521  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.1285   78.77215  78.40096  78.017555]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.62346 77.21315 76.7859  76.35203]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.909065 75.453636 74.98875  74.509674]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.01605  73.50612  72.98703  72.459915]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.92401  71.36844  70.7992   70.221375]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.63578  69.03934  68.432556 67.811806]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.17556  66.531425 65.87807  65.20808 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.5219   63.823784 63.117413 62.395233]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.65374  60.897606 60.12756  59.350273]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.565437 57.766575 56.95116  56.13439 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.30993  54.479877 53.646694 52.796844]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.946217 51.095608 50.24162  49.37818 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.50959  47.64321  46.775776 45.905167]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.03916  44.18239  43.337013 42.502686]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.67631  40.859127 40.055904 39.264362]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.48284  37.713814 36.953045 36.192406]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.432354 34.692474 33.952927 33.212666]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.47659  31.751698 31.036804 30.331177]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.636686 28.955868 28.287506 27.633995]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.003777 26.384207 25.775694 25.177233]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.590223 24.017233 23.45203  22.89418 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.344658 21.803444 21.275747 20.761864]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.257162 19.75053  19.244904 18.741842]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.2504   17.767893 17.293482 16.829092]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.372402 15.923214 15.483883 15.05903 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.641578 14.233289 13.842004 13.467518]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.111033 12.772134 12.45796  12.178069]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.937344 11.734554 11.563928 11.420774]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.307541 11.221659 11.157514 11.114083]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.092059 11.091441 11.136451 11.222056]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.333879 11.475272 11.658842 11.865209]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.091617 12.342449 12.614299 12.903891]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.20955  13.537459 13.880393 14.2373  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.612271 15.003724 15.407958 15.825462]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.26853  16.734278 17.212582 17.702986]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.221653 18.761597 19.321733 19.898409]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.491144 21.110462 21.745152 22.394377]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.059748 23.739391 24.430553 25.132235]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.853865 26.58467  27.326042 28.080513]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.84609  29.616228 30.389973 31.181421]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.980001 32.783356 33.597187 34.42296 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.26043  36.106655 36.968147 37.847843]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.741657 39.64387  40.55412  41.477707]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.42169  43.373066 44.3311   45.294315]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.25985  47.22774  48.198013 49.167202]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.130886 51.081738 52.01332  52.919525]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.810272 54.683147 55.53964  56.387074]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.231987 58.076015 58.920532 59.760376]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.58417  61.38741  62.16345  62.925644]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.672817 64.39052  65.09774  65.79201 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.47322  67.14242  67.79926  68.443054]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.063805 69.66366  70.250435 70.82477 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.3831   71.928505 72.46143  72.97083 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.46389 73.95059 74.42932 74.88068]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.31462  75.73104  76.13189  76.517044]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.86981 77.18835 77.47692 77.73508]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.94158 78.12072 78.27714 78.3996 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.49629  78.56202  78.593285 78.59909 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.589836 78.56883  78.51684  78.441635]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.347725 78.2304   78.08623  77.916145]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.72982 77.52098 77.2876  77.02888]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.720436 76.38832  76.02864  75.63297 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.21945  74.788536 74.341675 73.88113 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.40614 72.91619 72.39798 71.87148]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.33669  70.783295 70.217735 69.63624 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.03982  68.427734 67.793396 67.134094]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.722942  2.722037  2.7200108 2.7177045]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.7163897 2.7183552 2.72057   2.7212286]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.7222276 2.723132  2.7226412 2.719725 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.7180765 2.7174556 2.7156835 2.7140384]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.7124395 2.7117045 2.7115886 2.7120624]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.7126565 2.712456  2.7118511 2.712071 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.713082  2.7159815 2.7196438 2.7236104]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.7290452 2.733644  2.7357206 2.7342622]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.7331748 2.7356827 2.7417314 2.7471688]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.7500699 2.7518282 2.7538676 2.7540438]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.7517061 2.7489223 2.7487926 2.751593 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.7575085 2.766974  2.7796752 2.7982569]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.824654  2.8556693 2.8893409 2.9251957]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.9654915 3.0121412 3.062481  3.1171348]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.1798053 3.24917   3.3217661 3.3984058]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.4803188 3.569339  3.6681433 3.7754724]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.8880692 4.0034    4.123779  4.2490315]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.377237  4.5228834 4.675802  4.835993 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.0058684 5.1862016 5.380012  5.588283 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.8109255 6.0443525 6.287078  6.536795 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.793472  7.058108  7.329971  7.6082706]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.8972654 8.201771  8.525034  8.866245 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 9.223181  9.596213  9.991235 10.397472]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.81247  11.23644  11.668245 12.103267]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.54215  12.984759 13.430076 13.886193]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.355143 14.83045  15.320696 15.827028]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.350435 16.885683 17.4307   17.98607 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.554693 19.129734 19.715681 20.313845]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.919855 21.533443 22.157242 22.790363]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.437435 24.095352 24.762163 25.438536]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.125072 26.819393 27.517603 28.22364 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.941227 29.667345 30.401356 31.142645]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.889378 32.65339  33.424908 34.20274 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.98724  35.781166 36.583935 37.393703]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.20873  39.025723 39.848755 40.682587]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.526478 42.378605 43.236237 44.09774 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.950104 45.804325 46.66158  47.51725 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.3665   49.21052  50.051067 50.87878 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.696247 52.50241  53.30428  54.106567]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.903576 55.694546 56.479073 57.254734]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.026264 58.78826  59.536316 60.26851 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.98281  61.67815  62.365143 63.043327]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.712593 64.37659  65.04273  65.7097  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.37409  67.03591  67.68856  68.328835]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.95995 69.57973 70.19385 70.79553]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.38876  71.97452  72.54979  73.111595]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.666145 74.21254  74.74369  75.259926]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.769905 76.27279  76.767876 77.24869 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.71528  78.17318  78.622536 79.056465]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.47623  79.882904 80.27403  80.650375]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.01413 81.36279 81.69421 82.00959]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.30301 82.58252 82.84526 83.09175]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.32227  83.53635  83.735146 83.919586]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.09037  84.24544  84.38411  84.508995]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.61747 84.71205 84.79151 84.85504]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.90369  84.937775 84.9608   84.96896 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.958626 84.93214  84.8984   84.85759 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.81202 84.75675 84.68364 84.59692]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.4979  84.38859 84.26858 84.13113]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.98002  83.813156 83.63538  83.45083 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.255486 83.050644 82.83782  82.61353 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.37564 82.12126 81.85276 81.57456]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.28904 80.99405 80.68857 80.37786]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.05888  79.72357  79.378334 79.02925 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.67619  78.3187   77.95245  77.577354]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.19721  76.817154 76.436005 76.04419 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.64089  75.23224  74.820915 74.40014 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.965744 73.52043  73.06908  72.60928 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.14469  71.67422  71.19182  70.699265]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.20404  69.71028  69.217834 68.72003 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.21834  67.71177  67.199425 66.68342 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.16473  65.64423  65.119156 64.58877 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.051956 63.50584  62.948868 62.385994]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.817936 61.24775  60.67514  60.102222]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.527866 58.946953 58.36689  57.78754 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.206463 56.625168 56.04131  55.454575]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.8708   54.293026 53.72195  53.15863 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.60391  52.050312 51.494747 50.94003 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.390373 49.842873 49.292927 48.73701 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.173412 47.604355 47.040886 46.48199 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.92695  45.37266  44.81467  44.255302]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.697727 43.134    42.561634 41.987743]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.411427 40.830536 40.243057 39.650105]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.053413 38.456207 37.854935 37.249077]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.642845 36.036633 35.431244 34.82956 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.22545  33.6104   32.993774 32.37777 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.761318 31.146229 30.533897 29.917921]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.305761 28.702696 28.104925 27.505186]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.899689 26.29268  25.685944 25.082392]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.485374 23.894222 23.309582 22.7332  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.161503 21.595856 21.037882 20.489513]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.952337 19.426476 18.910994 18.410555]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.925512 17.453182 16.990475 16.538927]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.102865 15.68677  15.290436 14.918214]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.562984  14.226617  13.908774  13.6015835]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.302894 13.01843  12.750191 12.49003 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.24412  12.021961 11.814961 11.620845]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.443712 11.292655 11.168207 11.070267]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.994489 10.931685 10.880095 10.846339]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.840419 10.858386 10.894766 10.945232]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.016524 11.107634 11.212415 11.335492]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.477679 11.636012 11.807638 11.992718]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.195766 12.419331 12.659821 12.910581]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.174075 13.452844 13.74821  14.064218]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.403116 14.759138 15.132644 15.521023]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.922617 16.337938 16.768095 17.20361 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.647694 18.106277 18.577566 19.064682]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.564848 20.075972 20.595972 21.125647]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.66336  22.205551 22.750889 23.302376]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.862226 24.42804  24.99846  25.568962]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.13976  26.7158   27.292917 27.872337]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.453772 29.035364 29.617996 30.202393]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.784782 31.363876 31.931797 32.494736]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.06217  33.62301  34.1793   34.738846]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.296425 35.846893 36.394405 36.936443]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.470825 38.004745 38.535267 39.05992 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.57172  40.07496  40.572884 41.06772 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.560566 42.05634  42.549835 43.031643]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.51115 43.99494 44.47675 44.95487]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.43227  45.907555 46.378036 46.845222]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.314907 47.784355 48.24928  48.707085]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.159462 49.60593  50.05165  50.498924]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.945347 51.387165 51.81981  52.247902]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.677975 53.106915 53.533054 53.958576]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.38523  54.81143  55.236305 55.66036 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.079605 56.495667 56.90831  57.31553 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.72192  58.129772 58.53451  58.938053]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.339428 59.738403 60.1351   60.524853]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.90718  61.283512 61.657646 62.032738]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.40202  62.762302 63.112087 63.454376]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.790928 64.11975  64.44365  64.761986]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.07591  65.38373  65.68215  65.966774]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.24949 66.53008 66.8023  67.06416]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.31589  67.55906  67.794945 68.0177  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.223114 68.417175 68.60062  68.77429 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.936584 69.08614  69.2243   69.35057 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.46179  69.55653  69.639046 69.708855]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.75866  69.79484  69.82405  69.845795]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.85477 69.8522  69.84002 69.81136]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.76989  69.71791  69.654686 69.580826]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.49833  69.40623  69.299965 69.18004 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.04989  68.911064 68.76457  68.60654 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.43617  68.25535  68.062805 67.8592  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.64674  67.42447  67.191696 66.952095]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.7022  66.43948 66.16887 65.8869 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.59179  65.289024 64.98216  64.672585]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.3596   64.042915 63.725765 63.408638]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.091015 62.77362  62.462368 62.154327]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.851147 61.553703 61.266506 60.988697]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.718018 60.450733 60.186565 59.918373]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.648834 59.38528  59.12355  58.866783]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.615635 58.36998  58.131817 57.90434 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.688957 57.48733  57.30041  57.129826]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.974655 56.83379  56.705853 56.587383]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.478584 56.382427 56.299473 56.229797]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.171703 56.120434 56.081192 56.05607 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.046444 56.04621  56.053047 56.070118]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.095787 56.135674 56.190647 56.255356]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.331287 56.420826 56.52314  56.637394]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.76111 56.89539 57.04167 57.19555]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.35498  57.521595 57.69794  57.88845 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.092823 58.30833  58.533615 58.769337]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.016205 59.271736 59.535103 59.812958]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.107124 60.41171  60.723648 61.044987]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.377342 61.716934 62.063545 62.414497]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.769825 63.13112  63.49895  63.872807]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.250275 64.62249  64.997925 65.37538 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.750885 66.12121  66.48904  66.84974 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.20233  67.557304 67.91776  68.28011 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.64093 69.00067 69.3562  69.71434]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.06605  70.410576 70.74726  71.07608 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.39805 71.71093 72.01395 72.31061]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.60014 72.88037 73.15523 73.42473]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.68532  73.936966 74.1749   74.40144 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.62227  74.83077  75.021255 75.196495]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.36298  75.51735  75.657585 75.78845 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.902855 76.0024   76.09301  76.172646]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.240944 76.29958  76.345505 76.3799  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.40297 76.41555 76.42042 76.41778]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.40529  76.38479  76.35132  76.301926]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.24418  76.180214 76.10815  76.02792 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.94291  75.8509   75.74833  75.634895]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.51056 75.37782 75.23326 75.07194]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.89514  74.70442  74.501045 74.28021 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.04417 73.79581 73.53476 73.26115]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.975586 72.67518  72.362274 72.04418 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.72026  71.38498  71.04402  70.696846]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.33259  69.95379  69.56758  69.174126]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.77102 68.35824 67.92491 67.48214]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.03186  66.574326 66.10903  65.63377 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.15025 64.66506 64.17247 63.6675 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.15695 62.64256 62.12526 61.60585]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.0776   60.544376 60.00776  59.465874]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.919643 58.37032  57.816162 57.257668]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.699482 56.14461  55.586327 55.021255]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.45661  53.89459  53.33675  52.780636]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.224693 51.656883 51.09248  50.532467]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.975357 49.42037  48.862328 48.29676 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.72873  47.163513 46.608044 46.057606]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.507965 44.96165  44.42145  43.8845  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.352673 42.829693 42.316196 41.818707]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.338287 40.874195 40.427917 39.998577]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.58455  39.1871   38.812737 38.456352]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.11264  37.78161  37.462234 37.15768 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.8705   36.600636 36.343616 36.094013]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.855904 35.634106 35.42927  35.24152 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.06866  34.905346 34.751038 34.60895 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.480957 34.369747 34.273045 34.192684]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.129745 34.082897 34.05135  34.033176]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.024532 34.028843 34.052563 34.09169 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.144028 34.20725  34.28044  34.37191 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.483788 34.61269  34.75642  34.912434]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.08456  35.274982 35.479298 35.698006]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.93119  36.18142  36.453453 36.73804 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.030552 37.33115  37.644955 37.96764 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.31356  38.670105 39.03709  39.41145 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.794247 40.19329  40.61324  41.04619 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.48888  41.93728  42.39057  42.851894]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.322426 43.798187 44.27648  44.753723]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.230045 45.707584 46.180946 46.650444]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.11636  47.578625 48.03787  48.49427 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.947277 49.394726 49.83787  50.283302]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.732983 51.18713  51.64218  52.093304]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.5397   52.979584 53.419243 53.859516]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.29199  54.714863 55.13459  55.55453 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.97681  56.402206 56.830364 57.25685 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.68194  58.10347  58.518414 58.931248]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.343773 59.758427 60.17153  60.58027 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.987015 61.390545 61.7874   62.178085]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.571766 62.96592  63.355633 63.74534 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.13708 64.52996 64.92438 65.31859]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.711945 66.10514  66.49579  66.88355 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.265015 67.63999  68.013115 68.38496 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.75382 69.11734 69.48086 69.84798]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.216576 70.574104 70.925735 71.27125 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.61238  71.950035 72.28252  72.60789 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.92564  73.23824  73.549774 73.849976]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.139    74.42436  74.70875  74.992035]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.27197  75.547745 75.81529  76.074875]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.32961  76.578514 76.81818  77.05099 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.27869  77.501335 77.7161   77.92196 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.12036  78.309425 78.490875 78.66596 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.83734 79.00338 79.15884 79.30658]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.45429  79.594124 79.723305 79.84582 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.96326  80.06771  80.160286 80.24379 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.321655 80.39385  80.45563  80.50721 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.54507 80.57483 80.59826 80.61187]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.615845 80.61676  80.61571  80.600235]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.572624 80.53939  80.49485  80.43382 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.35928 80.27385 80.17842 80.07064]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.950836 79.82133  79.68298  79.53831 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.38426  79.22145  79.052956 78.87609 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.68895  78.491585 78.2809   78.05673 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.831154 77.60156  77.36706  77.1237  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.86703  76.600716 76.328156 76.0498  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.76364  75.463455 75.15677  74.844986]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.52436 74.19549 73.8603  73.51915]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.16677 72.80731 72.4431  72.07351]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.70191  71.331764 70.95358  70.564255]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.16758 69.76494 69.35418 68.93212]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.50623  68.07786  67.64712  67.213264]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.77298 66.33004 65.88607 65.43611]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.9818  64.52792 64.07119 63.60577]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.138264 62.670235 62.20007  61.727383]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.25623  60.78595  60.310444 59.833817]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.35849  58.881374 58.401157 57.91974 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.433407 56.937782 56.438435 55.93998 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.43974  54.9327   54.42504  53.916306]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.411625 52.91035  52.408768 51.909283]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.414913 50.923424 50.43525  49.950287]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.458626 48.959755 48.461708 47.96746 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.4728   46.97656  46.481327 45.987755]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.495354 45.00269  44.509377 44.019787]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.53221  43.048607 42.569115 42.092354]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.61944  41.153202 40.69765  40.252567]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.816605 39.3877   38.966045 38.55705 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.15844  37.766647 37.396893 37.043827]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.70667  36.393745 36.105183 35.83513 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.583626 35.35234  35.14108  34.943928]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.756935 34.579666 34.41287  34.257244]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.111256 33.971954 33.838932 33.717754]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.607624 33.50888  33.4235   33.351074]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.29394  33.251846 33.221603 33.19833 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.18147  33.17731  33.18545  33.204433]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.237103 33.28555  33.34906  33.42891 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.5265   33.63805  33.761635 33.897076]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.04452  34.204216 34.378384 34.560585]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.752445 34.956856 35.169903 35.383095]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.600407 35.825684 36.06038  36.311184]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.578373 36.85831  37.141567 37.432568]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.73621  38.050003 38.373875 38.708263]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.05423  39.412125 39.780426 40.15773 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.540546 40.92875  41.327106 41.739033]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.159176 42.58433  43.012424 43.443855]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.87948  44.317406 44.756817 45.197865]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.642605 46.07727  46.503357 46.920307]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.32724  47.725185 48.118073 48.504536]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.881737 49.253597 49.6255   49.999733]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.37406  50.747997 51.123955 51.500683]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.880745 52.265408 52.649437 53.032127]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.40812  53.77614  54.141853 54.503586]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.86565  55.22936  55.593533 55.960632]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.32981  56.696552 57.06095  57.426888]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.79307  58.155792 58.518223 58.882027]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.243923 59.60095  59.95304  60.308064]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.668743 61.029564 61.387035 61.740917]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.091152 62.439404 62.79173  63.14658 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.503033 63.86253  64.220245 64.57553 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.92898  65.284645 65.63848  65.9862  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.32727  66.66     66.98447  67.303795]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.619064 67.93112  68.24012  68.54598 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.84859  69.14456  69.43006  69.708305]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.978806 70.240654 70.49153  70.72847 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.95285 71.16714 71.37249 71.56851]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.74918  71.912186 72.06187  72.19889 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.323906 72.43499  72.53281  72.61832 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.68872 72.74199 72.77938 72.80414]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.815865 72.811134 72.79244  72.759926]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.714134 72.655815 72.5866   72.511116]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.43001 72.33818 72.23676 72.12148]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.994194 71.85871  71.713036 71.556984]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.39375 71.22186 71.03622 70.83691]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.62732  70.40339  70.16333  69.912636]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.65392 69.38558 69.10323 68.80172]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.48426  68.159645 67.8277   67.48831 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.14023  66.78285  66.416306 66.03809 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.64614 65.2408  64.82777 64.40868]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.986828 63.560486 63.130108 62.701496]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.277428 61.859886 61.444557 61.030487]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.61816 60.20832 59.79786 59.38907]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.985996 58.58682  58.190666 57.797356]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.40858  57.03262  56.671856 56.32195 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.98158  55.651226 55.33116  55.02469 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.737057 54.46758  54.215675 53.979446]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.75751  53.551155 53.363914 53.196396]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.045597 52.91209  52.794243 52.690784]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.604576 52.53578  52.484554 52.44947 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.428387 52.420456 52.4267   52.447426]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.481945 52.52861  52.587475 52.65908 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.740444 52.83042  52.9295   53.03865 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.162556 53.302876 53.452477 53.611706]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.78215  53.965595 54.161945 54.36924 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.588627 54.82023  55.064938 55.321537]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.583942 55.851757 56.130116 56.422676]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.72353  57.0326   57.35104  57.679707]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.02275  58.377514 58.741585 59.109673]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.486164 59.87237  60.26158  60.65035 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.03596  61.41923  61.80716  62.201107]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.5935   62.98855  63.39124  63.794872]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.20135  64.60792  65.013596 65.41734 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.81532  66.20944  66.604576 67.00052 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.394104 67.7852   68.173225 68.55951 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.938515 69.31033  69.68114  70.046585]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.401375 70.752075 71.09679  71.43315 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.76022 72.07841 72.39241 72.70293]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.00573  73.3012   73.594635 73.886246]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.17274 74.44822 74.71456 74.979  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.24039  75.49736  75.748436 75.993225]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.23276  76.46659  76.696655 76.92002 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.136475 77.353004 77.56625  77.77141 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.968704 78.167404 78.36777  78.56138 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.75226  78.94293  79.131676 79.318855]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.50621  79.689995 79.86913  80.04736 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.22295 80.39568 80.56534 80.73259]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.894806 81.04834  81.197075 81.34408 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.48632  81.624825 81.76711  81.91395 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.0548   82.187706 82.31384  82.43909 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.56262  82.680664 82.79607  82.90942 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.02097  83.131294 83.24203  83.352356]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.458435 83.55876  83.65703  83.75767 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.863045 83.964775 84.05591  84.14565 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.235214 84.3191   84.402794 84.49055 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.57609 84.66282 84.75154 84.84103]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.92675 85.00662 85.08403 85.16557]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.251625 85.33131  85.40521  85.479675]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.55997  85.64585  85.729645 85.8098  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.88928 85.9734  86.05511 86.13544]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.21653 86.29639 86.37429 86.44969]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.52358  86.60132  86.68507  86.771515]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.849144 86.916794 86.98788  87.06093 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.132515 87.19994  87.26587  87.33161 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.400856 87.46957  87.5343   87.59667 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.65729  87.71973  87.781265 87.83973 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.89841 87.95722 88.01291 88.06738]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.119316 88.162155 88.20086  88.2465  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.2949  88.34062 88.38522 88.43155]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.47425  88.508026 88.536026 88.56686 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.597694 88.62727  88.65412  88.67953 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.70415  88.72391  88.736946 88.74205 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.74779  88.75384  88.75526  88.754364]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.75096 88.74243 88.7271  88.70331]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.6784  88.65266 88.62223 88.59207]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.56131  88.532234 88.50373  88.463684]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.41889  88.375336 88.32573  88.26628 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.20063 88.132   88.05606 87.97202]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.88406  87.796165 87.70421  87.60649 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.502785 87.39487  87.28304  87.16452 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.03583  86.89703  86.754036 86.611534]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.46835 86.31567 86.15079 85.97602]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.79735 85.61141 85.41529 85.20572]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.984276 84.75458  84.51905  84.279015]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.03029  83.76882  83.49967  83.223564]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.94148 82.6549  82.36195 82.06363]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.76177 81.45206 81.13488 80.81126]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.482185 80.14625  79.80123  79.45003 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.09969 78.74704 78.3817  78.00867]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.63077  77.25031  76.870155 76.490234]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.1104  75.7245  75.33182 74.93542]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.54201  74.15217  73.759094 73.361336]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.95792 72.55066 72.14343 71.72919]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.30833 70.88809 70.46842 70.04512]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.61274  69.17128  68.73289  68.298744]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.8587  67.41275 66.97022 66.52684]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.07883  65.63358  65.191216 64.7441  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.29771  63.849033 63.396667 62.94346 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.486984 62.026516 61.563057 61.09324 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.623196 60.14981  59.66711  59.17649 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.681942 58.18638  57.689346 57.19238 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.69687  56.200928 55.705067 55.211166]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.720474 54.228783 53.73682  53.24534 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.756237 52.26655  51.77332  51.278076]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.780525 50.282497 49.78476  49.28718 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.78992  48.29444  47.797676 47.29909 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.80081  46.302605 45.80479  45.304714]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.8001   44.299557 43.80494  43.314686]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.82669  42.338722 41.85119  41.36016 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.874043 40.395363 39.91969  39.439617]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.95611  38.472427 37.99076  37.51478 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.042038 36.56817  36.093998 35.624023]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.156116 34.688236 34.223137 33.760025]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.296783 32.831745 32.365707 31.903551]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.448141 30.993523 30.541332 30.09105 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.639458 29.184801 28.725813 28.26407 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.806028 27.353003 26.901175 26.449867]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.003962 25.565765 25.13713  24.712267]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.291037 23.876484 23.469223 23.071426]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.682152 22.301205 21.926117 21.557285]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.197172 20.844563 20.49865  20.161879]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.833006 19.5083   19.185814 18.867344]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.557394 18.255661 17.958035 17.66557 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.379877 17.094593 16.811867 16.533085]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.255257 15.979758 15.710806 15.447771]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.189609  14.935329  14.6887245 14.448776 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.210812 13.970996 13.729087 13.487273]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.245029 13.00217  12.763074 12.525692]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.286809 12.047417 11.812821 11.580475]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.349904 11.124622 10.906107 10.691828]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.48144  10.275842 10.071407  9.867629]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.666041 9.467632 9.271928 9.079489]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.8950815 8.714796  8.535808  8.358185 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.181864 8.005953 7.833019 7.666038]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.5060124 7.354569  7.207681  7.0627565]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.9205585 6.782102  6.653002  6.528349 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.403644  6.281427  6.1625094 6.048615 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.940975  5.839541  5.742197  5.6461296]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.551209  5.458399  5.3668838 5.276249 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.1866016 5.1016245 5.017946  4.937535 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.8748803 4.828455  4.798781  4.7767215]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.764766 4.766029 4.777984 4.797942]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.8352375 4.8890553 4.960415  5.0501494]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.152377  5.264212  5.3848896 5.5090485]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.6359353 5.7624626 5.897238  6.0421667]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.1959114 6.3535357 6.514322  6.679458 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.8514557 7.031623  7.221208  7.4207153]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.6304493 7.8485575 8.0736475 8.304876 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.538503 8.778802 9.026687 9.276053]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 9.525055   9.77296   10.024129  10.2788925]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.535734 10.795632 11.059463 11.323112]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.588526 11.85815  12.13197  12.41405 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.704113 13.001216 13.304936 13.614817]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.92869  14.247103 14.573169 14.907565]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.252855 15.60392  15.954876 16.314308]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.687649 17.070398 17.459566 17.854588]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.255175 18.659853 19.071558 19.490177]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.913778 20.341913 20.777988 21.22898 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.681791 22.139957 22.606089 23.0641  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.515272 23.960197 24.403606 24.84537 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.284971 25.724329 26.164488 26.602322]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.03794  27.467884 27.890335 28.312628]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.735912 29.159784 29.590137 30.030863]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.48204  30.938498 31.398918 31.863817]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.339375 32.82472  33.31618  33.80937 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.305527 34.801853 35.296654 35.78631 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.269238 36.752968 37.239197 37.72553 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.206635 38.686024 39.17082  39.661144]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.15832  40.661873 41.172615 41.691704]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.21817  42.748917 43.283276 43.822006]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.36522  44.915718 45.46849  46.021114]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.57324  47.12433  47.675037 48.229477]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.78194  49.331806 49.88017  50.42631 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.97293  51.522556 52.06613  52.59855 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.124374 53.647648 54.172333 54.702393]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.239418 55.782852 56.330925 56.886272]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.45586  58.032986 58.605556 59.183426]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.765495 60.347122 60.927704 61.503384]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.075092 62.64304  63.208927 63.771538]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.32557  64.8713   65.410324 65.939835]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.45906  66.97512  67.48914  67.996765]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.49863  68.992546 69.47727  69.95533 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.42428 70.88355 71.33441 71.77492]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.20303  72.623566 73.042984 73.453514]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.85303  74.24744  74.63998  75.032585]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.42115  75.808876 76.193535 76.568634]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.94614  77.327805 77.70575  78.07923 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.44458  78.80078  79.148384 79.48892 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.825035 80.15427  80.47522  80.78866 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.0933   81.394035 81.687645 81.97414 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.25481  82.530014 82.798584 83.06648 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.33328 83.59497 83.84856 84.09115]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.32835  84.567444 84.80526  85.03834 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.263985 85.48017  85.68394  85.87464 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.05314 86.22222 86.3785  86.5135 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.62841  86.73325  86.82764  86.908005]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.97755  87.039665 87.100365 87.15746 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.20998  87.256485 87.29861  87.34081 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.38369 87.42403 87.46128 87.49518]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.520515 87.54164  87.56103  87.57731 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.59186 87.60243 87.60201 87.59187]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.57467 87.55269 87.52852 87.5035 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.470436 87.42846  87.382645 87.33622 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.28528 87.23096 87.17327 87.10971]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.033394 86.94836  86.86902  86.78114 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.676544 86.56526  86.44996  86.33094 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.20963  86.086426 85.96022  85.82942 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.69675  85.56138  85.42368  85.282684]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.13478  84.985176 84.83242  84.6758  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.516975 84.35082  84.17695  83.99462 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.80481  83.611084 83.41333  83.20869 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.99412  82.76875  82.541794 82.31573 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.08358 81.84565 81.60369 81.3548 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.09575  80.827965 80.55366  80.27794 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.99561 79.70598 79.41189 79.11295]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.808495 78.49777  78.181984 77.86368 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.54595  77.22799  76.910065 76.58526 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.25437  75.924515 75.59578  75.26614 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.9288  74.58417 74.23338 73.87899]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.52525 73.17598 72.82804 72.4788 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.12428 71.76836 71.41305 71.05812]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.700676 70.34921  70.00416  69.65546 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.30038 68.94022 68.57826 68.21681]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.85531  67.48478  67.106285 66.72817 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.34921  65.967445 65.582634 65.19401 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.80155  64.40668  64.01146  63.616333]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.220707 62.823013 62.421745 62.01716 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.610077 61.199978 60.784966 60.363335]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.93875  59.51343  59.088364 58.66479 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.241074 57.819874 57.401623 56.9864  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.573036 56.157467 55.741337 55.324947]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.906494 54.485847 54.065292 53.645435]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.22612  52.802505 52.37359  51.939716]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.499977 51.05435  50.601048 50.141327]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.6801   49.216614 48.744057 48.26194 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.770412 47.267853 46.762093 46.25954 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.754528 45.248028 44.74219  44.234425]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.7246   43.213295 42.70284  42.188683]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.67095  41.155075 40.642567 40.133965]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.623367 39.108257 38.59543  38.086456]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.571827 37.042625 36.51542  35.993126]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.47459  34.956123 34.43657  33.91809 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.401638 32.889004 32.379    31.8742  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.37408  30.874321 30.377901 29.88681 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.396196 28.904686 28.411377 27.918234]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.427002 26.938242 26.45099  25.965572]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.485321 25.015102 24.554302 24.101103]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.653107 23.209145 22.7692   22.3324  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.89491  21.455639 21.011574 20.570305]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.131285 19.688189 19.244541 18.808456]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.379025 17.954384 17.535738 17.123053]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.72287   16.334747  15.9579935 15.592428 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.237414 14.895341 14.564852 14.241902]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.928735 13.628639 13.337466 13.051859]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.773392 12.50236  12.235125 11.972844]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.711354 11.451138 11.197844 10.952006]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.71449  10.479523 10.247643 10.02278 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.80296   9.584358  9.3689165 9.1574   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.95045  8.746671 8.547278 8.352802]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.168402  7.987322  7.8087935 7.6340466]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.4643397 7.300844  7.1429186 6.990049 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.840222  6.6906004 6.540603  6.3915615]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.249572 6.110864 5.971719 5.836326]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.7061195 5.5817437 5.462001  5.3458223]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.2337503 5.1250296 5.0182834 4.9154496]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.813861  4.7137403 4.6136503 4.514243 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.417814  4.323309  4.2304125 4.140027 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.052597  3.97194   3.8957512 3.824741 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.7616992 3.7092285 3.660634  3.6153579]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.5753522 3.5407348 3.5167036 3.5023882]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.499507  3.509119  3.5328612 3.569418 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.6176522 3.6792448 3.7533166 3.838585 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.9348097 4.040858  4.1598186 4.2913194]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.434118  4.5981894 4.7739816 4.965914 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.173359  5.3916025 5.619759  5.855748 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.100905  6.358216  6.624218  6.8989463]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.185708  7.4840283 7.7959023 8.122407 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.457572 8.798355 9.153954 9.51868 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 9.88983  10.265413 10.644435 11.024244]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.40715  11.798717 12.198688 12.605513]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.020796  13.441714  13.8771515 14.322977 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.773264 15.229214 15.690796 16.15756 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.636703 17.128014 17.627758 18.133833]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.64598  19.175278 19.713282 20.25606 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.80564  21.363113 21.924513 22.489828]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.06193  23.641724 24.232359 24.833097]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.442734 26.066746 26.699968 27.336296]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.975027 28.62007  29.270002 29.92485 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.586584 31.249321 31.908848 32.564835]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.218678 33.8675   34.513012 35.150963]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.779575 36.39905  37.01263  37.625134]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.23295  38.834763 39.435097 40.035126]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.620823 41.200546 41.774323 42.34144 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.903557 43.464157 44.024715 44.58435 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.141056 45.695972 46.249474 46.801735]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.34969  47.8904   48.426598 48.961124]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.48828  49.999454 50.516777 51.03559 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.555923 52.082024 52.61398  53.14857 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.679024 54.2048   54.730938 55.259174]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.787117 56.30455  56.820496 57.339375]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.859276 58.381714 58.904762 59.41763 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.92256 60.42855 60.93713 61.44947]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.954487 62.457092 62.95567  63.447903]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.935585 64.4232   64.91025  65.39215 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.86809 66.3356  66.79736 67.25543]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.70169  68.134026 68.56475  68.99378 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.413635 69.82585  70.23259  70.63194 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.02589  71.4155   71.797935 72.16918 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.531876 72.88998  73.25044  73.614784]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.97835 74.33821 74.69326 75.0501 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.41039 75.76751 76.11722 76.46081]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.79813  77.132484 77.46501  77.7947  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.11905  78.43827  78.75418  79.066216]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.367386 79.65762  79.940125 80.218216]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.49222 80.75757 81.01701 81.27666]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.53664  81.78143  82.01935  82.253525]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.47945  82.69872  82.912796 83.12296 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.33028  83.532555 83.729385 83.92216 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.11015 84.29001 84.46218 84.6256 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.778694 84.92062  85.05741  85.19275 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.31491  85.424706 85.5253   85.615875]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.696304 85.76945  85.832954 85.88535 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.926025 85.95903  85.9861   86.007324]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.02343 86.03466 86.03933 86.03949]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.04093  86.03721  86.029686 86.018394]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.99689 85.96486 85.92616 85.87907]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.825516 85.76338  85.69335  85.613304]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.521355 85.41654  85.299095 85.17593 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.04286  84.89894  84.750275 84.59113 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.420944 84.24375  84.06465  83.87958 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.68497 83.47736 83.25791 83.02632]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.78416  82.5312   82.266785 81.9963  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.714226 81.423904 81.1288   80.828094]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.52238 80.21339 79.89978 79.58068]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.25869 78.93258 78.59759 78.25468]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.90924  77.56009  77.203354 76.84199 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.47715 76.10742 75.73343 75.3531 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.96335  74.56675  74.169075 73.77196 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.376076 72.977936 72.57346  72.169815]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.76868  71.364174 70.95398  70.54091 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.127655 69.71504  69.304    68.894   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.480255 68.062775 67.643585 67.22096 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.79078  66.353935 65.91208  65.46524 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.0133   64.55316  64.08809  63.623825]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.15789  62.684155 62.20733  61.7291  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.2493   60.76625  60.280834 59.793243]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.306927 58.819992 58.32991  57.84131 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.35351  56.861023 56.36862  55.877857]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.3863   54.888683 54.390488 53.89285 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.396362 52.9011   52.40683  51.91119 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.412502 50.912632 50.41411  49.922657]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.431774 48.93702  48.43924  47.939075]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.43721  46.935444 46.4324   45.92731 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.4158   44.899204 44.37359  43.83514 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.286213 42.736664 42.184372 41.61981 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.043938 40.4619   39.876877 39.289337]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.697514 38.098427 37.491135 36.877773]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.257748 35.630028 34.99467  34.350628]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.701733 33.052273 32.402634 31.757086]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.114645 30.473574 29.83744  29.20699 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.581823 27.95992  27.340685 26.7265  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.114653 25.505465 24.898785 24.299095]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.708551 23.124405 22.549147 21.985723]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.432423 20.883358 20.338871 19.802797]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.277134 18.763023 18.262167 17.77042 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.284878 16.805775 16.33298  15.865401]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.398758 14.942731 14.495314 14.050634]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.615478 13.189615 12.769499 12.357978]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.957387  11.569322  11.1950245 10.834167 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.484119 10.142152  9.807604  9.486661]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.182224 8.896407 8.63213  8.393142]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.172225  7.9695387 7.781587  7.617462 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.485339  7.3847556 7.3132167 7.270007 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.249174  7.2494907 7.278906  7.3432627]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.4364    7.5547156 7.692157  7.848932 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.0346985 8.246588  8.479539  8.733704 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.008012 9.296919 9.598571 9.917029]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.252791 10.605506 10.966439 11.336321]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.717094 12.108549 12.514358 12.938084]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.372608 13.818779 14.276653 14.747223]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.227955 15.722802 16.227821 16.740166]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.259327 17.78386  18.31244  18.849714]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.4004   19.960587 20.526207 21.098175]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.681557 22.271029 22.864702 23.465593]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.087576 24.7175   25.358274 26.015032]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.678017 27.349554 28.032564 28.721586]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.416468 30.120369 30.833992 31.556377]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.28737  33.021763 33.76184  34.510674]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.266987 36.029716 36.793484 37.557335]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.31873  39.073917 39.814877 40.547188]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.271973 41.996574 42.719383 43.42955 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.129307 44.816917 45.493042 46.158386]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.811653 47.451508 48.07709  48.691437]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.29104  49.881172 50.467735 51.039677]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.597622 52.14592  52.686543 53.227783]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.772316 54.318382 54.863712 55.412796]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.95638  56.494873 57.03243  57.568344]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.09862  58.61925  59.127827 59.625618]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.11262  60.603542 61.097874 61.59154 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.07999  62.56184  63.03053  63.487373]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.941887 64.39347  64.84066  65.280365]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.71001 66.13866 66.56632 66.98552]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.39586 67.79891 68.19372 68.56946]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.93783 69.29611 69.63881 69.96592]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.287285 70.60522  70.916725 71.22491 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.53071  71.83244  72.128944 72.42448 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.72267  73.01288  73.293884 73.568756]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.83451  74.09086  74.336716 74.57416 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.12056994 0.11975975 0.11850386 0.11790551]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.11825515 0.11837349 0.11848366 0.11728913]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.11680647 0.11774587 0.11770475 0.11705973]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.11742429 0.11792795 0.11817951 0.11655114]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.11276927 0.10855469 0.10443933 0.10267866]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.10158933 0.1017521  0.10309853 0.1038839 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.10401901 0.10296419 0.10037521 0.09767076]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.09624805 0.09627919 0.0971275  0.09732778]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.09660869 0.09943215 0.10551976 0.11177919]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.11965121 0.12877394 0.13839716 0.14849466]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.15860122 0.16872507 0.18148111 0.19390799]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.20173688 0.2044875  0.20279199 0.20592605]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.21543989 0.23801044 0.27328923 0.32683706]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.39290166 0.46909323 0.5531894  0.646022  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7476538 0.8565439 0.9673053 1.0798728]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.1962276 1.3220428 1.4581335 1.5978507]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.7392842 1.8787999 2.0173082 2.1570137]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.2984183 2.4405122 2.585107  2.7413683]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.9042094 3.0779972 3.2672017 3.4714024]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.6901197 3.9214792 4.165601  4.4234934]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.6935325 4.973322  5.263014  5.5606027]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.8664694 6.181109  6.503167  6.8325815]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.169236  7.5140514 7.8676033 8.230588 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.606915 9.00013  9.41336  9.850973]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.304098 10.769494 11.245838 11.732484]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.231623 12.742908 13.265606 13.797956]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.337918 14.887088 15.446087 16.01689 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.596802 17.188469 17.793497 18.410786]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.031387 19.653988 20.282442 20.918777]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.554405 22.18698  22.819221 23.460752]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.112408 24.774387 25.438988 26.106237]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.772675 27.441954 28.103767 28.764444]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.422909 30.079788 30.740255 31.403778]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.06682  32.73477  33.408356 34.08504 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.76037  35.433937 36.112404 36.79706 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.49202  38.196262 38.908604 39.62894 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.356194 41.08772  41.813427 42.53648 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.25199  43.954334 44.644344 45.320576]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.9836   46.63776  47.282566 47.91956 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.553802 49.187885 49.825317 50.46486 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.105984 51.75004  52.396248 53.045483]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.69751  54.35152  55.005524 55.652576]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.288597 56.90841  57.505768 58.08892 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.66507  59.23731  59.805817 60.366962]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.919094 61.459103 61.986633 62.509964]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.029667 63.537865 64.03278  64.51228 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.97799  65.433304 65.87576  66.31623 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.75982  67.202614 67.64305  68.08911 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.54029  68.99356  69.44346  69.889015]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.33002  70.76819  71.202354 71.61804 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.0227  72.42374 72.81444 73.19832]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.57527  73.94091  74.297585 74.64626 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.98434  75.31738  75.646484 75.96873 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.28473  76.594986 76.89952  77.19484 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.48503  77.775925 78.060684 78.34086 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.61953  78.89299  79.162476 79.428856]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.69314 79.95474 80.21524 80.47492]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.7319  80.98622 81.2451  81.50658]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.758705 81.99863  82.22695  82.44617 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.65683  82.86594  83.07314  83.273445]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.45986 83.63045 83.7916  83.95058]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.110565 84.27187  84.422165 84.563225]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.69883  84.83217  84.96514  85.093605]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.222984 85.36059  85.49799  85.62907 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.763954 85.9038   86.04604  86.185356]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.321045 86.457794 86.5954   86.725655]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.84675  86.96474  87.079865 87.18895 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.29622  87.40371  87.50921  87.608864]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.7115   87.81639  87.926544 88.030785]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.12513 88.21776 88.30939 88.39936]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.48727  88.575485 88.66628  88.75426 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.83446  88.91021  88.985085 89.05814 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.12881  89.19629  89.26306  89.319046]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.35412  89.38374  89.410255 89.42738 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.43717 89.43641 89.42964 89.42637]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.41974 89.40889 89.39431 89.37335]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.34736  89.30929  89.25911  89.207466]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.14394  89.07447  88.998505 88.91532 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.81647  88.69936  88.57233  88.443756]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.310486 88.17015  88.021706 87.861176]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.685875 87.510056 87.32932  87.14164 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.94303 86.73143 86.5109  86.27781]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.027435 85.762596 85.49201  85.21292 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.92282  84.624725 84.31773  84.00332 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.68272  83.352196 83.01533  82.67421 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.32346  81.96445  81.597885 81.22067 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.83411  80.441025 80.04713  79.64125 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.221985 78.79444  78.35471  77.91077 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.46191  77.009605 76.5546   76.09186 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.617134 75.13004  74.64207  74.15179 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.65589 73.15334 72.64079 72.12016]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.594765 71.0653   70.53737  70.01162 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.48275 68.95705 68.43873 67.93072]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.432144 66.937996 66.44854  65.97106 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.50621  65.05622  64.62539  64.211685]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.814568 63.431004 63.06101  62.71548 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.38806  62.07762  61.786118 61.5166  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.265972 61.030754 60.81235  60.6166  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.44115  60.279156 60.13346  60.010597]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.90826  59.823376 59.753918 59.700554]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.663704 59.644356 59.64731  59.66987 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.704815 59.747738 59.798668 59.859043]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.925648 60.003433 60.089745 60.183758]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.28778  60.395794 60.50891  60.62837 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.75233  60.88269  61.017616 61.147755]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.281162 61.414024 61.540394 61.662655]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.7841   61.9104   62.033745 62.15243 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.262424 62.366028 62.468063 62.569202]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.669075 62.76439  62.857323 62.946594]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.024605 63.10482  63.18543  63.258034]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.319546 63.37774  63.433    63.479492]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.513107 63.535507 63.541546 63.53384 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.51446  63.47482  63.417633 63.34305 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.24844  63.129147 62.98478  62.81627 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.626347 62.4159   62.184124 61.929718]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.65808  61.368526 61.05897  60.73258 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.396935 60.053093 59.6974   59.332706]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.96088  58.58819  58.21277  57.834957]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.459427 57.09036  56.72837  56.374573]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.029537 55.68949  55.35513  55.031036]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.714993 54.41102  54.119366 53.840332]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.56921  53.30352  53.042957 52.78841 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.547222 52.31866  52.104015 51.904224]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.717236 51.54549  51.39326  51.255554]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.136158 51.03816  50.96128  50.906113]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.8723   50.85741  50.858467 50.87739 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.916286 50.975708 51.053486 51.15081 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.267696 51.406055 51.567482 51.75277 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.951706 52.179615 52.43402  52.711514]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.00968  53.335587 53.692554 54.074413]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.472713 54.889698 55.324287 55.775974]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.244843 56.730484 57.233345 57.748745]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.275017 58.813137 59.35933  59.911446]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.472946 61.044224 61.61923  62.19527 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.77685  63.361874 63.950603 64.536476]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.11637 65.69551 66.27091 66.83837]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.392555 67.93372  68.46172  68.96103 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.433784 69.88062  70.30332  70.70303 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.081436 71.4339   71.760445 72.0639  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.34156 72.59563 72.82901 73.03397]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.21455  73.37109  73.504845 73.61668 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.708176 73.77477  73.81473  73.83179 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.82773  73.803894 73.756485 73.6807  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.58163 73.46138 73.3179  73.1445 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.948944 72.73629  72.50505  72.251785]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.977036 71.68351  71.37124  71.03784 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.67787  70.289246 69.88071  69.45549 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.99686  68.50952  68.00501  67.479706]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.92578  66.33932  65.719925 65.07783 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.4192   63.746937 63.05977  62.352375]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.62573  60.884228 60.13101  59.36722 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.59478  57.816715 57.03359  56.240955]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.441418 54.643066 53.8406   53.02761 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.20475  51.374462 50.53628  49.69077 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.83873  47.981827 47.124336 46.265533]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.40023  44.527885 43.661457 42.797153]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.9354   41.071033 40.20278  39.33252 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.46677  37.609234 36.759354 35.9138  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.072815 34.236397 33.41642  32.61091 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.816807 31.032797 30.25817  29.490952]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.732494 27.983557 27.24306  26.512539]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.792805 25.08384  24.387678 23.701582]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.024021 22.356535 21.705189 21.064936]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.435625 19.817589 19.209673 18.61297 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.027449 17.45325  16.892725 16.347795]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.817568 15.29938  14.79165  14.297413]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.821313 13.356817 12.899368 12.453306]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.022438 11.609503 11.211009 10.824636]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.448577 10.078329  9.723679  9.385646]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.056134 8.736968 8.426144 8.123997]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.8356314 7.563749  7.309117  7.071686 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.8511767 6.651742  6.4731307 6.3137674]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.1733017 6.0511823 5.9500356 5.8699236]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.80759   5.762105  5.7337623 5.7229533]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.728353  5.7526655 5.7947764 5.854991 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.9389896 6.046499  6.174015  6.3172507]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.475989  6.6510577 6.8447733 7.0542235]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.2788043 7.5179477 7.7739015 8.040828 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.316485 8.611233 8.924962 9.251613]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 9.591141  9.943006 10.305633 10.680914]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.068944  11.4703865 11.885084  12.310721 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.743839  13.1884365 13.6459    14.108834 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.579337 15.060046 15.54892  16.04571 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.549494 17.059536 17.574207 18.095425]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.622318 19.153234 19.687145 20.224627]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.768343 21.320538 21.87965  22.444746]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.015114 23.590168 24.168447 24.754366]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.351015 25.95197  26.554752 27.162521]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.774359 28.391834 29.011715 29.629076]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.242876 30.85412  31.459211 32.05492 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.64146  33.222828 33.802658 34.380337]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.954437 35.520916 36.078327 36.6309  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.18028  37.72387  38.265434 38.808926]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.350113 39.884773 40.41315  40.93612 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.453545 41.965767 42.473698 42.975883]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.470947 43.959835 44.445415 44.928658]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.407898 45.885204 46.366978 46.852077]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.33011  47.802048 48.277588 48.75615 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.230885 49.70147  50.16669  50.626648]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.089523 51.553467 52.018227 52.485893]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.952244 53.419598 53.891823 54.364517]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.838688 55.31623  55.78653  56.251987]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.719208 57.188683 57.659927 58.134464]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.60598 59.08679 59.57249 60.05097]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.5237   60.99239  61.460903 61.929573]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.400307 62.87237  63.33964  63.804184]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.26935  64.732155 65.19226  65.6508  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.1092   66.563484 67.013794 67.46182 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.90668  68.35178  68.79916  69.245514]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.69029  70.13271  70.57076  71.005646]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.437935 71.87315  72.31054  72.73731 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.158325 73.578575 73.99414  74.40481 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.812935 75.21586  75.61838  76.01983 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.413895 76.80573  77.2019   77.590645]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.96826 78.34566 78.72348 79.1016 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.47196  79.83249  80.18619  80.532684]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.86581 81.17818 81.48796 81.80166]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.11132 82.41506 82.7123  82.99516]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.266174 83.526955 83.77863  84.02057 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.24776 84.46159 84.67085 84.87527]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.067154 85.25686  85.44702  85.63296 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.805626 85.9695   86.12618  86.28171 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.42316 86.55609 86.69125 86.81548]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.94462  87.07543  87.197464 87.30897 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.411766 87.51189  87.6101   87.70521 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.798996 87.88767  87.96973  88.04872 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.1298   88.213234 88.29498  88.37348 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.44798 88.51915 88.58648 88.65015]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.71448  88.780205 88.84677  88.91228 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.97085 89.0207  89.0728  89.12431]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.17296  89.220985 89.26874  89.31661 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.36369  89.40214  89.433655 89.46715 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.500984 89.52411  89.54742  89.580536]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.6119   89.63518  89.649254 89.65779 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.66395 89.66889 89.67434 89.68383]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.69916 89.70521 89.70433 89.70408]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.70512  89.70507  89.70394  89.703094]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.700714 89.69609  89.691696 89.69039 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.694244 89.69795  89.695496 89.692635]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.68926 89.68094 89.6659  89.65278]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.63953  89.628784 89.61892  89.60902 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.59585 89.57847 89.56564 89.54146]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.50977  89.48095  89.456924 89.428246]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.39262  89.35955  89.32695  89.287605]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.24725 89.20924 89.16913 89.12497]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.075935 89.02373  88.96668  88.90824 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.852135 88.799446 88.73988  88.67207 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.598656 88.5241   88.447426 88.36604 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.28327 88.19973 88.11882 88.03831]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.94704 87.84959 87.75233 87.65222]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.546745 87.43996  87.3295   87.21359 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.096886 86.9812   86.855965 86.7169  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.56674  86.405136 86.24099  86.07188 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.893166 85.70552  85.51279  85.31753 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.11827  84.90999  84.692825 84.467476]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.2346   83.992195 83.740746 83.48361 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.22207  82.95181  82.66827  82.372665]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.06661  81.75464  81.43761  81.112625]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.77855 80.43507 80.08526 79.73014]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.365944 78.994514 78.612    78.21764 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.81219 77.40019 76.98167 76.5509 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.105606 75.64324  75.172935 74.697014]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.20306  73.690765 73.1718   72.646164]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.10732 71.5552  70.99751 70.43392]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.86382  69.28681  68.69579  68.095985]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.487465 66.86765  66.236786 65.59531 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.93901  64.27068  63.594814 62.909348]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.21582  61.513596 60.806725 60.10735 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.41546  58.72519  58.041595 57.365536]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.690628 56.020374 55.35775  54.703568]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.0574   53.427322 52.826675 52.25747 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.70997  51.175632 50.661053 50.17321 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.708244 49.255337 48.81378  48.39292 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.994843 47.616688 47.254547 46.91056 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.587597 46.284485 45.998653 45.729282]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.491238 45.280777 45.09325  44.92386 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.771645 44.637978 44.525734 44.43522 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.365105 44.312855 44.277866 44.259823]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.255127 44.268818 44.30236  44.357315]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.432716 44.523617 44.63249  44.76405 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.92029  45.104168 45.308414 45.53253 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.781853 46.054752 46.349266 46.665646]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.000977 47.34719  47.70397  48.072124]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.457443 48.864025 49.279938 49.7015  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.14108  50.597843 51.06674  51.547565]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.03945  52.540714 53.049343 53.56467 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.08465  54.599    55.106064 55.60635 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.099228 56.582664 57.058167 57.52512 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.979496 58.416565 58.833805 59.23289 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.61348  59.962105 60.27624  60.57165 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.848793 61.099716 61.319237 61.507103]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.675327 61.82532  61.9452   62.036327]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.10381  62.1507   62.17647  62.179565]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.159058 62.118084 62.05702  61.961685]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.83918  61.69379  61.532436 61.357014]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.169197 60.969963 60.759117 60.539818]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.31295  60.07871  59.839302 59.598988]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.357628 59.121895 58.890602 58.66453 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.44759  58.24163  58.045574 57.858723]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.67696  57.50394  57.337524 57.176044]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.01749  56.85789  56.699917 56.550507]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.411606 56.28098  56.15926  56.044727]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.93553  55.834534 55.74482  55.664574]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.598522 55.549164 55.515102 55.49814 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.49904  55.51752  55.552677 55.606594]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.682777 55.776833 55.886806 56.01389 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.156677 56.314    56.49199  56.688477]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.898727 57.12445  57.365562 57.619297]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.89395  58.19127  58.505325 58.83339 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.175713 59.53168  59.904564 60.291355]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.68936  61.10114  61.527668 61.96666 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.41965  62.88636  63.360188 63.847496]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.34616 64.85665 65.37749 65.90593]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.44643  66.99595  67.543526 68.08681 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.6278  69.16825 69.70711 70.24124]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.76365  71.27401  71.772896 72.25873 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.72464  73.156784 73.5695   73.96325 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.33277 74.67703 74.9966  75.2883 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.55044 75.7899  76.0054  76.1955 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.36452  76.50699  76.62434  76.717674]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.78632 76.82771 76.84118 76.82741]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.78791  76.72525  76.641106 76.52717 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.37476 76.19185 75.97869 75.73308]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.45632 75.14691 74.80369 74.42985]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.029945 73.60381  73.15593  72.68716 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.19416 71.67856 71.14503 70.60182]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.04938 69.48932 68.91884 68.32737]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.717995 67.094635 66.46066  65.81996 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.171936 64.513214 63.838707 63.1484  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.44427 61.72889 61.00384 60.26609]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.517307 58.766106 58.00647  57.238438]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.4629   55.682808 54.895313 54.093838]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.280712 52.46314  51.6386   50.802307]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.952755 49.08985  48.21662  47.340164]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.458702 45.570732 44.680424 43.788906]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.908978 42.025272 41.13607  40.24523 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.35983  38.47639  37.597775 36.725254]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.856346 34.990356 34.128536 33.272644]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.431847 31.602886 30.781157 29.970448]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.174261 28.393274 27.62585  26.870949]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.12737  25.392475 24.668667 23.965298]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.276134 22.60012  21.94081  21.299812]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.676954 20.070454 19.485367 18.920603]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.371946 17.83312  17.305683 16.794655]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.29493  15.80482  15.323658 14.851669]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.384287 13.922215 13.471249 13.030408]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.596369 12.167878 11.745055 11.330866]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.927189  10.5375595 10.161656   9.798677 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.4460535 9.1041765 8.771503  8.444702 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.125881  7.816714  7.5170245 7.225708 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.9430795 6.6752667 6.424227  6.200659 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.010068  5.8674273 5.7769675 5.740072 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.7408314 5.7662215 5.8157234 5.891033 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.9993653 6.138201  6.3003573 6.4856277]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.6926646 6.9250965 7.182809  7.4571977]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.749707 8.05602  8.377836 8.729585]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 9.112454  9.520824  9.947546 10.390518]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.849641 11.327979 11.825388 12.341937]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.878824  13.439544  14.021421  14.6168585]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.227149  15.8610525 16.51309   17.180447 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.863546 18.5603   19.261389 19.968386]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.682861 21.401997 22.132952 22.865791]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.601461 24.343443 25.091057 25.844007]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.603493 27.371536 28.146118 28.927935]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.71376  30.50488  31.304968 32.108173]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.91324  33.724148 34.542133 35.36658 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.19827  37.034554 37.877148 38.727745]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.58148  40.439938 41.30955  42.187397]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.06882  43.953403 44.840546 45.72968 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.63001  47.53345  48.425392 49.299107]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.14074  50.957676 51.739178 52.50084 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.258904 54.010387 54.74816  55.466488]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.167786 56.859173 57.541855 58.217045]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.882393 59.533863 60.176826 60.815323]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.44883  62.075584 62.681656 63.265648]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.833645 64.391884 64.94237  65.4804  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.0038  66.51575 67.01622 67.50333]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.97446 68.43138 68.87947 69.31752]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.737526 70.144485 70.539696 70.92255 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.30406  71.69048  72.077225 72.46137 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.8401  73.20793 73.57529 73.94076]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.30134  74.657036 75.00899  75.361015]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.71003 76.05675 76.3977  76.73554]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.07673  77.40944  77.739944 78.06963 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.39396 78.71012 79.01878 79.32082]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.619995 79.91254  80.18764  80.44968 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.71277 80.97263 81.22077 81.46312]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.70023 81.93261 82.15956 82.36769]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.565475 82.75492  82.93684  83.110535]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.2689   83.42122  83.562225 83.68835 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.80241 83.9126  84.01864 84.11126]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.18457 84.24416 84.28966 84.31758]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.33643  84.34262  84.324776 84.28905 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.24373  84.18785  84.11176  84.017685]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.906845 83.777145 83.63208  83.47253 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.295334 83.103516 82.894226 82.6634  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.41862  82.16249  81.893585 81.612274]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.32087 81.01718 80.69652 80.3529 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.98883 79.60757 79.21074 78.79679]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.36357  77.91165  77.44613  76.967766]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.472145 75.956085 75.4238   74.87486 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.30106 73.70861 73.10028 72.47109]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.8291  71.17628 70.51261 69.83822]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.14131 68.42659 67.69005 66.93104]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.15527  65.37169  64.57915  63.767033]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.936317 62.10309  61.264847 60.422256]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.584045 58.744694 57.90882  57.076595]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.247738 55.42505  54.61688  53.823517]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.048378 52.29888  51.575466 50.87812 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.205105 49.56315  48.95614  48.384556]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.840767 47.324387 46.835125 46.36738 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.924736 45.51992  45.143692 44.790833]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.463142 44.167892 43.900547 43.657173]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.43784  43.2425   43.079727 42.953106]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.847633 42.765396 42.717518 42.694725]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.69153  42.71212  42.75695  42.823456]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.915092 43.03854  43.187176 43.356747]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.548817 43.76386  44.001015 44.26115 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.549576 44.86217  45.19686  45.558605]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.950424 46.361504 46.792477 47.253887]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.742558 48.25316  48.78173  49.327232]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.8884   50.465378 51.052834 51.66357 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.29474  52.950764 53.63426  54.33801 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.06096 55.80229 56.56033 57.33159]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.107506 58.887497 59.66962  60.442295]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.220505 62.001358 62.778038 63.551224]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.321465 65.08121  65.828    66.55669 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.26502  67.9519   68.61487  69.251785]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.85546 70.42882 70.97351 71.48959]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.97953  72.447426 72.89434  73.314354]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.701935 74.06612  74.4056   74.71829 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.00667  75.271835 75.51625  75.746185]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.954414 76.13365  76.28088  76.40618 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.508415 76.581894 76.63022  76.66193 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.67587  76.66637  76.63564  76.587234]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.518684 76.4295   76.326355 76.20854 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.07434  75.92421  75.7597   75.576065]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.372406 75.151794 74.91446  74.65684 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.380554 74.08748  73.7757   73.44492 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.10166 72.74686 72.37767 71.99354]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.595276 71.18858  70.773796 70.35179 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.922424 69.47494  69.012    68.536156]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.04639 67.54061 67.02392 66.49762]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.961205 65.41578  64.86356  64.294235]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.703644 63.103317 62.494537 61.873528]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.23872  60.594555 59.944557 59.269306]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.58468  57.895    57.19927  56.496944]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.78288  55.060707 54.33344  53.59972 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.859417 52.11005  51.353405 50.593277]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.828007 49.055683 48.287586 47.5258  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.770798 46.022697 45.279922 44.5488  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.827263 43.115543 42.414616 41.7282  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.056915 40.402107 39.762455 39.141026]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.542877 37.97038  37.41511  36.876446]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.354294 35.84486  35.35563  34.88221 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.42267 33.97764 33.54849 33.14069]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.753334 32.382126 32.03088  31.702055]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.390543 31.094118 30.816488 30.559593]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.320358 30.100597 29.897806 29.712713]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.54616  29.396929 29.261118 29.134924]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.02718  28.937004 28.864159 28.811724]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.785221 28.780985 28.793    28.82186 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.870796 28.941662 29.03667  29.161388]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.309572 29.478142 29.66897  29.883125]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.115997 30.366533 30.656744 30.971792]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.30866  31.67005  32.056488 32.463863]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.890324 33.338696 33.813396 34.30909 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.826508 35.36581  35.924698 36.508884]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.11876  37.74172  38.376144 39.020832]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.672966 40.328472 40.989025 41.65678 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.331127 43.008823 43.687164 44.36481 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.040115 45.712666 46.378567 47.034924]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.679695 48.3207   48.962967 49.606506]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.250412 50.887672 51.524086 52.16613 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.81307  53.464172 54.112858 54.75552 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.397686 56.040115 56.68113  57.324314]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.96896  58.61323  59.25777  59.898155]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.530457 61.15867  61.78193  62.397175]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.004864 63.603977 64.19737  64.78554 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.371956 65.9552   66.52891  67.08664 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.639175 68.19305  68.742874 69.27774 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.80031  70.3099   70.810905 71.310394]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.803535 72.28522  72.75585  73.21551 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.66433  74.099785 74.52106  74.92677 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.313736 75.687416 76.05973  76.428085]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.77803  77.11172  77.43364  77.744026]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.04486  78.33711  78.61148  78.868256]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.10672 79.32793 79.53377 79.72417]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.89603  80.054085 80.19735  80.322205]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.43172  80.527214 80.60758  80.67046 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.71187 80.73111 80.72585 80.69951]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.65371 80.58413 80.49267 80.38071]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.24815  80.096466 79.922195 79.72302 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.5018  79.26193 79.00413 78.7263 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.42657 78.10504 77.76519 77.40935]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.036545 76.64047  76.22774  75.80337 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.36607 74.91496 74.45218 73.97854]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.49337 72.9954  72.48816 71.96891]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.43787 70.89725 70.34613 69.78005]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.198204 68.60674  68.01504  67.421684]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.826126 66.222305 65.60101  64.96462 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.32796  63.689434 63.040115 62.380642]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.71141  61.03507  60.354233 59.672607]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.985146 58.285618 57.577206 56.870796]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.159664 55.444443 54.72539  54.000206]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.266975 52.531147 51.794914 51.06001 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.321842 49.577564 48.828545 48.08379 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.337864 46.5903   45.84658  45.108738]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.375603 43.645363 42.91612  42.19246 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.4793   40.77874  40.0821   39.389492]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.701935 38.01869  37.34072  36.668335]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.999985 35.33658  34.677227 34.018967]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.364536 32.716076 32.073765 31.439398]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.812473 30.188488 29.568083 28.952215]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.340778 27.73043  27.12048  26.50994 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.90447  25.305725 24.709564 24.118576]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.535639 22.957508 22.384747 21.81519 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.247543 20.687021 20.12891  19.572004]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.022898 18.484383 17.95575  17.437788]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.933401 16.443623 15.967779 15.496315]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.028327 14.573615 14.135462 13.706872]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.285204  12.8735895 12.470537  12.071807 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.682331 11.30289  10.932339 10.570538]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.2172785  9.874108   9.542339   9.21999  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.904746 8.596771 8.297075 8.010571]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.7397733 7.483354  7.233763  6.989681 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.7520432 6.5174904 6.2830987 6.044226 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.8112783 5.599404  5.3996377 5.211251 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.0368237 4.88019   4.737056  4.6071143]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.491409  4.389485  4.3055525 4.2431116]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.1985807 4.175336  4.1807785 4.201243 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.2322206 4.281363  4.3485904 4.4313784]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.5305643 4.645745  4.77584   4.921401 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.0898952 5.283306  5.4995723 5.7332892]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.983176  6.2505026 6.53439   6.8378158]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.1619234 7.501787  7.8589315 8.234618 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.627557  9.037475  9.463086  9.8958235]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.328621 10.757649 11.180541 11.596792]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.012043  12.4247465 12.830363  13.235081 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.63814  14.035829 14.43412  14.832051]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.2241335 15.610893  15.995051  16.37414  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.74488  17.109241 17.46857  17.822477]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.16901  18.506268 18.836239 19.155079]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.45927  19.744566 20.009619 20.258053]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.492294 20.712177 20.918457 21.115446]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.307686 21.496977 21.68906  21.872965]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.035847 22.183542 22.313015 22.42553 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.521486 22.602585 22.673386 22.73618 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.788755 22.826529 22.852942 22.864492]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.859774 22.836454 22.805443 22.77522 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.741442 22.707497 22.675247 22.665033]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.66868  22.682278 22.698656 22.712748]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.72187  22.7291   22.746807 22.766426]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.7864   22.805027 22.819307 22.8359  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.853546 22.868553 22.883436 22.898485]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.915468 22.936153 22.961252 22.99154 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.027475 23.072632 23.132048 23.19895 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.276423 23.366991 23.463858 23.563229]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.664417 23.767818 23.875278 23.986078]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.095848 24.207354 24.323593 24.446074]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.572126 24.697006 24.821096 24.940424]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.055582 25.160295 25.257765 25.347223]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.43497  25.5231   25.609589 25.695328]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.78219  25.870047 25.95479  26.04139 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.128927 26.216375 26.300406 26.374708]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.434542 26.492517 26.547369 26.599367]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.64359  26.67283  26.68868  26.691917]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.681194 26.658812 26.633196 26.6092  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.583399 26.556383 26.524002 26.48247 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.425047 26.35048  26.247145 26.126278]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.996391 25.874529 25.769167 25.68158 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.611713 25.539183 25.471142 25.409163]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.35597  25.310326 25.262592 25.217281]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.179922 25.154924 25.148092 25.15195 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.159838 25.174192 25.198656 25.230217]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.266373 25.29799  25.327353 25.357664]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.390924 25.422369 25.456944 25.497114]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.543228 25.59028  25.638475 25.689962]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.741722 25.792757 25.844507 25.898592]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.957748 26.02068  26.095362 26.183916]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.288355 26.408005 26.542387 26.687794]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.837315 26.99164  27.153202 27.317934]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.486736 27.661839 27.83602  28.009848]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.182444 28.359707 28.546808 28.748775]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.95077  29.155441 29.364655 29.579779]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.79949  30.020378 30.244488 30.475933]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.705383 30.932632 31.152866 31.35934 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.555838 31.74327  31.922005 32.093327]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.257034 32.404907 32.53547  32.649105]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.7371   32.79234  32.815243 32.803844]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.76891  32.712215 32.6406   32.554825]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.45336  32.338036 32.21192  32.078312]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.942846 31.798553 31.65083  31.50058 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.345766 31.189054 31.036938 30.88742 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.74711  30.617586 30.493006 30.370235]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.252298 30.140282 30.036806 29.941359]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.857155 29.784456 29.725094 29.677753]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.640205 29.61087  29.591158 29.58495 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.5944   29.621555 29.654997 29.69879 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.765175 29.849781 29.948755 30.06971 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.211426 30.365147 30.5332   30.712992]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.903593 31.104834 31.315163 31.534986]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.763166 31.997406 32.241177 32.496647]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.761044 33.033154 33.319458 33.608368]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.89754  34.184994 34.471836 34.761047]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.051056 35.342194 35.63638  35.92605 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.209484 36.488163 36.765438 37.04388 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.32379  37.600018 37.86541  38.12679 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.38133  38.626392 38.865494 39.0964  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.319508 39.530468 39.72215  39.890602]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.04363  40.177555 40.286407 40.35799 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.3929   40.39365  40.365517 40.306526]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.213455 40.085518 39.925774 39.730877]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.500546 39.2393   38.950073 38.643272]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.317513 37.965557 37.58815  37.191074]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.777294 36.349922 35.910896 35.460247]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.999496 34.53362  34.05699  33.56749 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.06419  32.558537 32.056393 31.555553]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.066273 30.593681 30.134214 29.677513]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.229023 28.803179 28.408209 28.027283]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.666882 27.32458  26.994234 26.679245]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.386387 26.110065 25.849747 25.605206]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.37468  25.152988 24.955923 24.785658]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.641048 24.517395 24.413862 24.330235]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.263622 24.21615  24.189693 24.18554 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.202482 24.24421  24.311943 24.396688]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.503065 24.634768 24.782175 24.946625]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.12906  25.32815  25.54757  25.783802]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.034557 26.307215 26.60764  26.926273]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.264793 27.624838 28.007904 28.413324]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.840204 29.290077 29.765795 30.266607]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.792677 31.343462 31.918331 32.51689 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.141304 33.790375 34.458256 35.141533]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.842716 36.561176 37.293335 38.03863 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.792206 39.55354  40.324738 41.112484]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.90417  42.702335 43.509907 44.32518 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.15616  45.98324  46.8004   47.607475]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.40606  49.194378 49.975536 50.749992]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.50581  52.23737  52.96739  53.684788]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.384953 55.078136 55.764046 56.43232 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.08563  57.732655 58.373127 59.00654 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.63102  60.2452   60.84427  61.430607]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.004738 62.566074 63.12361  63.67109 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.210335 64.740654 65.26198  65.7768  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.27856  66.76438  67.24331  67.714935]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.17596  68.62744  69.073784 69.51568 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.95144  70.376    70.791046 71.20215 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.60854  72.006615 72.3984   72.785866]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.16844  73.54665  73.913925 74.268555]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.61339  74.94733  75.269554 75.57935 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.877655 76.16236  76.43157  76.6855  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.92644  77.152084 77.36246  77.557434]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.72917 77.87781 78.00005 78.09922]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.1827   78.24838  78.28788  78.310005]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.31525  78.30249  78.26789  78.199394]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.11474  78.01595  77.89938  77.762344]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.60644  77.436554 77.25249  77.054146]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.84252  76.60617  76.346954 76.07439 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.78495  75.48083  75.170845 74.85605 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.52767 74.18574 73.83152 73.46824]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.10178 72.73642 72.37198 72.00847]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.63949  71.260826 70.88373  70.50853 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.13636  69.76612  69.3935   69.017914]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.64413  68.27002  67.891045 67.51125 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.13261 66.75444 66.37862 66.00221]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.62486  65.249916 64.874435 64.49828 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.12392  63.75044  63.374985 62.997665]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.61894  62.246105 61.87956  61.51585 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.156048 60.800587 60.44869  60.09975 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.75612  59.41844  59.085052 58.755596]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.43226  58.11745  57.811066 57.520195]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.24339 56.98387 56.74184 56.51056]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.29083  56.082516 55.883278 55.69488 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.51542  55.344963 55.18631  55.036636]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.893974 54.75926  54.63148  54.5116  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.40022  54.298264 54.20515  54.119694]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.04569  53.983017 53.929802 53.888256]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.854774 53.82845  53.806995 53.79104 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.781    53.775887 53.77422  53.77097 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.76728  53.763546 53.75981  53.76141 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.76406  53.765705 53.76573  53.76401 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.761044 53.758484 53.759212 53.75917 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.755978 53.755333 53.756466 53.758007]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.757366 53.756054 53.755856 53.754715]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.75284  53.752235 53.751614 53.751083]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.750237 53.74975  53.74975  53.7505  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.752754 53.752934 53.750748 53.74974 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.74936  53.748547 53.747517 53.746376]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.745316 53.743877 53.742542 53.741894]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.742054 53.74189  53.742176 53.74312 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.74376  53.74352  53.742733 53.74194 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.74282  53.744602 53.74446  53.743633]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.74568  53.748764 53.75027  53.749844]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.748703 53.75003  53.753677 53.75497 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.75404  53.752754 53.751274 53.750515]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.751026 53.751278 53.750885 53.75028 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.749184 53.748417 53.74795  53.747665]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.74814  53.74966  53.75044  53.750576]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.750065 53.74921  53.747524 53.745678]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.745228 53.745945 53.746193 53.746616]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.748142 53.748642 53.748466 53.750607]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.753555 53.75517  53.75588  53.755173]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.750694 53.747547 53.749283 53.75299 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.757057 53.760242 53.76102  53.762207]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.76494  53.770874 53.778465 53.781796]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.786213 53.794746 53.802166 53.807125]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.810783 53.81266  53.81506  53.82008 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.824146 53.82632  53.829063 53.833126]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.843987 53.855087 53.866074 53.878326]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.891327 53.906628 53.925682 53.949356]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.976727 54.00823  54.047108 54.087837]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.128307 54.17325  54.223297 54.278843]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.33796  54.396667 54.459858 54.52726 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.59875  54.675358 54.75391  54.834713]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.91364  54.99197  55.072315 55.15277 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.232662 55.31608  55.4031   55.49106 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.579983 55.671906 55.76169  55.84636 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.92971  56.012135 56.092937 56.171738]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.25129  56.33352  56.406452 56.476948]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.545235 56.61042  56.673386 56.735264]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.79667  56.85713  56.917465 56.978786]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.034374 57.083706 57.125153 57.160007]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.192894 57.22408  57.250187 57.273983]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.29358  57.305603 57.31702  57.327644]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.0737529 1.0845418 1.0943455 1.1058085]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.118931  1.1333996 1.1498389 1.1687108]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.1880665 1.2048136 1.220018  1.2349283]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.2506306 1.2657975 1.2808082 1.2959418]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.3100978 1.3226836 1.3354957 1.3512505]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.3652039 1.3781359 1.3789158 1.381009 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.3894982 1.3967386 1.4025868 1.4105133]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.4204552 1.4233727 1.4248738 1.4273276]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.4281456 1.4272442 1.4274575 1.4286464]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.4286232 1.4276142 1.426351  1.4267044]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.4276159 1.4285169 1.427765  1.4258764]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.4238806 1.4234786 1.4230934 1.4222183]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.4239528 1.4245062 1.4220979 1.4193397]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.4176781 1.4157774 1.4146146 1.4143227]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.4146713 1.4158901 1.4180439 1.4189587]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.4182028 1.4157761 1.4131883 1.4105649]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.4076521 1.4048876 1.4019899 1.3984358]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.3936808 1.3878348 1.3821951 1.3774593]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.3745604 1.3731428 1.3713503 1.3612322]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.3498347 1.3418975 1.3318043 1.315929 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.3006195 1.2885566 1.2745733 1.2589996]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.2406217 1.2221677 1.2037419 1.1841831]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.1650394 1.1475577 1.1298021 1.1115767]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.0949857 1.0801685 1.0651089 1.0524293]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.0394739 1.0279092 1.0240312 1.0189005]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.0126191  1.005013   0.9924832  0.97748506]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.9688633  0.96527714 0.9637658  0.9652624 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.9706225 0.9763123 0.9800481 0.9861672]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.99751824 1.0082965  1.0163318  1.0235891 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.0303806 1.039481  1.0505674 1.0627548]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.0758096 1.0917957 1.1097716 1.1297446]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.15313   1.1794215 1.208107  1.2393733]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.2727243 1.3070003 1.3421884 1.3785483]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.4163942 1.4561003 1.4986736 1.5448079]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.5945033 1.646813  1.6993766 1.752463 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.8067874 1.8611835 1.9140971 1.9715524]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.03279   2.0929253 2.1519241 2.2080832]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.262098  2.3282619 2.3991725 2.4741719]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.5468676 2.6200902 2.6998875 2.7847154]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.865876  2.9492426 3.0354176 3.1256526]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.2166145 3.3095834 3.4047837 3.50071  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.5978458 3.6980963 3.7990763 3.899078 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.9979658 4.097756  4.1992044 4.304999 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.412788  4.517119  4.620387  4.7204375]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.822704  4.928492  5.0333676 5.1355696]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.2335315 5.32833   5.424764  5.5215187]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.6152205 5.708884  5.800924  5.891037 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.9864783 6.084761  6.183355  6.284807 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.386899  6.488003  6.589736  6.6920958]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.7875957 6.879061  6.970431  7.0614014]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.1518626 7.2377    7.319895  7.399131 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.4745116 7.5497036 7.624678  7.7013226]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.780035  7.861223  7.9414344 8.019761 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.096941 8.176494 8.260427 8.34228 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.42241  8.501431 8.582663 8.66542 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.753097 8.84085  8.929731 9.021685]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.107077 9.195185 9.28292  9.371158]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.45805  9.5412   9.621965 9.704619]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 9.787121  9.867475  9.945802 10.026458]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.109052 10.197113 10.284226 10.366795]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.451805 10.540997 10.628002 10.714243]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.8050165 10.894938  10.98016   11.061869 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.1396055 11.217469  11.308478  11.400236 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.493463 11.587721 11.684056 11.784947]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.889772  11.991277  12.089769  12.1892805]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.292093 12.397999 12.501061 12.604419]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.710325 12.815382 12.916724 13.015898]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.118271 13.22611  13.332928 13.443771]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.557266 13.669402 13.78016  13.888719]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.999281 14.11103  14.225481 14.343727]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.465314 14.587174 14.706424 14.81975 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.933581 15.051181 15.169195 15.288073]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.407675 15.523925 15.641622 15.755862]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.869761 15.980059 16.086754 16.190119]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.29447  16.398405 16.503447 16.605297]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.70428  16.803486 16.906004 17.014545]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.123981 17.228825 17.332005 17.433521]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.535181 17.650936 17.762201 17.864567]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.960514 18.060059 18.164396 18.265335]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.363108 18.46386  18.565937 18.668917]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.772804 18.873732 18.974997 19.080662]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.190725 19.290865 19.385754 19.477793]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.569735 19.66149  19.753292 19.845142]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.940346 20.03899  20.140575 20.24202 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.342735 20.445978 20.546453 20.646553]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.745956 20.843622 20.941284 21.035162]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.122705 21.213043 21.306175 21.402767]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.502914 21.603548 21.705402 21.808752]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.913094 22.017792 22.123074 22.230303]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.337002 22.438477 22.541063 22.642204]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.741316 22.840322 22.938202 23.034912]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.132643 23.232815 23.341515 23.457573]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.575172 23.691988 23.815813 23.946577]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.071198 24.191845 24.312527 24.435844]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.56577  24.691883 24.813463 24.939129]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.06846  25.200994 25.33889  25.482103]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.615074 25.749521 25.886362 26.02044 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.149467 26.2801   26.411991 26.540157]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.665026 26.792135 26.916615 27.041758]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.170292 27.296562 27.426771 27.561028]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.701134 27.847183 27.992588 28.1346  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.275383 28.415365 28.55163  28.683054]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.811518 28.93912  29.066565 29.199217]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.333715 29.465677 29.595343 29.727503]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.863197 30.000269 30.138159 30.27462 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.407011 30.53937  30.672752 30.802586]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.931175 31.064186 31.196001 31.325836]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.451777 31.573595 31.686808 31.814281]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.949965 32.090054 32.22165  32.347652]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.47329  32.597427 32.716347 32.832882]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.947884 33.062943 33.1754   33.286568]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.397682 33.501537 33.6005   33.711838]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.831787 33.94699  34.05666  34.159218]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.261387 34.361427 34.45925  34.5561  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.65197  34.743744 34.83066  34.913998]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.99706  35.08615  35.180027 35.278057]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.371384 35.461887 35.55555  35.65589 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.756    35.854496 35.949394 36.04106 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.133842 36.220566 36.306934 36.391136]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.47361  36.561543 36.65548  36.748802]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.84298  36.943054 37.04595  37.146053]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.24469  37.342533 37.43447  37.521797]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.608955 37.699566 37.79281  37.88622 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.97867  38.071354 38.168076 38.268475]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.36781  38.462406 38.557552 38.654484]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.74491  38.834827 38.930195 39.03    ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.13105  39.233353 39.320732 39.409794]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.502495 39.60631  39.71069  39.81748 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.927803 40.041233 40.15143  40.255527]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.351852 40.45206  40.549686 40.647503]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.750183 40.85683  40.964924 41.063953]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.162136 41.261765 41.361465 41.466743]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.577927 41.69097  41.802002 41.911022]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.019714 42.128372 42.242596 42.356846]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.466793 42.574875 42.681717 42.790203]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.902267 43.012466 43.1239   43.2394  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.350903 43.460598 43.574436 43.691166]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.806606 43.914448 44.01677  44.11961 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.221104 44.321774 44.422905 44.518223]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.61059  44.70416  44.802723 44.90386 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.00714  45.11309  45.224285 45.339138]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.44786  45.550373 45.651443 45.756027]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.863342 45.966568 46.0665   46.164997]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.264656 46.364758 46.46932  46.582973]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.69852  46.8097   46.91767  47.026886]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.138313 47.24892  47.354546 47.455563]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.55541  47.663757 47.768204 47.86811 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.966335 48.065998 48.176235 48.28966 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.39951  48.513012 48.62776  48.740242]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.84989  48.952656 49.057613 49.1725  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.290455 49.40887  49.523636 49.636154]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.747    49.858456 49.970375 50.08412 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.20048  50.31332  50.427387 50.543045]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.66322  50.78448  50.902485 51.014904]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.132675 51.249157 51.35991  51.47072 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.583786 51.69443  51.804077 51.914097]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.02485  52.1412   52.259388 52.375862]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.490463 52.609447 52.73227  52.851776]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.96761  53.081093 53.19173  53.301357]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.41024  53.517776 53.621006 53.730576]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.84639  53.966484 54.08478  54.199585]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.314735 54.431248 54.54993  54.672924]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.79495  54.91119  55.022118 55.134377]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.24902  55.36278  55.47441  55.588085]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.704678 55.82212  55.94016  56.057037]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.17093  56.289455 56.409897 56.532246]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.65636  56.774906 56.89596  57.01691 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.137226 57.25735  57.375484 57.490295]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.605476 57.7223   57.835785 57.952023]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.072083 58.19493  58.315495 58.43496 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.554707 58.676136 58.79944  58.922764]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.046455 59.171326 59.297165 59.421486]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.546494 59.67512  59.804443 59.928936]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.051323 60.17525  60.305134 60.437355]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.571846 60.708145 60.847004 60.98587 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.121574 61.256813 61.394352 61.5308  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.662563 61.793854 61.926624 62.061546]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.20073  62.34362  62.48807  62.629677]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.76821 62.90906 63.04816 63.18731]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.326176 63.46178  63.596485 63.735817]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.87375 64.01029 64.14375 64.2791 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.41417  64.54574  64.673065 64.79892 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.93035  65.06637  65.20105  65.334526]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.467476 65.59888  65.72937  65.85856 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.98599 66.11503 66.24523 66.37717]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.511024 66.644394 66.77689  66.90821 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.03842  67.168    67.29946  67.439255]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.580025 67.715454 67.844894 67.970436]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.09728  68.23052  68.365326 68.49847 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.627495 68.75451  68.88154  69.007095]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.131004 69.25459  69.37578  69.49569 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.61583 69.73835 69.86304 69.98542]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.10398  70.217964 70.34601  70.46407 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.57599  70.68727  70.80061  70.915565]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.0297  71.14206 71.25512 71.36889]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.48457  71.604256 71.72861  71.85031 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.96803 72.08181 72.19431 72.3031 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.40936 72.51835 72.6253  72.72925]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.835434 72.9439   73.04936  73.156906]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.27078 73.37893 73.47912 73.58162]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.68724 73.79654 73.90701 74.01249]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.118996 74.229324 74.33962  74.44733 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.553856 74.65784  74.761345 74.864815]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.964714 75.06282  75.15979  75.261765]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.368645 75.47476  75.580215 75.68573 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.79318 75.89982 75.9998  76.10009]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.20462  76.31205  76.42097  76.527336]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.630455 76.73217  76.834656 76.94258 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.05108  77.156494 77.26224  77.36826 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.47326 77.57731 77.67946 77.77978]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.87834 77.97536 78.07548 78.17951]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.2863  78.37836 78.47889 78.58355]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.68647  78.78767  78.88753  78.986496]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.08525 79.18336 79.28081 79.37947]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.478264 79.57266  79.66398  79.75534 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.847084 79.94031  80.038704 80.14104 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.240486 80.33888  80.43645  80.53427 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.63331 80.73238 80.83034 80.92096]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.01556 81.11531 81.21247 81.30769]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.4028  81.49826 81.59698 81.6931 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.786804 81.883095 81.97831  82.074   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.174034 82.27331  82.370605 82.46945 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.56865 82.66767 82.7617  82.85387]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.94672  83.04014  83.13357  83.224625]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.31503  83.40434  83.491875 83.57867 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.664856 83.750946 83.838585 83.925285]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.01229 84.08966 84.16793 84.2514 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.338165 84.423676 84.51016  84.59612 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.677376 84.76049  84.8508   84.93658 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.01705  85.099754 85.18496  85.27152 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.357346 85.44232  85.524025 85.60221 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.67764 85.76225 85.8479  85.93283]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.02088 86.10928 86.19225 86.27024]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.34643 86.42341 86.50054 86.5808 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.66135  86.74114  86.819405 86.89704 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.97441  87.05376  87.13622  87.215836]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.29322 87.37072 87.44834 87.52715]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.6101   87.691475 87.77062  87.85399 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.93508  88.014275 88.093666 88.17198 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.24702 88.32537 88.4067  88.4906 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.58005 88.66495 88.74529 88.82095]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.89437 88.96557 89.03126 89.09491]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.157234 89.21825  89.27799  89.33557 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.39084  89.44351  89.493126 89.54158 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.588196 89.62637  89.65965  89.6948  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.74061 89.78019 89.81331 89.8451 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.86687  89.877686 89.88546  89.89567 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.90329 89.90286 89.90602 89.90862]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.90021  89.880775 89.85504  89.826164]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.795456 89.76536  89.73588  89.70625 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.66343 89.6137  89.55771 89.49217]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.42263  89.351715 89.2782   89.20188 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.12166  89.03577  88.942444 88.846214]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.74711  88.645134 88.538414 88.42564 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.30417  88.1748   88.04246  87.906876]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.76681 87.62271 87.47317 87.31711]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.159615 86.99951  86.82961  86.65832 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.4836   86.304146 86.11949  85.932755]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.74474  85.549866 85.343    85.12761 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.9149   84.704384 84.49623  84.287575]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.07466  83.86017  83.647865 83.43652 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.221794 83.00556  82.789246 82.57276 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.35617 82.13929 81.92035 81.69364]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.46904 81.24649 81.02085 80.79397]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.567116 80.336975 80.10442  79.88095 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.66586 79.45188 79.23804 79.0244 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.81119 78.5984  78.39067 78.1877 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.98616 77.78185 77.58176 77.3854 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.19013 76.99588 76.80004 76.60544]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.414276 76.22946  76.050865 75.86974 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.68822  75.50964  75.33354  75.161575]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.99584 74.83479 74.67    74.50257]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.33613 74.1729  74.01526 73.86403]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.716064 73.570564 73.427284 73.2849  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.14335  73.002785 72.86995  72.742   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.61439 72.48642 72.36142 72.23939]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.11935  72.000145 71.8762   71.75308 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.63272  71.514534 71.39528  71.27751 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.16174 71.04463 70.92595 70.80878]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.69482 70.58069 70.46728 70.35567]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.243866 70.133575 70.02367  69.91203 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.80263 69.69772 69.5956  69.49134]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.389595 69.28638  69.17893  69.068855]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.96081 68.85482 68.75025 68.64498]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.537445 68.42772  68.315765 68.20985 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.10419 67.99708 67.88731 67.77775]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.67802 67.57755 67.47274 67.36572]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.25662 67.15327 67.05231 66.94745]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.84102  66.734116 66.632706 66.52694 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.42008  66.312096 66.20473  66.09598 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.984215 65.87642  65.77322  65.668915]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.56283  65.458534 65.3573   65.25175 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.14182  65.028915 64.91677  64.80519 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.69831 64.58739 64.46987 64.35567]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.24639  64.12935  64.00765  63.887287]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.76898  63.64231  63.51434  63.389133]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.268917 63.15377  63.037502 62.914936]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.794594 62.677757 62.56034  62.43825 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.30931  62.178    62.050835 61.924515]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.795902 61.665684 61.536514 61.40456 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.267673 61.133316 61.00335  60.873444]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.744488 60.618332 60.492702 60.364254]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.233784 60.102528 59.96946  59.836575]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.70626  59.58027  59.448048 59.311756]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.17372  59.035503 58.9071   58.77787 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.646294 58.515167 58.383286 58.253956]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.12237  57.9898   57.86238  57.736454]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.60718  57.474216 57.345737 57.217396]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.09397  56.97312  56.851074 56.727394]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.603146 56.47658  56.349754 56.22619 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.099613 55.966526 55.83694  55.713226]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.596165 55.482624 55.36348  55.239525]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.11673  54.997738 54.875698 54.752365]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.63393  54.51532  54.39486  54.275326]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.171    54.068157 53.95984  53.84403 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.72217  53.59971  53.480003 53.35812 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.2398   53.12289  53.009098 52.895317]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.778202 52.660366 52.54523  52.433258]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.32499  52.219593 52.114376 52.0083  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.901062 51.79035  51.675373 51.55817 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.439583 51.324493 51.213318 51.104347]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.99727  50.888634 50.778618 50.671402]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.56344  50.45279  50.340122 50.22844 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.12081  50.009872 49.900986 49.791794]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.682243 49.568996 49.452263 49.33588 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.218483 49.098907 48.98336  48.87577 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.76988  48.66274  48.54877  48.428516]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.303566 48.17877  48.055485 47.934834]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.81682  47.699623 47.580025 47.460983]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.34539  47.231308 47.115154 46.994915]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.869358 46.749626 46.63596  46.51848 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.39769 46.27588 46.15133 46.02838]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.908005 45.784695 45.660583 45.53704 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.417595 45.30226  45.18901  45.07648 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.961998 44.846752 44.731937 44.617477]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.505127 44.38945  44.27402  44.15869 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.043907 43.933987 43.823948 43.70946 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.58736  43.462704 43.342937 43.228333]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.119144 43.010815 42.90305  42.79502 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.6842  42.57305 42.46011 42.34435]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.231148 42.120728 42.007534 41.8949  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.7863   41.684673 41.592777 41.495903]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.391872 41.278526 41.1627   41.048782]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.94143  40.835274 40.730774 40.63045 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.528294 40.419    40.304756 40.19373 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.089737 39.992817 39.89684  39.800045]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.702442 39.604748 39.504154 39.402733]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.303505 39.20469  39.104164 39.001   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.895336 38.7898   38.690098 38.598522]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.51158  38.41401  38.309628 38.201683]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.0998   38.00559  37.910583 37.809   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.71263  37.617844 37.521595 37.418953]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.312798 37.215576 37.113674 37.02059 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.93535  36.848892 36.754364 36.654545]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.556416 36.462463 36.371704 36.279842]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.181747 36.08203  35.9848   35.890717]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.797306 35.69842  35.594563 35.487816]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.387886 35.284477 35.184036 35.086464]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.985203 34.882687 34.78334  34.687195]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.585316 34.481407 34.3742   34.263718]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.149883 34.026733 33.906837 33.79014 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.67625  33.570496 33.465927 33.36159 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.2639   33.166496 33.05869  32.949646]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.84294  32.734997 32.623455 32.509518]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.39495  32.276848 32.157745 32.03943 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.922235 31.806698 31.691006 31.57395 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.456564 31.336954 31.217466 31.098282]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.977562 30.856722 30.73493  30.608868]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.485285 30.369198 30.26061  30.1448  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.02028  29.897507 29.788012 29.678127]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.555384 29.431904 29.313198 29.194542]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.06704  28.943521 28.814716 28.686228]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.556007 28.423239 28.295418 28.170996]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.047174 27.923492 27.797903 27.67122 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.543821 27.415873 27.289106 27.163523]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.038475 26.918705 26.7981   26.670761]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.546085 26.416878 26.291552 26.17143 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.047073 25.919483 25.7951   25.672064]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.544508 25.420753 25.300203 25.181417]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.064262 24.947601 24.829517 24.709362]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.585081 24.460175 24.335527 24.206528]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.069242 23.937048 23.811388 23.685295]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.558353 23.430885 23.301004 23.170866]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.043427 22.91606  22.790081 22.667084]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.548517 22.430012 22.307901 22.182976]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.058279 21.931826 21.812117 21.699818]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.579275 21.453306 21.326777 21.201824]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.074825 20.945417 20.816368 20.687475]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.559328 20.427925 20.292732 20.16948 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.049364 19.924995 19.80089  19.677334]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.555122 19.434902 19.31459  19.193512]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.075407 18.954185 18.82985  18.704023]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.578522 18.453701 18.329504 18.206953]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.086529 17.966177 17.844751 17.72996 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.61447  17.496569 17.379375 17.264194]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.150032 17.028376 16.905003 16.787205]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.667734 16.539968 16.41258  16.2891  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.160965 16.033682 15.906397 15.777787]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.648044  15.515987  15.3843775 15.250501 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.113528 14.977508 14.843858 14.71669 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.58656  14.452331 14.316296 14.181077]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.047295 13.915521 13.783    13.650502]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.520289 13.38838  13.247465 13.099556]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.95358  12.810175 12.668787 12.530318]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.396999 12.260022 12.119386 11.978902]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.8329735 11.692214  11.546844  11.397886 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.250871 11.108502 10.966182 10.816608]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.66762  10.523319 10.372842 10.216629]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.0587015  9.90284    9.746693   9.586504 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.425038 9.260913 9.104281 8.949894]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.794223 8.637266 8.479097 8.323405]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.16983  8.018156 7.861337 7.696685]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.53175   7.369085  7.2006426 7.0282965]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.8577805 6.6831856 6.5079646 6.336382 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.1724806 6.0118465 5.854111  5.695432 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.537567  5.3869967 5.240189  5.0948462]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.949154  4.8071146 4.665817  4.526106 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.3959074 4.275333  4.164163  4.056321 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.9483988 3.8401394 3.7329726 3.6331568]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.5386293 3.4515014 3.3717506 3.2961004]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.2193112 3.1434357 3.0751348 3.013815 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.9582777 2.913528  2.875498  2.8365   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.7989028 2.7659588 2.7371836 2.7115045]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6927917 2.6787064 2.6744056 2.6786046]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6802816 2.69027   2.710308  2.7363577]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.7608159 2.7869272 2.8187022 2.851446 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.8860464 2.9267557 2.972807  3.0260053]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.083137  3.1471431 3.2217827 3.3004248]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.3820248 3.4665546 3.5563412 3.6543016]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.7555432 3.856287  3.9570506 4.0563765]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.1544023 4.2617965 4.3889008 4.524944 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.6683335 4.8104644 4.958323  5.1125565]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.274748  5.4416194 5.611527  5.7780647]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.940009 6.10497  6.279122 6.46314 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.6499615 6.839741  7.032455  7.22364  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.413657 7.607312 7.810128 8.016545]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.223357 8.424857 8.625198 8.824382]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.032226 9.237539 9.443256 9.648914]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 9.852637 10.053964 10.25726  10.466186]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.678985 10.889228 11.096839 11.305307]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.515108 11.724954 11.936415 12.147225]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.355325  12.5636015 12.770805  12.972347 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.172592 13.372396 13.575357 13.780183]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.982952 14.182454 14.380186 14.580556]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.7827215 14.987478  15.19691   15.40773  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.617299 15.824441 16.027374 16.227766]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.425222 16.620163 16.813414 17.00549 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.200073 17.395662 17.591938 17.787735]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.987585 18.192942 18.399462 18.604446]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.8064   18.998262 19.184258 19.364405]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.550156 19.739353 19.927742 20.117481]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.305717 20.492693 20.680876 20.86917 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.051794 21.23168  21.406115 21.577156]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.745901 21.915863 22.088917 22.26254 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.434912 22.602541 22.768532 22.929213]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.088808 23.2484   23.411283 23.570976]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.73632  23.903088 24.070892 24.234642]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.396355 24.55547  24.712975 24.870983]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.029518 25.184458 25.332916 25.478596]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.623232 25.766712 25.912027 26.062223]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.21219  26.360748 26.506775 26.647884]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.786545 26.923784 27.06143  27.200197]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.33854  27.473461 27.609426 27.749866]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.890858 28.027058 28.15873  28.294079]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.43372  28.574354 28.71245  28.851562]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.99213  29.13158  29.268696 29.399826]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.528065 29.653902 29.781294 29.912169]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.040565 30.172287 30.308847 30.447006]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.585829 30.72605  30.862207 30.99749 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.13047  31.262495 31.39349  31.52602 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.665205 31.800053 31.934679 32.07581 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.213387 32.347874 32.485138 32.62713 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.772038 32.918205 33.06353  33.210598]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.356697 33.49836  33.64022  33.780663]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.92137  34.065037 34.201153 34.333584]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.466484 34.597385 34.729946 34.861984]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.00206  35.14985  35.289146 35.423786]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.56191  35.70199  35.835533 35.966843]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.10001  36.241425 36.37966  36.514946]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.653324 36.79419  36.93429  37.07573 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.220196 37.36187  37.503143 37.641666]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.781055 37.923195 38.063946 38.202522]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.340343 38.48396  38.627213 38.767567]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.907536 39.049477 39.191513 39.33306 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.475834 39.62007  39.765182 39.909435]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.053123 40.19901  40.34782  40.4962  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.634426 40.764133 40.885986 41.0066  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.132515 41.26558  41.399113 41.535477]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.671017 41.801888 41.928085 42.0452  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.169044 42.298607 42.426544 42.558754]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.6934   42.825104 42.951786 43.075752]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.204063 43.33252  43.45747  43.581547]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.7058   43.830067 43.956284 44.084007]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.205845 44.32976  44.4594   44.588234]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.716785 44.846943 44.97861  45.103085]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.220364 45.34449  45.468586 45.592358]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.718803 45.84195  45.968117 46.094616]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.21508 46.33742 46.46438 46.59102]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.714645 46.843258 46.977062 47.10047 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.219193 47.340775 47.467735 47.596504]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.728527 47.865128 48.00485  48.143276]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.27477  48.4044   48.534725 48.666065]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.797928 48.933792 49.070114 49.204304]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.338024 49.47221  49.607857 49.747524]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.877666 50.00405  50.13506  50.274376]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.417423 50.5669   50.716946 50.860306]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.000755 51.14216  51.27965  51.419544]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.564358 51.713745 51.86353  52.008904]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.1503   52.288044 52.429935 52.575466]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.720688 52.86578  53.008816 53.145325]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.282143 53.423233 53.563457 53.70104 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.83836  53.97825  54.121655 54.269226]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.41814  54.56145  54.70081  54.839695]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.977497 55.11648  55.255707 55.393333]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.531193 55.67313  55.814255 55.952198]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.100723 56.249218 56.39786  56.546593]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.69243  56.83476  56.974045 57.114807]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.2529   57.38474  57.523655 57.669044]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.81365  57.95746  58.100468 58.238483]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.373196 58.50569  58.638798 58.77689 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.91796  59.059277 59.200832 59.342953]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.481983 59.619453 59.75598  59.88764 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.017654 60.1503   60.2755   60.40989 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.54757  60.679638 60.80636  60.93639 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.067722 61.195942 61.324688 61.45718 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.589005 61.718655 61.847332 61.97697 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.10509  62.23454  62.36675  62.499573]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.632553 62.75872  62.881042 63.004482]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.128902 63.25729  63.38822  63.519764]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.6499   63.77976  63.911148 64.04187 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.17159 64.29908 64.42398 64.54718]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.67226 64.80113 64.93199 65.06568]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.19842  65.32941  65.45927  65.587715]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.713715 65.8494   65.98633  66.11982 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.249435 66.378296 66.506386 66.63646 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.767654 66.89987  67.03312  67.16751 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.30138  67.434715 67.56768  67.70059 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.8335  67.96638 68.09915 68.23394]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.37042  68.506874 68.63984  68.77234 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.9064   69.041405 69.17645  69.309395]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.44165 69.57518 69.71347 69.85477]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.99399 70.13817 70.28146 70.42065]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.559364 70.70298  70.84715  70.99166 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.13983  71.288635 71.43513  71.58089 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.72738 71.8735  72.02063 72.16983]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.31931  72.467804 72.61759  72.76828 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.919876 73.07276  73.22967  73.38527 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.53622 73.68799 73.84066 73.99459]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.149376 74.30342  74.45811  74.61178 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.76547  74.920616 75.0778   75.23504 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.39041 75.54212 75.69111 75.84149]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.99112  76.14123  76.291595 76.442535]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.58276 76.72327 76.86871 77.0187 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.16668  77.312675 77.45672  77.599556]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.74105  77.88116  78.01953  78.160576]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.29793 78.43327 78.57325 78.7132 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.85064  78.98867  79.126495 79.25902 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.38953 79.52436 79.66106 79.80009]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.93921  80.077934 80.212845 80.34669 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.48422  80.6252   80.765205 80.900734]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.03408 81.16738 81.30083 81.43568]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.569984 81.70318  81.8341   81.96295 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.09123  82.22038  82.35132  82.483345]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.61511  82.744514 82.87077  82.99804 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.12639  83.25315  83.38071  83.510826]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.64011 83.76413 83.89037 84.01909]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.14529 84.26877 84.38953 84.50918]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.63001 84.74753 84.85663 84.96798]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.08993 85.20713 85.32074 85.43688]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.554184 85.669304 85.78061  85.892136]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.003876 86.11797  86.2365   86.35844 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.4836  86.60701 86.72369 86.83687]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.94704  87.057465 87.16816  87.279274]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.40069 87.51912 87.63803 87.76379]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.89486  88.024124 88.150314 88.274704]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.39292 88.503   88.6145  88.72806]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.83854  88.945595 89.05307  89.16435 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.274536 89.38133  89.49739  89.61371 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.718124 89.82359  89.93563  90.04964 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.1629  90.2745  90.38101 90.48329]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.581535 90.66969  90.746475 90.81895 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.88647 90.95793 91.01975 91.07   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.105576 91.133316 91.15492  91.17427 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.191   91.20216 91.20538 91.20207]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.196205 91.18655  91.168976 91.14121 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.10642  91.065994 91.02336  90.97789 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.930786 90.88716  90.83605  90.76164 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.6881   90.615    90.53569  90.450645]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.362236 90.26942  90.17462  90.07742 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.97511 89.8653  89.74761 89.62202]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.48998 89.35619 89.22064 89.08283]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.94033  88.796455 88.6473   88.48436 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.322845 88.15474  87.971985 87.7835  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.59171  87.3967   87.19305  86.991585]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.79127 86.58438 86.37447 86.16386]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.94753 85.71981 85.48084 85.23716]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.99622 84.7425  84.48016 84.22315]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.96082  83.68997  83.414116 83.13682 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.85574  82.57104  82.286194 82.00156 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.723274 81.45131  81.17823  80.9029  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.61352  80.32761  80.048065 79.77482 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.50617  79.2384   78.966606 78.69303 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.42099  78.151695 77.87952  77.60447 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.33061  77.05725  76.78588  76.518135]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.25264  75.98592  75.717964 75.45032 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.18168 74.91305 74.6522  74.38975]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.124794 73.8583   73.5921   73.325966]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.06307 72.80034 72.53637 72.2714 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.00543  71.738846 71.4726   71.208145]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.94772  70.69093  70.43072  70.168396]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.904305 69.643105 69.384285 69.123825]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.85911 68.6009  68.34069 68.07802]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.81865 67.56689 67.3148  67.06365]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.81734  66.570305 66.32106  66.073685]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.8253  65.57876 65.33788 65.09332]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.84344 64.59872 64.35893 64.11896]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.880417 63.642555 63.39848  63.150414]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.907375 62.66635  62.42561  62.18515 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.949635 61.716995 61.48632  61.26077 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.0391   60.817608 60.59962  60.382984]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.164925 59.944336 59.727737 59.51583 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.300613 59.085983 58.881626 58.676075]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.473103 58.27409  58.07678  57.883175]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.69269  57.505093 57.320717 57.135048]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.946415 56.760014 56.5749   56.38617 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.197426 56.014244 55.836002 55.656784]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.470596 55.28154  55.095192 54.911552]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.72896  54.544205 54.36454  54.187874]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.00583  53.82662  53.650703 53.47128 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.28589  53.101894 52.922424 52.74837 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.577038 52.40669  52.231636 52.053818]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.875572 51.691563 51.50616  51.32062 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.132664 50.944305 50.758068 50.57818 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.400093 50.217873 50.040024 49.874123]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.70406 49.52485 49.34541 49.16654]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.98779  48.820183 48.653973 48.481094]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.31685  48.151535 47.982807 47.80561 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.62648  47.450653 47.276882 47.101795]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.92821  46.75192  46.573627 46.397934]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.225365 46.049908 45.8702   45.693523]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.517025 45.334167 45.152927 44.96998 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.784557 44.595814 44.40207  44.20409 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.003777 43.801613 43.603653 43.407288]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.204494 42.999485 42.800632 42.605446]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.40947  42.211285 42.007168 41.803425]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.599113 41.39289  41.186222 40.980835]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.774456 40.55824  40.337845 40.12269 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.91636  39.705944 39.494587 39.288517]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.079845 38.86812  38.653393 38.434258]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.212032 37.992706 37.777615 37.566753]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.363358 37.1606   36.95777  36.75852 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.54994  36.33714  36.12121  35.909138]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.69986  35.488197 35.268574 35.046463]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.828472 34.61111  34.391144 34.16836 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.93735  33.713783 33.499477 33.2774  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.058674 32.839916 32.622173 32.40957 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.19285  31.971352 31.74843  31.524925]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.30407  31.08474  30.867548 30.654373]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.443314 30.23341  30.025324 29.82136 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.614393 29.405952 29.200804 28.997646]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.796267 28.5958   28.398954 28.204912]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.008175 27.808582 27.61089  27.41709 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.224031 27.028694 26.830942 26.630667]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.427662 26.22269  26.025944 25.836458]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.650984 25.468409 25.287666 25.113281]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.937954 24.764118 24.593185 24.415434]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.238943 24.069149 23.900072 23.728516]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.556765 23.387392 23.221737 23.059437]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.89711  22.734804 22.57546  22.418009]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.263178 22.105152 21.943886 21.788012]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.635468 21.492481 21.344393 21.192387]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.04474  20.90158  20.767038 20.637152]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.506645 20.377132 20.24795  20.123922]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.000322 19.871119 19.738462 19.603544]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.471153 19.341505 19.210762 19.086075]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.96387  18.841225 18.71809  18.59884 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.478241 18.352997 18.226461 18.10781 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.99049  17.86847  17.744558 17.619448]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.491251 17.363136 17.235622 17.10925 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.984543 16.856628 16.7232   16.589994]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.457586 16.32448  16.191742 16.063816]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.938447 15.810544 15.679585 15.547048]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.424781 15.301691 15.175812 15.046996]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.916866 14.78398  14.647029 14.515404]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.385929 14.251532 14.117195 13.983091]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.84563  13.713216 13.583353 13.449592]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.312799  13.173815  13.034878  12.8972225]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.756988  12.612627  12.465518  12.3160925]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.172763 12.032618 11.895657 11.75566 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.606606  11.4574585 11.308141  11.157779 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.009019 10.85798  10.70543  10.546524]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.386877 10.226807 10.062025  9.895034]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.730056 9.562511 9.392485 9.221545]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.057471 8.895613 8.729991 8.563975]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.397012  8.226827  8.049748  7.8759413]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.7054057 7.5342746 7.354643  7.1719213]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.9941545 6.821811  6.6530657 6.484434 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.312449  6.142214  5.973773  5.8071256]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.6439877 5.485768  5.33174   5.1837387]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.3323883 1.4105241 1.4840357 1.5557504]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.6276532 1.7054487 1.7882376 1.8738163]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.9662988 2.058043  2.148239  2.2297876]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.3050861 2.3788059 2.4541955 2.5328386]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6149797 2.6987958 2.7851648 2.869885 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.9510725 3.0306833 3.1107721 3.1901352]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.269944  3.3560357 3.444834  3.5316439]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.616173  3.6975331 3.7784786 3.858786 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.9355893 4.0118465 4.0890274 4.1658816]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.247519  4.3318405 4.4140234 4.494804 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.5784    4.6633463 4.744979  4.821812 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.8934274 4.9636426 5.0353975 5.108068 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.171612  5.2295775 5.290171  5.3538537]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.4191    5.4865303 5.5565453 5.6260624]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.6904273 5.754613  5.8228607 5.891213 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.9558363 6.021704  6.0876627 6.154255 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.2219467 6.289843  6.3550744 6.411518 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.4592266 6.497831  6.5349584 6.5750437]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.6110206 6.6404147 6.65767   6.667599 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.6745663 6.683427  6.690533  6.695623 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.6996703 6.703803  6.7077293 6.70353  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.6966896 6.686902  6.686792  6.6967983]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.7054067 6.7124186 6.718342  6.7232485]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.7245536 6.7211037 6.716405  6.714783 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.7165446 6.716377  6.718173  6.721763 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.722024  6.723578  6.7287836 6.736756 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.744972  6.7515173 6.7591157 6.766888 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.768515  6.771761  6.775309  6.7716756]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.766443  6.762165  6.7576165 6.7541084]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.752815  6.7513204 6.7512555 6.7500086]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.743473  6.7320466 6.716357  6.7044916]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.6892757 6.669205  6.6465626 6.6231065]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.5982738 6.572999  6.5528593 6.5343537]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.505255  6.467641  6.426265  6.3850765]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.347135  6.3112965 6.2717023 6.228976 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.183318  6.129252  6.0728364 6.0193014]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.969989  5.9231925 5.8665566 5.8054414]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.7461963 5.6925235 5.632674  5.568538 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.5027432 5.440625  5.3811283 5.322736 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.262336  5.1971297 5.129454  5.0616736]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.9934816 4.91921   4.8370295 4.7552013]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.6740575 4.5921087 4.5095387 4.428709 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.3486032 4.266861  4.182267  4.0956964]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.012042  3.9299657 3.8455164 3.757389 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.665621  3.5725608 3.4796221 3.3902555]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.30505   3.2208288 3.1411986 3.0664606]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.9865258 2.9047973 2.826402  2.7492824]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6764174 2.611714  2.5431757 2.472078 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.4019568 2.3318362 2.2637813 2.200571 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.1415858 2.0866988 2.0371366 1.9953815]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.9584588 1.9298413 1.9047098 1.8783413]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.8524797 1.8276671 1.8003411 1.7828702]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.7793825 1.7806264 1.7798079 1.7779498]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.778207  1.7814167 1.7859893 1.792075 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.8071245 1.8274415 1.8465407 1.8629893]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.8768282 1.8950648 1.9164803 1.937313 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.9603064 1.9953177 2.036453  2.0798738]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.122114  2.160337  2.1971238 2.2362692]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.2799447 2.327559  2.38133   2.4417994]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.5045424 2.5690734 2.6353874 2.7039962]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.7750976 2.8489795 2.92568   3.0013843]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.0785832 3.157008  3.237338  3.3212483]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.4065459 3.4950366 3.5862622 3.6796055]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.773958  3.867287  3.9570713 4.0432305]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.127158  4.211054  4.3006063 4.3953214]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.488992  4.580195  4.6711435 4.7581615]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.8367944 4.9190226 5.005522  5.09483  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.186171  5.274277  5.3574076 5.4383607]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.517308  5.5971513 5.6769996 5.7562623]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.827718  5.894913  5.9661884 6.039762 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.1131587 6.189613  6.2652626 6.3347907]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.403582  6.4751315 6.5442557 6.608688 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.672711  6.738092  6.805489  6.8736806]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.9406414 7.0077806 7.075938  7.1432533]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.208134  7.2701926 7.3281636 7.384639 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.4402723 7.4970746 7.5567775 7.614767 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.6750207 7.7383485 7.8006597 7.861651 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.9212866 7.9756117 8.02461   8.074768 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.12736  8.183637 8.241728 8.298463]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.353041 8.405238 8.460243 8.51779 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.579373 8.64256  8.697387 8.747836]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.797996 8.849826 8.904151 8.95894 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.01103  9.065008 9.12018  9.17751 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.236134 9.294922 9.349124 9.400686]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.453111 9.505579 9.558296 9.61031 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.660622 9.709721 9.757861 9.802936]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 9.85057   9.906569  9.966411 10.02779 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.084495 10.134163 10.183351 10.23894 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.295038 10.348026 10.398844 10.453106]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.507781  10.561093  10.6126995 10.665151 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.718736 10.770564 10.823007 10.876385]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.928433 10.978319 11.029775 11.085299]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.1437845 11.201157  11.256985  11.314373 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.37382  11.435837 11.496121 11.555333]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.611927 11.665655 11.720957 11.775429]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.830849 11.886785 11.939343 11.990507]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.046541 12.106572 12.1666   12.227188]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.286905 12.344385 12.401704 12.458643]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.51851  12.582404 12.647651 12.711003]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.769145 12.830605 12.894008 12.956919]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.01857  13.076905 13.134961 13.197324]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.260074 13.320873 13.380008 13.438029]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.496181 13.559194 13.622738 13.688165]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.759219 13.831158 13.893969 13.956831]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.019322 14.083257 14.149994 14.216079]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.278419  14.339365  14.398908  14.4537325]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.51138  14.567237 14.623229 14.682552]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.742649 14.803327 14.859109 14.912749]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.965783 15.019395 15.071117 15.120801]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.172236 15.225669 15.28596  15.352945]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.422882  15.490966  15.555088  15.6131935]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.667596 15.727273 15.791039 15.856875]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.922708 15.986616 16.048302 16.10807 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.173424 16.245743 16.317814 16.383657]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.44447  16.508854 16.577534 16.641489]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.70366  16.769487 16.834572 16.894876]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.95199  17.006323 17.065804 17.128382]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.17932  17.226585 17.277815 17.331293]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.381966 17.43087  17.477354 17.52552 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.574722 17.621378 17.669281 17.722479]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.77388  17.82034  17.866274 17.913692]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.954987 17.996597 18.036142 18.072891]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.112545 18.162773 18.212702 18.262526]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.308609 18.350418 18.390293 18.427776]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.467169 18.511349 18.561922 18.61216 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.658878 18.699867 18.739576 18.78373 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.831781 18.880127 18.92852  18.97666 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.024506 19.082787 19.14472  19.21044 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.278152 19.341763 19.401743 19.457373]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.507345 19.55772  19.612156 19.675123]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.73443  19.784422 19.831741 19.880741]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.92957  19.97838  20.032784 20.083988]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.128002 20.177475 20.237326 20.301014]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.363802 20.42329  20.482903 20.548079]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.61445  20.682703 20.752459 20.823053]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.896069 20.965122 21.030657 21.101788]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.170681 21.232561 21.293354 21.360361]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.433777 21.511976 21.5891   21.665155]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.741922 21.81449  21.887545 21.9683  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.052643 22.134882 22.217133 22.302704]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.389893 22.479416 22.568865 22.65485 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.740883 22.830393 22.920736 23.012486]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.110464 23.214027 23.320349 23.423101]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.51919  23.616049 23.716328 23.822079]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.930244 24.039394 24.145164 24.244469]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.34545  24.450144 24.553528 24.654562]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.74641  24.836395 24.922583 25.006704]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.089485 25.171556 25.258232 25.355038]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.45424  25.552103 25.645872 25.741072]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.843159 25.947218 26.049782 26.154594]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.258024 26.356495 26.452726 26.551212]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.647415 26.735476 26.823133 26.913128]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.00312  27.091385 27.174963 27.256702]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.337269 27.420425 27.507483 27.596611]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.686432 27.772526 27.856657 27.944283]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.03428  28.127138 28.219362 28.307411]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.391962 28.474398 28.556425 28.63958 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.722486 28.798773 28.872635 28.950554]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.033258 29.107967 29.178858 29.252329]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.327044 29.396729 29.466515 29.538261]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.609482 29.680573 29.75068  29.816898]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.881042 29.944225 30.00632  30.072033]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.140121 30.208353 30.27525  30.337643]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.398567 30.462837 30.533262 30.605295]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.673872 30.743505 30.814135 30.883963]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.953562 31.024578 31.096533 31.168346]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.239986 31.313967 31.385254 31.453884]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.522661 31.591019 31.658321 31.72389 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.78974  31.858112 31.929518 32.00346 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.076668 32.14916  32.22094  32.29358 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.367004 32.438553 32.508648 32.581814]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.65823  32.734886 32.80875  32.87933 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.948868 33.017555 33.088913 33.158905]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.223232 33.29532  33.368675 33.43849 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.50731  33.57782  33.649788 33.72316 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.8012   33.882557 33.9615   34.04036 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.11903  34.195694 34.270348 34.343803]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.421497 34.501568 34.57965  34.657   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.738735 34.820137 34.897926 34.973045]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.05013 35.1306  35.21304 35.29589]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.380695 35.467697 35.554234 35.640163]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.725586 35.809376 35.895027 35.980427]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.0638   36.145515 36.22535  36.305202]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.384155 36.464855 36.547127 36.628365]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.708973 36.78909  36.86916  36.949505]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.027607 37.10542  37.18373  37.261642]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.339752 37.417576 37.4939   37.56764 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.63797  37.70689  37.781124 37.85809 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.935795 38.011868 38.084694 38.156174]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.228764 38.30017  38.36851  38.43673 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.505318 38.575787 38.64915  38.723225]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.797256 38.86668  38.93457  39.00953 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.079815 39.149887 39.22468  39.300842]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.377777 39.456005 39.52809  39.59409 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.658295 39.730515 39.812294 39.896137]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.979122 40.06013  40.14181  40.222923]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.305225 40.3861   40.468143 40.552776]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.639553 40.72767  40.818245 40.9069  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.992622 41.07655  41.159016 41.24205 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.326157 41.410038 41.494045 41.58101 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.66906  41.756588 41.846046 41.93547 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.02307 42.10934 42.19639 42.28652]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.377678 42.46578  42.552383 42.64176 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.73309  42.825153 42.917797 43.010345]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.1004   43.189095 43.27732  43.364647]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.45164  43.54315  43.64043  43.738857]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.835003 43.924    44.00855  44.089222]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.17033  44.25251  44.338814 44.4293  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.52241 44.61712 44.71142 44.80669]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.903786 45.001366 45.099174 45.195126]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.29071  45.39062  45.492447 45.595665]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.70011  45.81048  45.916218 46.019283]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.126453 46.241505 46.356964 46.472435]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.584335 46.69198  46.80067  46.909424]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.018116 47.120903 47.221607 47.33543 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.450096 47.56453  47.683132 47.80129 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.915188 48.023766 48.133488 48.246056]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.35644  48.460503 48.560085 48.66573 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.77482  48.8832   48.978653 49.06788 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.163055 49.261612 49.363335 49.465366]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.566868 49.66556  49.761066 49.857246]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.955715 50.056965 50.15596  50.25239 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.346634 50.437733 50.5276   50.61949 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.716278 50.815224 50.91424  51.011955]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.10461  51.19358  51.283592 51.375072]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.47003  51.569607 51.674675 51.782997]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.889805 51.99257  52.092213 52.188095]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.279217 52.374435 52.472477 52.56645 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.65748  52.745777 52.833477 52.921062]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.004463 53.084538 53.168983 53.256153]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.333904 53.41214  53.487667 53.555065]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.61824  53.680973 53.74241  53.806767]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.87319  53.9431   54.01485  54.086487]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.159267 54.23167  54.295216 54.36244 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.42631  54.481617 54.534794 54.5907  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.64964  54.707615 54.76417  54.824135]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.892746 54.96677  55.034595 55.09772 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.158455 55.2268   55.29868  55.364605]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.43156  55.496944 55.55963  55.624306]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.692703 55.76484  55.835205 55.901745]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.965015 56.02809  56.092773 56.165806]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.240047 56.31055  56.37378  56.437477]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.50501  56.5727   56.641495 56.716415]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.794518 56.873188 56.949425 57.022083]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.08949  57.15273  57.216717 57.283295]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.3485  57.41503 57.48693 57.56273]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.634182 57.69994  57.766163 57.834858]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.906353 57.980755 58.05467  58.1285  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.206146 58.281906 58.352043 58.424423]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.505817 58.589386 58.67364  58.758617]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.844757 58.932465 59.017548 59.095734]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.17082  59.248573 59.326538 59.404957]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.48571  59.56251  59.636723 59.711502]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.786343 59.863075 59.94228  60.02207 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.099113 60.174404 60.24914  60.322437]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.392033 60.459583 60.528236 60.59935 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.675156 60.753506 60.83107  60.904434]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.97639  61.04995  61.121098 61.19064 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.259144 61.328335 61.39835  61.469585]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.542587 61.615017 61.676323 61.73761 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.805965 61.882877 61.957687 62.02572 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.0925   62.158264 62.221226 62.28179 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.34087  62.40157  62.463085 62.52667 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.589497 62.647263 62.703518 62.76264 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.821312 62.876225 62.929974 62.98241 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.035748 63.090977 63.144665 63.196384]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.24389  63.290646 63.338528 63.384136]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.426136 63.474026 63.528454 63.573135]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.61431  63.655045 63.695896 63.73806 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.779476 63.81962  63.862736 63.904408]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.946095 63.98929  64.03032  64.070465]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.11155  64.154396 64.19953  64.24592 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.292625 64.341    64.391396 64.443184]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.49708 64.55163 64.60396 64.65578]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.710144 64.76764  64.82814  64.88238 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.93323  64.989746 65.0481   65.10511 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.16374  65.225136 65.285065 65.34236 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.39902  65.45703  65.515434 65.57334 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.63428  65.69744  65.757744 65.815186]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.870705 65.926025 65.98366  66.039925]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.10021  66.164024 66.22417  66.28161 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.33779 66.39462 66.45276 66.51305]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.57453  66.636185 66.697174 66.75988 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.82572 66.89513 66.9648  67.03284]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.095345 67.15657  67.22504  67.3003  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.37904  67.45877  67.538734 67.61965 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.69653  67.76524  67.836266 67.91167 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.98961 68.06856 68.14646 68.22103]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.293526 68.365395 68.443306 68.51823 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.5907   68.663536 68.73441  68.807236]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.8797   68.94939  69.019104 69.08907 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.15308 69.21059 69.26386 69.31562]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.37123  69.430176 69.48875  69.54435 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.59719  69.645706 69.697235 69.754814]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.81037  69.862976 69.91572  69.967705]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.016685 70.0652   70.11655  70.169   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.22255 70.27632 70.32752 70.37466]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.42316 70.47638 70.52934 70.58083]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.62958  70.67838  70.73343  70.791885]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.84113  70.889496 70.93777  70.98515 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.03159 71.08344 71.13987 71.19299]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.243996 71.294525 71.34408  71.39497 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.44561 71.49676 71.54771 71.60067]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.65251 71.69342 71.72497 71.75982]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.79819  71.838455 71.879745 71.91709 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.95499  71.99369  72.031906 72.070076]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.10951 72.15102 72.19718 72.24282]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.28372 72.31905 72.35704 72.39738]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.44027  72.489235 72.53406  72.57222 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.612526 72.654655 72.694855 72.73725 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.78451  72.836205 72.88945  72.94108 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.98852  73.02981  73.065765 73.10103 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.13991 73.18748 73.234   73.27808]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.3166   73.350044 73.38392  73.41901 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.45526 73.49166 73.52811 73.56409]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.60086 73.64196 73.6867  73.73436]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.781136 73.82623  73.87173  73.917465]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.96255 74.00481 74.04174 74.08414]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.13017  74.173065 74.2119   74.2603  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.30628  74.34164  74.376625 74.41525 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.456184 74.50819  74.55931  74.60619 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.6515  74.69981 74.75455 74.81542]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.88347  74.950516 75.01035  75.073784]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.13699 75.19589 75.2568  75.31784]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.37766 75.43635 75.49111 75.53502]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.58003  75.62411  75.669975 75.72196 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.779175 75.835266 75.890495 75.94421 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.99042  76.03042  76.075325 76.12022 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.16813  76.22106  76.273186 76.32079 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.36265  76.40675  76.451836 76.49753 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.54852  76.60249  76.65311  76.699585]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.73762  76.78253  76.83013  76.883545]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.94073  76.993095 77.040344 77.09187 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.14717 77.2004  77.25214 77.29457]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.33399  77.37521  77.41365  77.448006]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.48285  77.517784 77.55647  77.59835 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.64274  77.68155  77.713066 77.74413 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.78123  77.822525 77.85358  77.881   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.914085 77.95068  77.99105  78.03404 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.05852 78.07582 78.09336 78.11504]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.13972  78.159294 78.169426 78.180176]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.19815  78.21565  78.22692  78.239845]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.25371 78.26546 78.28182 78.30222]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.32355 78.34595 78.37488 78.39664]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.413795 78.43267  78.45113  78.46189 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.46944  78.47662  78.486885 78.502365]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.52624  78.551575 78.5775   78.59955 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.61554  78.627815 78.63906  78.65925 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.678696 78.69635  78.71249  78.72288 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.741295 78.77022  78.793594 78.81006 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.81839  78.825195 78.83159  78.83913 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.85096 78.86897 78.8855  78.90088]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.919556 78.94041  78.96529  78.98418 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.99859 79.01409 79.03865 79.06426]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.0831   79.101555 79.1225   79.1353  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.14937 79.17712 79.20831 79.23856]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.270294 79.30413  79.330635 79.35242 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.38139  79.40648  79.42251  79.441414]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.46313  79.48612  79.507614 79.530846]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.56026 79.58746 79.60704 79.62367]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.64444  79.670044 79.688    79.70385 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.721214 79.732605 79.73836  79.742424]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.74739 79.75476 79.76764 79.78847]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.81098 79.83417 79.85978 79.88709]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.90564  79.91661  79.928635 79.94022 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.949844 79.95926  79.97001  79.98871 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.01661 80.04387 80.06453 80.08514]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.10359 80.12442 80.15014 80.16877]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.1828  80.20643 80.24112 80.27453]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.30312 80.3352  80.37173 80.4019 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.42871 80.4573  80.4792  80.49214]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.50342 80.52042 80.54738 80.5783 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.605   80.61993 80.63323 80.65848]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.68779  80.705986 80.72824  80.76191 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.78968 80.80598 80.82104 80.83811]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.861885 80.897    80.93159  80.95471 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.98281  81.017876 81.05679  81.09    ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.11823 81.14371 81.1641  81.18232]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.20891  81.238716 81.267876 81.29306 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.31329  81.33905  81.36897  81.403984]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.44574 81.48558 81.52026 81.55413]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.58642 81.61549 81.63961 81.66113]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.68728 81.71932 81.74991 81.78747]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.824776 81.85466  81.882805 81.911   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.93766 81.96283 81.98139 81.99414]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.01235  82.034424 82.053665 82.07216 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.09337  82.11936  82.146935 82.16983 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.18596  82.196785 82.20874  82.225494]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.238716 82.247375 82.26098  82.273926]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.281204 82.29036  82.30455  82.31773 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.3281   82.332565 82.34262  82.35123 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.358345 82.364845 82.36544  82.36285 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.36725 82.36883 82.35916 82.34669]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.347115 82.342064 82.33213  82.31311 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.29011  82.271805 82.25645  82.23193 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.20195 82.17212 82.14909 82.12959]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.11061 82.09173 82.06951 82.04661]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.023766 82.001755 81.97957  81.9582  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.94043  81.923676 81.904526 81.87813 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.85337 81.83814 81.82669 81.8116 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.797905 81.78526  81.77245  81.75461 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.732605 81.714035 81.700806 81.69129 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.681885 81.67359  81.666046 81.658775]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.65025 81.64142 81.63613 81.6334 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.63238 81.63229 81.6312  81.62732]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.62342  81.62081  81.61987  81.622665]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.62732 81.63333 81.64124 81.65266]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.66229  81.667816 81.67453  81.68211 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.690285 81.69905  81.70857  81.71941 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.73493  81.74047  81.75159  81.771736]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.79488  81.81684  81.832985 81.84683 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.86419 81.88497 81.90806 81.92637]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.95165 81.98037 82.01223 82.04509]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.07948 82.11342 82.14851 82.18769]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.22657 82.27106 82.31768 82.36161]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.40346  82.444275 82.484276 82.52574 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.570564 82.61884  82.665565 82.70793 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.74908  82.78927  82.832016 82.87714 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.91783 82.95472 83.00116 83.05224]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.10522  83.159935 83.21337  83.26412 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.31218  83.356285 83.40159  83.45386 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.505165 83.554276 83.60613  83.66503 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.71764  83.76321  83.809525 83.855484]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.89959 83.94309 83.98484 84.02426]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.0632  84.10439 84.14588 84.18475]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.21974  84.24869  84.274574 84.30329 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.339096 84.37733  84.41512  84.4522  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.4888  84.52703 84.5675  84.61224]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.66234 84.69741 84.73159 84.76755]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.80288 84.83656 84.87015 84.90397]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.94041  84.978836 85.01508  85.05047 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.086815 85.12092  85.154045 85.18992 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.22866  85.26678  85.30357  85.339584]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.37119 85.40526 85.45033 85.49818]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.540855 85.580734 85.617744 85.65313 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.6877  85.72058 85.75201 85.7821 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.810616 85.83751  85.866905 85.9015  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.93612  85.9655   85.99114  86.015656]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.04019  86.06313  86.08374  86.104294]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.12604 86.14933 86.17125 86.19541]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.223694 86.251564 86.27184  86.29791 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.32817  86.355095 86.38149  86.40958 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.43998 86.47226 86.50444 86.53505]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.56343 86.59135 86.62158 86.65482]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.69156 86.72783 86.76506 86.79962]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.831314 86.86318  86.895256 86.92693 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.956276 86.98414  87.00514  87.019905]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.04754 87.07301 87.09624 87.12319]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.15235 87.18223 87.21445 87.24627]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.27703 87.31106 87.34716 87.37855]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.409325 87.441185 87.47063  87.50105 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.53459 87.5689  87.60168 87.63218]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.659615 87.67992  87.70002  87.724144]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.7506  87.78039 87.81093 87.84066]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.87156 87.90453 87.93745 87.96995]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.00412  88.03632  88.06659  88.100204]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.13636  88.1741   88.213684 88.24663 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.27743  88.311195 88.34553  88.3773  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.407425 88.43536  88.46066  88.48315 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.50483  88.528015 88.55198  88.57718 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.60308  88.627525 88.650986 88.67682 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.70318  88.72699  88.75274  88.779236]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.8047  88.829   88.85544 88.88496]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.91537  88.944824 88.969215 88.99832 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.0318   89.06531  89.099    89.131485]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.16365 89.19777 89.23125 89.26354]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.298355 89.33442  89.369736 89.40793 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.4487   89.485176 89.516304 89.543594]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.56796 89.59154 89.61909 89.64942]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.67854  89.7064   89.733795 89.76166 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.790794 89.82331  89.85918  89.89315 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.922295 89.94977  89.97963  90.01276 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.04468  90.07455  90.104645 90.133934]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.16262  90.192856 90.22468  90.2568  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.29239 90.32794 90.35703 90.38168]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.40182  90.41645  90.43668  90.461296]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.485405 90.51031  90.53803  90.566734]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.59655 90.62741 90.65908 90.688  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.71377  90.74081  90.76965  90.801506]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.835045 90.867935 90.897285 90.92624 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.95565  90.982346 91.01335  91.04682 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.07853 91.10579 91.12942 91.15289]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.17613 91.20261 91.22423 91.24495]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.26798  91.292366 91.32058  91.34987 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.37533  91.401505 91.431076 91.461334]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.491684 91.522606 91.55201  91.58055 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.61227  91.64512  91.678665 91.71287 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.7465  91.78041 91.81201 91.84366]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.877556 91.91166  91.943245 91.97213 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.998604 92.021805 92.04616  92.07811 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.112785 92.14197  92.170105 92.19948 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.228645 92.25705  92.28444  92.31037 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.33575 92.36105 92.38565 92.40663]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.42587  92.44883  92.473885 92.50072 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.52933 92.55663 92.58444 92.6131 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.641304 92.67002  92.699646 92.72948 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.75951  92.790634 92.82433  92.859695]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.893265 92.92479  92.95342  92.98092 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.0119   93.04623  93.07853  93.111435]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.14483  93.171715 93.1985   93.227715]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.2575  93.28925 93.32143 93.35322]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.38825 93.42577 93.46092 93.49351]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.52472  93.55303  93.579056 93.60824 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.642944 93.68048  93.717865 93.74843 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.77349 93.79847 93.82148 93.84263]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.86826 93.8987  93.92591 93.94682]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.966156 93.98705  94.00896  94.02912 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.04737  94.068634 94.095146 94.11907 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.141    94.166626 94.18925  94.207146]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.2316   94.256874 94.2758   94.293396]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.31317 94.33308 94.3503  94.36723]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.38518  94.397575 94.401726 94.39983 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.39433  94.385895 94.37504  94.36185 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.34558  94.325325 94.302185 94.276436]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.24907  94.21991  94.188774 94.15581 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.1192   94.07641  94.02962  93.978806]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.931725 93.89052  93.83829  93.77373 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.70507 93.63605 93.56303 93.48551]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.403145 93.31666  93.228546 93.13386 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.032486 92.92791  92.82216  92.71665 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.61122 92.50094 92.38927 92.27849]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.16829 92.05463 91.9297  91.79556]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.65627  91.51404  91.374344 91.23522 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.09706  90.96359  90.828735 90.68697 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.54064  90.3968   90.256905 90.11595 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.97398  89.82938  89.68204  89.539536]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.39895 89.25082 89.10209 88.95231]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.79795 88.64396 88.49453 88.34798]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.20361  88.06255  87.917854 87.76568 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.60845  87.454475 87.30042  87.15279 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.01364 86.87728 86.74164 86.60588]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.46668  86.32318  86.183876 86.049225]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.91768 85.78423 85.64569 85.50616]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.368164 85.23242  85.098404 84.96294 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.82732 84.69053 84.55683 84.4225 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.2832   84.15545  84.034676 83.91169 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.78709  83.66185  83.53488  83.416275]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.301735 83.18864  83.08156  82.97939 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.88215  82.78802  82.68721  82.581276]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.47934  82.37923  82.27935  82.179596]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.08149  81.99142  81.906334 81.82393 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.743614 81.66486  81.58909  81.51138 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.42957  81.35095  81.282486 81.218056]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.14242 81.07103 81.00537 80.94221]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.87932  80.821266 80.7615   80.70461 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.65052  80.59649  80.550644 80.51096 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.47407  80.43541  80.38973  80.340805]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.29989 80.26571 80.23713 80.21557]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.19415  80.17119  80.139946 80.10544 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.07673 80.05378 80.03178 80.01449]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.003685 79.991394 79.97888  79.96115 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.94443  79.940414 79.94221  79.94466 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.94478 79.94098 79.93673 79.93304]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.93291  79.93284  79.932495 79.93207 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.9278  79.92255 79.9169  79.91421]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.91747  79.926956 79.93255  79.93309 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.93048 79.92629 79.92363 79.91756]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.904785 79.89512  79.88881  79.88205 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.87516  79.86866  79.8619   79.856255]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.851425 79.84675  79.839645 79.82938 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.819374 79.810646 79.80365  79.795616]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.78376  79.7643   79.740295 79.72441 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.70122 79.66906 79.63149 79.58715]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.530106 79.46061  79.37676  79.28588 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.187584 79.076164 78.95578  78.82698 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.68694  78.546036 78.40201  78.250275]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.09694 77.94452 77.7903  77.63291]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.47847 77.32688 77.17459 77.0203 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.8621   76.704926 76.55047  76.39554 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.23897  76.079666 75.92393  75.76928 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.607285 75.43996  75.27113  75.10356 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.93884 74.77745 74.61165 74.44123]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.26903  74.09047  73.905426 73.7177  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.52771 73.3373  73.14391 72.94713]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.749016 72.55011  72.35106  72.15177 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.95677 71.76048 71.56138 71.36612]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.17496  70.98132  70.784706 70.58676 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.38628  70.18228  69.975746 69.76573 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.551735 69.336235 69.12357  68.9113  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.69326  68.469536 68.243416 68.01884 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.79587  67.572914 67.3522   67.134254]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.91787  66.700485 66.48918  66.28665 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.090965 65.89976  65.71364  65.53607 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.36552  65.19751  65.03333  64.878914]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.72786  64.577324 64.4262   64.274925]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.12243  63.969498 63.818935 63.66548 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.50387  63.33803  63.169567 62.998703]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.823433 62.6439   62.462868 62.277203]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.087334 61.895283 61.69998  61.498787]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.301903 61.1089   60.913208 60.71605 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.5206  60.33311 60.15085 59.96782]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.782787 59.6041   59.430805 59.26065 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.09203  58.926098 58.75926  58.590416]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.42503  58.26281  58.098507 57.929962]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.76025  57.590958 57.419434 57.24443 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.0697   56.890247 56.706028 56.520298]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.333828 56.145935 55.951622 55.74841 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.551304 55.355137 55.15477  54.95612 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.76076  54.567936 54.376144 54.188705]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.006954 53.824833 53.64248  53.45694 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.262947 53.066833 52.870056 52.67334 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.475574 52.27487  52.070515 51.865208]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.65965  51.450012 51.234913 51.013206]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.787525 50.558167 50.32398  50.08501 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.84329  49.599182 49.35557  49.115414]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.87447  48.633076 48.39376  48.15605 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.916447 47.671364 47.4253   47.178818]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.9316   46.685516 46.44231  46.191647]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.93374  45.67343  45.408302 45.138615]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.878876 44.61718  44.358124 44.110184]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.8621   43.611176 43.35999  43.10982 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.859333 42.61198  42.3694   42.12484 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.876793 41.63296  41.392628 41.150845]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.908417 40.670265 40.436653 40.203346]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.978172 39.76033  39.538128 39.313   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.09493  38.886337 38.684483 38.47529 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.262024 38.046474 37.827385 37.60217 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.3711   37.136543 36.89462  36.646248]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.39889  36.153835 35.909428 35.675922]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.45132  35.227802 35.00085  34.77597 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.556614 34.34322  34.130875 33.916164]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.704624 33.50433  33.3084   33.110992]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.91704  32.728886 32.54829  32.369667]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.18576  31.998716 31.810888 31.625097]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.439405 31.249657 31.059402 30.870966]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.687084 30.507101 30.323978 30.13605 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.95216  29.772888 29.593567 29.415817]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.244038 29.076666 28.908451 28.729565]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.552448 28.373777 28.188604 28.002632]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.818684 27.636005 27.449734 27.260101]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.068304 26.873201 26.675829 26.477156]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.27438  26.068674 25.86433  25.66512 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.465132 25.263098 25.063549 24.863873]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.659828 24.461287 24.27348  24.08435 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.89273  23.706345 23.524675 23.34332 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.164652 22.986082 22.811304 22.646585]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.484062 22.321995 22.156944 21.986834]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.812733 21.639729 21.45897  21.26614 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.067503 20.872166 20.67674  20.48325 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.291475 20.102718 19.916784 19.73066 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.539076 19.349699 19.16486  18.979273]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.796404 18.612299 18.428572 18.25042 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.072832 17.8985   17.72742  17.559052]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.396685 17.239603 17.084953 16.930153]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.77584  16.628431 16.490751 16.351408]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.20518  16.06155  15.921096 15.778728]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.637514 15.495769 15.35266  15.209606]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.069867 14.932291 14.79444  14.656174]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.515459 14.377544 14.243907 14.111696]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.980076 13.852186 13.730835 13.606536]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.484898 13.365088 13.242892 13.118745]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.988679 12.852186 12.715918 12.588788]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.467004 12.340841 12.210794 12.079674]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.944152 11.80324  11.663042 11.531192]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.405859 11.280121 11.15052  11.023516]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.89627  10.771143 10.652556 10.537238]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.42503  10.324693 10.235874 10.152198]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.069784  9.98848   9.902252  9.80724 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.712492 9.621227 9.528652 9.43414 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.340883 9.249599 9.156771 9.059295]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.960349 8.863667 8.770544 8.68252 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.594371 8.503536 8.413463 8.324434]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.231552  8.136683  8.0443535 7.955257 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.874443  7.7924366 7.7084355 7.6303997]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.5532827 7.4713383 7.388013  7.304858 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.226944 7.153344 7.081352 7.005279]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.923969  6.8420043 6.763828  6.6895723]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.615754  6.53775   6.4572024 6.3821473]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.3094225 6.234868  6.163973  6.0856514]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.0058365 5.9263015 5.847253  5.7732944]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.705611  5.637872  5.5640616 5.4842095]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.4026294 5.32543   5.2524915 5.1825457]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.116053  5.0504785 4.9775295 4.901352 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.8273673 4.7564726 4.6838627 4.6120744]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.541708  4.468529  4.3931556 4.320198 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.248255  4.170772  4.0941133 4.020607 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.9484785 3.8782847 3.813916  3.746546 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.6747258 3.6054459 3.5390284 3.4719522]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.4059732 3.3400874 3.271747  3.2032661]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.1331725 3.059669  2.9848013 2.911204 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.8418808 2.77374   2.7046146 2.6385608]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.5742517 2.511194  2.4486043 2.3795788]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.3100414 2.24402   2.1791527 2.115884 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.0560668 1.998086  1.9436063 1.8916895]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.8399136 1.788718  1.7397963 1.6936185]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.6469135 1.5982562 1.5515891 1.509133 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.472965  1.4405143 1.4034896 1.3606244]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.3173953 1.2789465 1.2464675 1.2144383]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.1828877 1.1520505 1.1208638 1.0896192]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.0585746 1.0275761 0.998749  0.9716482]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.94325596 0.91525185 0.88974607 0.8666679 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.8435673  0.82198757 0.800792   0.7788979 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7533721  0.72819275 0.7055213  0.6864828 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.67175007 0.6576136  0.64124256 0.62347656]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.60918003 0.5981692  0.5866491  0.5755403 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.56586003 0.5576631  0.55325866 0.55278933]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.5540136  0.5550072  0.55617684 0.5584392 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.5664801  0.57956153 0.5940685  0.61162394]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.63407284 0.65884966 0.6834959  0.706106  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.9858394  0.9849763  0.98632264 0.9910491 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.9973914 1.0032357 1.0130894 1.0246589]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.0385853 1.0601118 1.0847948 1.1133527]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.150036  1.1880368 1.2246407 1.2649859]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.3095753 1.3564799 1.4057959 1.4590335]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.5166476 1.5795481 1.6451155 1.7127918]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.7863855 1.8652928 1.9493862 2.038185 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.130615  2.2218888 2.3157372 2.4224787]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.539504  2.6542523 2.7700942 2.8911293]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.0140543 3.1358893 3.2554185 3.3781145]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.5114326 3.6491146 3.7871814 3.9255776]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.0647063 4.206632  4.3510923 4.4912415]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.6305747 4.7740245 4.924499  5.0819592]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.2424245 5.404601  5.5671625 5.7306123]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.8951893 6.057453  6.2182183 6.3826065]\n",
      "<class 'numpy.ndarray'>\n",
      "[6.55007   6.7186594 6.887972  7.0536523]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.224729  7.400963  7.5759854 7.750039 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.9245334 8.101869  8.282483  8.464327 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[8.644638 8.82437  9.001324 9.173858]\n",
      "<class 'numpy.ndarray'>\n",
      "[9.3485775 9.527585  9.7059965 9.883716 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.065443  10.2495775 10.43366   10.615839 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.793713 10.970394 11.145565 11.330644]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.5254965 11.727198  11.930684  12.134554 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.341257 12.550664 12.761857 12.972014]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.177379 13.382385 13.58901  13.794782]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.99847  14.196079 14.390163 14.584837]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.781722  14.980406  15.1769705 15.369137 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.561811 15.752113 15.936909 16.119862]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.305101 16.501802 16.702702 16.894215]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.080875 17.265684 17.449354 17.631205]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.814663 18.004745 18.199339 18.393507]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.588146 18.777142 18.95918  19.140463]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.323885 19.508612 19.693645 19.877953]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.063276 20.249947 20.437506 20.627771]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.814472 20.999092 21.187698 21.369646]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.54428  21.720854 21.900038 22.080227]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.26053  22.437687 22.613997 22.792233]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.972363 23.154848 23.335686 23.511082]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.68259  23.855412 24.03221  24.20408 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.372955 24.544556 24.716223 24.88891 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.063776 25.236877 25.396845 25.545649]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.6898   25.831024 25.968393 26.103065]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.235577 26.36698  26.499222 26.635942]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.77522  26.911076 27.045372 27.18334 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.327888 27.475584 27.623474 27.774952]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.92665  28.0798   28.23613  28.397575]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.564648 28.729164 28.889776 29.057686]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.226974 29.393616 29.564692 29.740442]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.920542 30.104973 30.29246  30.476044]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.652536 30.832129 31.016245 31.199043]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.381405 31.5646   31.748226 31.93052 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.113903 32.29879  32.483395 32.67473 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.87402  33.070126 33.25703  33.44693 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.642666 33.834972 34.02236  34.212635]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.402813 34.592377 34.7816   34.971107]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.164948 35.362328 35.557743 35.74945 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.9383   36.127956 36.32108  36.515873]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.708023 36.896824 37.08631  37.273792]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.459385 37.647648 37.835938 38.02699 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.21636  38.402866 38.5842   38.762707]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.94278  39.125294 39.308537 39.489265]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.66568  39.837498 40.005993 40.17305 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.34344  40.515293 40.684322 40.85096 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.01517  41.17759  41.334152 41.484924]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.630367 41.77404  41.91689  42.06316 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.21369  42.363827 42.505634 42.643078]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.77924  42.906685 43.03218  43.156387]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.276684 43.396477 43.526028 43.6503  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.77071  43.89232  44.01307  44.131226]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.249935 44.370033 44.487534 44.60279 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.714336 44.823395 44.93652  45.044533]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.14515  45.24964  45.35416  45.454685]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.5581   45.666393 45.7734   45.878826]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.985516 46.093975 46.204338 46.31125 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.40995  46.513813 46.62283  46.73116 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.838528 46.94397  47.04535  47.144726]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.241585 47.33844  47.434624 47.528896]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.622787 47.715233 47.80651  47.904396]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.008606 48.11608  48.221725 48.32169 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.418045 48.512398 48.60709  48.701885]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.7959   48.88721  48.97674  49.063858]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.14528  49.223976 49.30134  49.381374]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.463184 49.544807 49.62614  49.703148]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.77643 49.84437 49.90519 49.96719]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.03942  50.11243  50.174805 50.23199 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.29673  50.363255 50.430542 50.49761 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.562717 50.616055 50.67229  50.730286]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.784897 50.837646 50.894363 50.95377 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.014767 51.075283 51.133656 51.192963]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.256218 51.305256 51.349377 51.40269 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.45843  51.51609  51.573097 51.62995 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.690426 51.753212 51.817223 51.879677]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.940357 51.992332 52.040215 52.096226]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.148113 52.19148  52.233784 52.27656 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.321575 52.37065  52.425476 52.483677]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.542404 52.602886 52.661922 52.71731 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.769833 52.818092 52.8622   52.90204 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.94459  53.00389  53.067394 53.132107]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.197132 53.26123  53.322376 53.3838  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.44695  53.507843 53.567837 53.63211 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.698505 53.768368 53.839584 53.912506]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.98536  54.058453 54.13517  54.214783]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.296898 54.378532 54.45806  54.52947 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.593998 54.66554  54.741592 54.81534 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.88586  54.954433 55.02662  55.105824]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.187687 55.271465 55.354282 55.436317]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.518246 55.594643 55.663517 55.73087 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.80196  55.875732 55.949585 56.021442]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.09627  56.172554 56.245502 56.315018]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.382423 56.45129  56.522305 56.591976]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.66059  56.727955 56.7933   56.857815]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.930237 57.011887 57.09294  57.17601 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.261757 57.346825 57.4302   57.514496]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.596085 57.67577  57.75528  57.834538]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.916897 58.007164 58.098145 58.188644]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.28187  58.378582 58.477173 58.575417]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.65939  58.7405   58.82082  58.901398]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.98383  59.071407 59.160736 59.24917 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.340874 59.43691  59.53529  59.63537 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.730103 59.81979  59.907253 59.99586 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.087418 60.183483 60.281048 60.374176]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.46329  60.553932 60.64621  60.74064 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.83229  60.921062 61.009003 61.096188]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.182735 61.268063 61.34906  61.42713 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.502644 61.579647 61.658054 61.738132]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.821747 61.908752 61.9908   62.068714]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.143723 62.216778 62.29214  62.3673  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.441612 62.516594 62.592175 62.667118]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.741203 62.81415  62.88652  62.958687]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.029285 63.09801  63.165283 63.229595]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.290447 63.34683  63.396976 63.447353]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.501003 63.55589  63.608482 63.662582]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.716713 63.76981  63.822517 63.87265 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.917084 63.9592   63.999966 64.03701 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.069626 64.10089  64.131966 64.164024]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.19794 64.23334 64.26793 64.30064]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.33116 64.36035 64.39242 64.42528]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.45599  64.48438  64.511986 64.543106]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.576096 64.61094  64.6465   64.6786  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.707214 64.73585  64.76831  64.80104 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.83243  64.865906 64.90261  64.94411 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.99296  65.046936 65.10194  65.15741 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.21211  65.26326  65.31464  65.368034]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.42306 65.48015 65.53625 65.59187]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.649765 65.70982  65.77183  65.83701 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.90522 65.97377 66.04391 66.11141]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.175804 66.23924  66.30164  66.36249 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.42084  66.476494 66.53211  66.59003 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.65074  66.71334  66.777794 66.83681 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.89393 66.95324 67.01354 67.07549]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.13582 67.19395 67.24887 67.30186]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.35706 67.41334 67.46939 67.52626]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.584526 67.64302  67.70193  67.75976 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.81672 67.87685 67.93558 67.99156]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.04576 68.09955 68.15742 68.21665]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.27456  68.32928  68.382034 68.432945]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.4822  68.52883 68.5733  68.61623]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.657166 68.698975 68.745575 68.79452 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.83907  68.883514 68.937035 68.989136]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.0358   69.078766 69.116486 69.152885]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.18689  69.219765 69.2519   69.28343 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.31752  69.353065 69.38555  69.41257 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.438225 69.46246  69.484344 69.510765]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.54138 69.57073 69.599   69.62487]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.64655  69.663376 69.684906 69.71122 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.73715  69.76138  69.78206  69.803116]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.82654  69.850945 69.877205 69.902794]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.92716 69.95117 69.98087 70.01586]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.04886 70.07773 70.10293 70.12958]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.15815  70.188194 70.218185 70.24643 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.27696  70.30762  70.33796  70.367615]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.39621 70.42388 70.45297 70.48342]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.51175 70.54097 70.5719  70.60471]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.63965  70.67414  70.709335 70.74545 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.78135  70.8193   70.860016 70.89415 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.92684  70.958725 70.989815 71.01774 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.04192  71.06295  71.083496 71.10918 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.1369   71.159035 71.182014 71.208046]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.23356  71.2548   71.273125 71.29111 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.31035 71.33026 71.34899 71.36671]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.38362 71.40682 71.43784 71.46636]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.495224 71.525696 71.54911  71.56724 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.5842   71.59985  71.616684 71.635765]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.66186  71.68771  71.708626 71.73019 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.75418 71.78325 71.81428 71.84321]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.86852  71.89077  71.912865 71.93481 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.951775 71.966125 71.98128  72.00351 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.03343  72.061455 72.08577  72.10875 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.1315   72.15488  72.17619  72.192505]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.208824 72.22473  72.24052  72.25746 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.278336 72.30114  72.3237   72.34543 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.36943  72.3907   72.40216  72.413086]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.42232  72.429436 72.437004 72.44653 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.45362  72.46006  72.47704  72.498886]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.52191 72.54687 72.57411 72.60076]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.62601  72.649185 72.670166 72.688   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.705444 72.72339  72.73885  72.75761 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.78792  72.817924 72.84367  72.86692 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.88989 72.9138  72.93701 72.95918]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.978584 72.99753  73.01646  73.03259 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.044014 73.054695 73.07017  73.09213 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.11751 73.14457 73.17098 73.19391]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.21637  73.23729  73.257645 73.279335]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.30067 73.32096 73.34183 73.36305]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.38364 73.40379 73.42279 73.44532]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.470856 73.49113  73.50575  73.517075]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.53066 73.55055 73.57375 73.59558]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.61598  73.636734 73.66022  73.68546 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.71131 73.73584 73.75826 73.78114]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.80517  73.82704  73.84755  73.865616]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.88583  73.914505 73.942604 73.97082 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.00035 74.0307  74.06032 74.08725]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.10953  74.129295 74.14803  74.17252 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.19896 74.22227 74.24546 74.26989]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.294975 74.320915 74.34943  74.37842 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.40546  74.432724 74.457535 74.479126]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.50461  74.53378  74.56533  74.595375]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.62535  74.65587  74.686104 74.713806]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.73625  74.75854  74.783066 74.81297 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.84479 74.87275 74.89682 74.91995]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.94296 74.97076 74.99915 75.02552]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.05741  75.09101  75.122345 75.152885]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.1848  75.21673 75.24818 75.28029]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.31345 75.35309 75.39369 75.42808]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.46451 75.50486 75.54371 75.58054]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.61512  75.646355 75.676796 75.70949 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.74593  75.781586 75.81593  75.8488  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.882866 75.92218  75.96142  76.000534]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.04225  76.083664 76.12294  76.1612  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.19896 76.23578 76.27167 76.30526]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.33655  76.37311  76.414474 76.45937 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.50393  76.544945 76.58018  76.61303 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.646576 76.67882  76.71183  76.74617 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.781395 76.8185   76.859215 76.89798 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.936676 76.97731  77.01472  77.04506 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.07495  77.10525  77.136986 77.16961 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.20413  77.241196 77.28064  77.31939 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.35762 77.39766 77.43724 77.47197]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.501724 77.53808  77.57471  77.60761 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.63708  77.66489  77.691696 77.71867 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.74896  77.778694 77.80675  77.83277 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.85866 77.88523 77.91194 77.9389 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.960526 77.98089  78.00369  78.02562 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.04684  78.0705   78.099464 78.12683 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.153366 78.18157  78.20857  78.23316 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.25929  78.28834  78.31667  78.341156]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.3634  78.38417 78.40268 78.42115]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.440475 78.4609   78.483025 78.50473 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.52525 78.54303 78.56034 78.5795 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.593376 78.60749  78.62447  78.644325]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.66801 78.69203 78.71483 78.73454]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.7513   78.769264 78.786896 78.80113 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.81412  78.82827  78.84651  78.865486]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.88146  78.897705 78.92139  78.946594]\n",
      "<class 'numpy.ndarray'>\n",
      "[78.97064 78.99118 79.007   79.02465]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.0423  79.05317 79.06474 79.07991]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.099686 79.12556  79.15581  79.18922 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.21517  79.24117  79.27191  79.302536]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.33361  79.36562  79.39292  79.407974]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.424286 79.45057  79.48294  79.51854 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.55399  79.587685 79.61589  79.64161 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.6679   79.697235 79.72992  79.75893 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.78379 79.80964 79.84107 79.8783 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[79.91674 79.94471 79.97227 79.99918]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.02737  80.05793  80.08686  80.112366]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.13619  80.159164 80.18046  80.20119 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.22437  80.25178  80.283485 80.31882 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.35564  80.38858  80.420296 80.46024 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.49783  80.530045 80.55347  80.57922 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.60866 80.64054 80.67849 80.72481]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.76516 80.79901 80.83879 80.87869]\n",
      "<class 'numpy.ndarray'>\n",
      "[80.90608  80.93004  80.959076 80.99449 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.03354 81.06853 81.10161 81.13222]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.1637  81.20392 81.24335 81.27547]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.31255 81.35142 81.38741 81.41681]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.43838  81.45918  81.487015 81.51706 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.54725  81.572426 81.59448  81.61861 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.6427  81.66745 81.70547 81.74785]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.77806 81.80005 81.82037 81.84218]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.86653 81.89609 81.92838 81.96233]\n",
      "<class 'numpy.ndarray'>\n",
      "[81.99638  82.023994 82.051544 82.07974 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.10513 82.1295  82.15068 82.17206]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.19983 82.22744 82.25731 82.28529]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.31419  82.35161  82.38989  82.423836]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.45795 82.49557 82.53159 82.5619 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.58703 82.60955 82.63154 82.65452]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.681885 82.70926  82.7274   82.745766]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.76749  82.79458  82.816895 82.83771 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.85487  82.86706  82.880844 82.90445 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[82.927864 82.94817  82.97405  83.00876 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.04471 83.07559 83.10235 83.12334]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.14017 83.16181 83.18543 83.20938]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.227776 83.24334  83.2656   83.28871 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.31091  83.33832  83.37063  83.399254]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.42376  83.441956 83.45779  83.47246 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.49126  83.51549  83.54401  83.577354]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.61044  83.63493  83.651535 83.669876]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.68975 83.70874 83.72699 83.7442 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.76285 83.78621 83.80603 83.82038]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.83815  83.857895 83.87944  83.89932 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.91688  83.93409  83.951294 83.972595]\n",
      "<class 'numpy.ndarray'>\n",
      "[83.99271 84.01324 84.03304 84.05029]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.06395 84.07834 84.09558 84.11314]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.13035  84.14971  84.170555 84.190025]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.207825 84.22545  84.24635  84.27556 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.30965 84.3458  84.38109 84.4143 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.44413 84.4708  84.49557 84.52287]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.5532  84.57842 84.60025 84.62602]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.64989  84.66697  84.682    84.696884]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.71352 84.73232 84.7524  84.77344]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.78839  84.80435  84.824745 84.84659 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.86837 84.8912  84.91675 84.94796]\n",
      "<class 'numpy.ndarray'>\n",
      "[84.97669  85.00067  85.02977  85.063034]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.09456  85.12493  85.15849  85.193184]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.22199  85.243515 85.26384  85.28451 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.304214 85.32893  85.35586  85.3805  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.40811 85.44044 85.47348 85.50371]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.52329 85.54128 85.56378 85.58799]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.61353  85.64074  85.669235 85.69752 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.72643  85.75638  85.788284 85.82237 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.85268 85.87629 85.89815 85.91935]\n",
      "<class 'numpy.ndarray'>\n",
      "[85.938644 85.958145 85.97763  85.99837 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.0215  86.04383 86.06772 86.09584]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.123955 86.14962  86.1669   86.178604]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.18727  86.19586  86.207146 86.224236]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.24725  86.269135 86.29004  86.31258 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.3368  86.35954 86.38557 86.41458]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.440414 86.46313  86.4873   86.51038 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.53095  86.55374  86.57976  86.607414]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.63505  86.658646 86.682106 86.708336]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.733475 86.755264 86.77209  86.78834 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.80244  86.815475 86.82865  86.84351 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.86264  86.877975 86.89086  86.90777 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[86.92674 86.94457 86.96561 86.98933]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.01083  87.03288  87.057396 87.08003 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.10112  87.12229  87.143364 87.168274]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.19511  87.22286  87.250946 87.27786 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.30335 87.32788 87.35184 87.37459]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.39573  87.416916 87.44066  87.46483 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.4931  87.52612 87.56156 87.59654]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.63055 87.66465 87.69513 87.72207]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.75215  87.78325  87.814064 87.84625 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[87.876236 87.90938  87.94736  87.98621 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.02431 88.06132 88.09648 88.12865]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.16125  88.194664 88.22681  88.25686 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.28629 88.31531 88.34408 88.37558]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.41003 88.44376 88.47946 88.51477]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.54871 88.58095 88.61189 88.64201]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.67512  88.70926  88.74106  88.771965]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.803406 88.83562  88.86869  88.904335]\n",
      "<class 'numpy.ndarray'>\n",
      "[88.93957 88.97393 89.0079  89.03902]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.07314  89.11169  89.1496   89.183235]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.2153   89.24702  89.281845 89.3177  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.35362  89.389786 89.42258  89.452   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.480736 89.51018  89.541336 89.57688 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.61414 89.64713 89.68068 89.71407]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.74623  89.777596 89.808975 89.84204 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.87601  89.903046 89.9266   89.94982 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[89.97369 90.00156 90.03414 90.06803]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.10014 90.13171 90.16594 90.20095]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.23664 90.27118 90.3047  90.34261]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.37997  90.412155 90.44327  90.47427 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.503235 90.53016  90.56071  90.59211 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.61896 90.64432 90.67114 90.69575]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.71775 90.74076 90.7657  90.79068]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.81572  90.8396   90.8659   90.894646]\n",
      "<class 'numpy.ndarray'>\n",
      "[90.92441  90.954796 90.983765 91.0108  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.0384   91.06737  91.09758  91.127914]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.15731  91.18498  91.212944 91.24065 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.271614 91.307236 91.339424 91.3669  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.39177  91.41752  91.446976 91.4776  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.51024  91.539734 91.567635 91.595764]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.62569 91.65931 91.69158 91.72095]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.7491   91.776405 91.80045  91.821915]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.847664 91.87283  91.89674  91.91994 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[91.94383 91.96694 91.98924 92.01325]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.03908  92.06481  92.09252  92.120255]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.14207  92.16224  92.18533  92.210846]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.23752 92.26191 92.28394 92.3019 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.317184 92.331566 92.344536 92.35796 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.37574 92.39641 92.41218 92.42553]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.43928 92.44959 92.4597  92.47869]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.50311 92.52352 92.54311 92.56405]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.58124 92.59456 92.60933 92.62584]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.64313  92.660355 92.67648  92.693054]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.71161  92.734634 92.76097  92.782196]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.800514 92.81814  92.83528  92.85149 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.86801  92.887474 92.90824  92.925   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[92.93793  92.954666 92.97473  92.99465 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.01417  93.031334 93.04634  93.06198 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.08023  93.09898  93.116455 93.13255 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.14759  93.16422  93.183426 93.20349 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.223404 93.243835 93.26468  93.2881  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.31449  93.338196 93.36051  93.38166 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.406296 93.433075 93.45304  93.47204 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.4931   93.514206 93.534454 93.55835 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.58426 93.60405 93.61948 93.63569]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.65259  93.66968  93.688675 93.70882 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.730064 93.75192  93.7743   93.794106]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.81026  93.830894 93.85448  93.87969 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.905266 93.93055  93.95316  93.97406 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[93.99685 94.01999 94.04258 94.06607]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.08966  94.11245  94.135475 94.157394]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.17848  94.2001   94.221535 94.24154 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.26031  94.2811   94.304054 94.32653 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.34642  94.36668  94.387985 94.40951 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.430244 94.4522   94.47565  94.49834 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.51998 94.53984 94.55647 94.57254]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.590645 94.610504 94.629265 94.6481  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.667625 94.687546 94.70744  94.72646 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.74485  94.76587  94.78949  94.814026]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.837944 94.861145 94.88116  94.89909 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.91916  94.94084  94.963905 94.983154]\n",
      "<class 'numpy.ndarray'>\n",
      "[94.99688 95.01302 95.0314  95.05689]\n",
      "<class 'numpy.ndarray'>\n",
      "[95.08452  95.1022   95.11464  95.127014]\n",
      "<class 'numpy.ndarray'>\n",
      "[95.14345  95.16393  95.187294 95.21295 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[95.238846 95.26441  95.288025 95.31125 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[95.334564 95.35557  95.37581  95.39882 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[95.42398  95.45109  95.47794  95.503586]\n",
      "<class 'numpy.ndarray'>\n",
      "[95.52997 95.55516 95.5774  95.60065]\n",
      "<class 'numpy.ndarray'>\n",
      "[95.62743 95.65343 95.67778 95.70362]\n",
      "<class 'numpy.ndarray'>\n",
      "[95.7297   95.754906 95.78091  95.808975]\n",
      "<class 'numpy.ndarray'>\n",
      "[95.837105 95.865204 95.89398  95.922806]\n",
      "<class 'numpy.ndarray'>\n",
      "[95.94859  95.97602  96.004715 96.034386]\n",
      "<class 'numpy.ndarray'>\n",
      "[96.06244 96.08998 96.11675 96.14139]\n",
      "<class 'numpy.ndarray'>\n",
      "[96.16584  96.19115  96.216064 96.23958 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[96.26378  96.28994  96.31618  96.343796]\n",
      "<class 'numpy.ndarray'>\n",
      "[96.37312 96.40196 96.4285  96.45469]\n",
      "<class 'numpy.ndarray'>\n",
      "[96.480934 96.50987  96.53975  96.567986]\n",
      "<class 'numpy.ndarray'>\n",
      "[96.5925  96.61363 96.63425 96.65466]\n",
      "<class 'numpy.ndarray'>\n",
      "[96.67603  96.69835  96.717636 96.73909 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[96.76345  96.78744  96.81178  96.835884]\n",
      "<class 'numpy.ndarray'>\n",
      "[96.85817  96.87952  96.896935 96.91587 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[96.94568  96.97986  97.010635 97.04137 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[97.073   97.10122 97.12518 97.15062]\n",
      "<class 'numpy.ndarray'>\n",
      "[97.17804 97.20612 97.23191 97.25543]\n",
      "<class 'numpy.ndarray'>\n",
      "[97.27796  97.2997   97.32218  97.345726]\n",
      "<class 'numpy.ndarray'>\n",
      "[97.370346 97.39519  97.418816 97.443756]\n",
      "<class 'numpy.ndarray'>\n",
      "[97.47235  97.502655 97.532616 97.55837 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[97.58415  97.61188  97.63873  97.664085]\n",
      "<class 'numpy.ndarray'>\n",
      "[97.690575 97.71816  97.74317  97.76735 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6345782 2.6339421 2.633478  2.6330404]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.631147  2.62864   2.6273184 2.6260333]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6246545 2.622953  2.6203833 2.6178756]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6153193 2.6126404 2.610633  2.6097717]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6099381 2.6101103 2.6089976 2.607906 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6068559 2.607419  2.6076634 2.6056056]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6032493 2.6026769 2.60227   2.601935 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6014552 2.6008754 2.6002953 2.6001735]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.599809  2.5990462 2.5979297 2.5983446]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.5983775 2.59623   2.592946  2.58922  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.5847282 2.580563  2.577703  2.576377 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.575168  2.57403   2.573226  2.5727422]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.5770762 2.5812132 2.584522  2.5858688]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.5863044 2.5885272 2.5932136 2.6002655]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6076832 2.6140137 2.6169448 2.6171663]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6190066 2.6226397 2.6275296 2.6315513]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6341863 2.6364098 2.637606  2.6378248]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6384444 2.639925  2.6414888 2.6440835]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.64495   2.6438518 2.642098  2.6432521]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6481395 2.6561594 2.6691804 2.6879282]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.7106261 2.7347462 2.762062  2.7982824]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.8439271 2.8990993 2.965688  3.0445268]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.1389596 3.24984   3.3781593 3.5180805]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.6683204 3.8291283 4.031052  4.249471 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[4.4841657 4.7618155 5.062427  5.3816466]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.7178226 6.0751176 6.4543443 6.8636374]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.304176  7.8076468 8.34103   8.904412 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 9.495548  10.1034565 10.726458  11.359877 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.998815 12.638411 13.272361 13.897302]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.504474 15.094362 15.669796 16.230629]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.74896  17.226522 17.679415 18.094208]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.44966  18.78003  19.086288 19.347641]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.583591 19.80421  20.009033 20.194592]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.382025 20.57661  20.785324 21.009459]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.259222 21.54104  21.857582 22.208048]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.586891 22.987455 23.410583 23.883116]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.402983 24.948013 25.527466 26.166233]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.822023 27.49304  28.195124 28.896688]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.597258 30.287323 30.957825 31.61208 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.25216  32.849316 33.40503  33.920776]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.38157  34.78854  35.146214 35.451176]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.70251  35.895836 36.034985 36.135944]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.20401  36.2166   36.203285 36.169235]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.087856 35.986942 35.87649  35.756027]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.6261   35.511448 35.385803 35.249294]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.080475 34.910877 34.740406 34.575962]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.42674  34.290176 34.170204 34.072475]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.990032 33.928486 33.897625 33.904613]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.936325 33.983967 34.04606  34.145718]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.279037 34.425537 34.58894  34.77472 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.954746 35.138832 35.34133  35.511147]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.669025 35.81504  35.940254 36.024456]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.07333  36.090984 36.07762  36.031048]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.949017 35.837055 35.701374 35.549953]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.3998   35.258133 35.09961  34.927242]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.76989  34.618664 34.45232  34.2984  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.153282 34.003822 33.91266  33.83594 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.782032 33.762684 33.75838  33.7648  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.802956 33.85534  33.91302  33.988598]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.069454 34.150154 34.23057  34.30414 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.372322 34.437473 34.49119  34.534203]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.5723   34.604565 34.636887 34.672638]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.712433 34.755047 34.81904  34.897305]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.964104 35.044052 35.141693 35.26037 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.403267 35.5487   35.69237  35.839474]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.988735 36.13486  36.26371  36.381733]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.477608 36.537262 36.558712 36.54535 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.499584 36.41509  36.29699  36.147995]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.974293 35.78624  35.60466  35.389545]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.15408  34.922962 34.725212 34.512886]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.32382  34.172855 34.07327  34.013897]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.983665 34.008125 34.070625 34.161285]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.279118 34.45001  34.644787 34.854313]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.07826  35.300182 35.51084  35.697365]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.855297 35.97362  36.06264  36.11867 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.13705  36.078228 35.97398  35.825962]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.63176  35.359737 35.059177 34.73755 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.379356 34.00937  33.64327  33.293575]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.965965 32.67818  32.437466 32.25632 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.137184 32.081234 32.089554 32.150417]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.25588 32.39488 32.5937  32.83356]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.09821  33.357403 33.609596 33.856293]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.09669  34.302235 34.45926  34.552784]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.599415 34.5991   34.53975  34.414272]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.23357  34.012203 33.74433  33.443756]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.122017 32.788593 32.45573  32.13794 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.848242 31.59888  31.397469 31.245817]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.141544 31.084646 31.109554 31.25129 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.455448 31.722027 32.1016   32.559258]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.07983  33.63411  34.221752 34.837524]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.4531   36.097485 36.7533   37.40675 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.0278   38.634758 39.213512 39.754   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.26587  40.747158 41.19934  41.612835]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.9946   42.33896  42.663277 42.972218]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.273422 43.536488 43.765717 43.9834  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.2338   44.463505 44.674023 44.89235 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.113056 45.326317 45.536217 45.745483]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.95666 46.1747  46.39658 46.62868]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.86714  47.106133 47.344696 47.58679 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.852097 48.113773 48.37785  48.648125]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.93199  49.21847  49.506443 49.799023]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.145885 50.511868 50.910328 51.37678 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.868835 52.395298 52.971535 53.577652]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.208904 54.86675  55.531876 56.210327]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.895275 57.573875 58.234806 58.872627]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.471413 60.008358 60.490074 60.915077]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.28513  61.601665 61.807858 61.960274]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.060078 62.073555 61.983364 61.83355 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.640697 61.423347 61.195198 60.94865 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.66917  60.405018 60.15813  59.92681 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.725384 59.561638 59.43128  59.341557]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.306366 59.312984 59.37256  59.473812]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.626854 59.804924 60.000298 60.212414]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.483765 60.778088 61.052208 61.331627]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.62757  61.94087  62.271    62.594173]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.880093 63.16149  63.430412 63.66904 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.872684 64.03579  64.15186  64.22574 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.28015  64.319534 64.355446 64.374306]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.37941  64.372665 64.36115  64.36641 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.40225  64.445816 64.48864  64.53557 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.61473  64.695206 64.73178  64.76663 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.7914  64.78928 64.77545 64.74783]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.714134 64.677376 64.62038  64.55079 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.47465  64.385086 64.29559  64.211784]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.14817  64.09429  64.04608  63.999817]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.958588 63.941925 63.93598  63.949036]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.98543  64.06867  64.18547  64.343185]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.5267   64.72264  64.928764 65.13945 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.358215 65.58394  65.794174 65.984184]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.17775  66.34468  66.482315 66.5744  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.63419  66.64868  66.62712  66.566536]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.479965 66.3685   66.224754 66.04542 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.855865 65.680405 65.49799  65.31067 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.11804 64.92299 64.75308 64.64114]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.547935 64.48191  64.442566 64.45154 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.506065 64.60293  64.74592  64.91399 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.11523 65.33626 65.56994 65.80614]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.04791  66.29326  66.51754  66.713524]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.902756 67.07843  67.18999  67.241356]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.26181 67.26708 67.22055 67.1545 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.08128 66.99614 66.88387 66.75246]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.6185   66.46097  66.300095 66.151245]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.01703 65.87813 65.7469  65.63037]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.53829 65.48194 65.47441 65.51995]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.60129 65.717   65.86094 66.03102]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.22803 66.43678 66.62702 66.79804]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.96277 67.15503 67.33722 67.48097]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.605354 67.700165 67.763245 67.76966 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.75069  67.70453  67.638626 67.54621 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.44311 67.33486 67.21781 67.0942 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.962944 66.8355   66.71826  66.622   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.54592  66.49732  66.475266 66.477806]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.507256 66.54953  66.60809  66.71948 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.856346 67.017    67.219734 67.43705 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.66145 67.8836  68.10907 68.3123 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.49037  68.64759  68.797356 68.91327 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.9944  69.03536 69.04281 69.01799]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.96771  68.868706 68.730865 68.575485]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.40389 68.25272 68.0626  67.86262]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.66618 67.49559 67.30554 67.11482]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.9862   66.89757  66.844345 66.83093 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.890915 67.00724  67.16969  67.3816  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.64144 67.9311  68.24037 68.5623 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.8853   69.19841  69.49996  69.788414]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.03396  70.257996 70.45041  70.5875  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.634346 70.63935  70.60379  70.505516]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.33838 70.13162 69.89295 69.62913]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.35272  69.06965  68.77735  68.474365]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.18095 67.90447 67.64884 67.4171 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.180565 66.96251  66.75785  66.56785 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.3836   66.19832  66.01147  65.816605]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.61364  65.392845 65.11768  64.80706 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.46255  64.04188  63.5716   63.066444]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.523666 61.926537 61.2878   60.615284]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.90853  59.15818  58.387913 57.600872]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.810402 56.019684 55.231766 54.446648]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.698864 52.985287 52.290886 51.61187 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.94299  50.32378  49.724846 49.137188]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.56122  48.036385 47.54507  47.053127]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.548428 46.0211   45.474873 44.885864]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.2564   43.589043 42.870956 42.09751 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.276333 40.408596 39.49679  38.54242 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.55766  36.54254  35.48813  34.422623]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.34498  32.27071  31.215956 30.18402 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.167303 28.168144 27.196436 26.292082]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.42622  24.597979 23.807909 23.051514]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.327967 21.650797 21.053465 20.491137]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.960194 19.462252 19.00051  18.598568]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.228163 17.897692 17.614056 17.368883]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.17095  17.026213 16.92135  16.85629 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.836016 16.8606   16.930355 17.026411]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.1497   17.303518 17.506933 17.741331]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.012918 18.343445 18.703407 19.089977]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.515846 19.979868 20.471048 20.990534]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.544624 22.124928 22.729322 23.361809]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.017336 24.69684  25.403988 26.140305]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.895187 27.665173 28.447498 29.241745]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.047285 30.848087 31.643768 32.441677]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.21311  33.969013 34.70907  35.421043]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.093647 36.752335 37.39479  38.019665]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.618916 39.19356  39.685604 40.116592]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.508045 40.86437  41.188675 41.49094 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.768726 42.030983 42.287567 42.543797]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.79605  43.065105 43.353085 43.65959 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.99121  44.351913 44.761997 45.21563 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.69092  46.155315 46.651638 47.17563 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.71342  48.26234  48.83459  49.432457]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.04725  50.664948 51.2656   51.838604]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.362812 52.84818  53.290424 53.677494]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.007465 54.27031  54.480858 54.635483]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.732727 54.74665  54.70251  54.60554 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.4334   54.17277  53.87559  53.529167]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.137825 52.72931  52.306553 51.882748]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.47617  51.07635  50.686874 50.3357  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.01757  49.72654  49.465412 49.240227]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.067333 48.94354  48.854305 48.789944]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.758385 48.744415 48.74481  48.781677]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.83827  48.90844  49.004623 49.10939 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.201416 49.29007  49.359207 49.39983 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.444214 49.554626 49.681232 49.81532 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.96061  50.120064 50.302986 50.516697]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.755775 51.023827 51.32484  51.663452]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.028522 52.41794  52.830803 53.259865]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.699688 54.113644 54.50099  54.881912]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.25287  55.593327 55.90203  56.187973]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.424145 56.596573 56.704514 56.74749 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.708813 56.613304 56.47071  56.292316]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.076233 55.828022 55.556698 55.27665 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.00257  54.74682  54.52149  54.338985]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.217426 54.15388  54.147667 54.204514]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.37651  54.60322  54.87885  55.203964]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.62439  56.067543 56.51261  56.959015]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.437508 57.90684  58.33834  58.72361 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.054726 59.330486 59.53833  59.66526 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.717297 59.701637 59.622005 59.475243]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.282284 59.057613 58.809364 58.536755]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.249676 57.956367 57.686356 57.442413]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.227333 57.060505 56.959225 56.93739 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.96915  57.086014 57.297596 57.571014]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.900894 58.28127  58.690952 59.116295]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.538265 59.941574 60.30604  60.619934]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.87953 61.08104 61.22285 61.30548]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.334167 61.314064 61.228176 61.058544]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.811733 60.510284 60.17526  59.838802]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.505592 59.1791   58.870815 58.605495]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.41046  58.284092 58.232723 58.26327 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.372578 58.569244 58.84213  59.183468]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.577904 60.013542 60.45576  60.885864]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.291664 61.662296 61.986897 62.241127]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.43021  62.557064 62.62522  62.575756]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.476448 62.322346 62.10115  61.772087]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.41232  61.028683 60.632637 60.24153 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.870144 59.536545 59.25699  59.035637]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.88107  58.79961  58.79566  58.856407]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.97061  59.139797 59.364773 59.629486]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.908527 60.19815  60.48632  60.758286]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.005795 61.228848 61.356384 61.42256 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.424034 61.330727 61.161354 60.925106]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.61028  60.204704 59.765476 59.303024]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.830574 58.370808 57.935345 57.535263]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.183765 56.891857 56.658566 56.482872]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.364758 56.32234  56.372017 56.50624 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.70494  56.943893 57.202633 57.47983 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.762897 58.03718  58.29777  58.51531 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.659447 58.751038 58.788914 58.753395]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.636116 58.443756 58.181103 57.85885 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.496346 57.11556  56.72831  56.354233]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.002216 55.68956  55.429665 55.24964 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.13388  55.06663  55.044983 55.128025]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.274017 55.475426 55.74338  56.041782]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.363598 56.692482 57.021652 57.328365]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.603188 57.817265 57.961544 58.039482]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.049843 57.993668 57.872616 57.67852 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.412205 57.07579  56.677666 56.247494]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.802002 55.346336 54.870255 54.360588]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.843166 53.33877  52.835075 52.34715 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.88202  51.432514 51.009167 50.612568]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.260162 49.903183 49.546696 49.18024 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.79601  48.384995 47.940834 47.45724 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.926064 46.348454 45.731102 45.07529 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.38019  43.65025  42.890003 42.105656]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.30173  40.49215  39.676426 38.8634  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.057407 37.261066 36.480644 35.740566]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.016556 34.312897 33.648365 33.0186  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.42194  31.856592 31.31683  30.791885]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.287554 29.795616 29.306898 28.822357]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.32992  27.829212 27.296366 26.748898]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.199026 25.664799 25.096376 24.526577]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.947817 23.37157  22.823895 22.314913]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.848349 21.418905 21.03305  20.689026]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.399864 20.18646  20.044098 19.969257]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.964848 20.0373   20.176304 20.380285]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.655521 21.027784 21.47288  21.963356]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.497759 23.09213  23.759777 24.468624]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.214392 25.997597 26.795038 27.605225]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.440928 29.274195 30.102203 30.926962]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.747126 32.548065 33.3267   34.080364]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.770226 35.436253 36.072636 36.668575]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.235947 37.776314 38.281487 38.75778 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.212643 39.644005 40.051083 40.4383  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.808567 41.161037 41.497826 41.82613 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.146313 42.46225  42.77646  43.09336 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.409424 43.737602 44.076637 44.42394 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.78726  45.167557 45.56218  45.965267]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.384277 46.818573 47.28869  47.787888]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.313805 48.87634  49.46509  50.076656]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.68716  51.286533 51.886517 52.49221 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.098125 53.678402 54.2392   54.780823]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.299713 55.792225 56.25795  56.695232]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.085213 57.40473  57.66552  57.866535]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.998123 58.075924 58.09638  58.05038 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.9106   57.703773 57.444668 57.11893 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.720722 56.269012 55.765125 55.209206]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.610855 53.978798 53.33104  52.668674]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.997627 51.30407  50.620106 49.94994 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.3188   48.729885 48.186523 47.74748 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.370815 47.05197  46.80122  46.664944]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.605072 46.617863 46.726376 46.927635]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.19109  47.517235 47.949963 48.44823 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.00577  49.61375  50.25663  50.927757]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.61341  52.311676 53.01968  53.71837 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.390537 55.036304 55.650223 56.220413]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.743477 57.191822 57.563313 57.868484]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.099102 58.204018 58.21805  58.15396 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.001877 57.747566 57.365807 56.879803]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.314983 55.680523 54.991447 54.267223]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.518387 52.73927  51.93884  51.137577]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.349415 49.60086  48.91369  48.279713]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.710464 47.198666 46.744415 46.359795]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.092308 45.903362 45.795708 45.784107]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.851234 45.986523 46.201286 46.493946]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.841053 47.24535  47.700905 48.193687]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.712185 49.23183  49.750717 50.264473]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.788593 51.27922  51.734016 52.135773]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.490597 52.802116 53.044323 53.176807]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.258854 53.290466 53.186485 53.004585]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.76859  52.45405  52.059784 51.632633]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.172714 50.64778  50.098972 49.553883]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.019608 48.503193 48.02195  47.584126]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.186237 46.830627 46.539    46.3035  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.119564 45.98813  45.95123  45.993446]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.114433 46.304882 46.536316 46.809784]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.14704  47.510376 47.887424 48.27627 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.688404 49.14718  49.630646 50.110126]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.576847 51.02058  51.427742 51.78852 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.08818  52.32618  52.52326  52.672764]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.753426 52.692604 52.560257 52.368034]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.09423  51.7792   51.425404 50.99784 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.551083 50.082592 49.582504 49.073692]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.565544 48.06192  47.57512  47.11827 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.69428  46.320705 46.016354 45.77496 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.600716 45.496544 45.465958 45.50354 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.606033 45.766354 45.985905 46.25779 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.578003 46.935135 47.36008  47.787937]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.225677 48.698162 49.143692 49.57686 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.994465 50.377518 50.7114   51.009632]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.25559  51.419823 51.482677 51.459347]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.35266  51.164337 50.911648 50.601143]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.24086  49.83264  49.395287 48.941273]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.470592 47.957363 47.465332 46.981625]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.517704 46.123383 45.770756 45.46391 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.242954 45.092106 44.989597 44.952827]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.988094 45.07219  45.208412 45.39137 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.626293 45.892536 46.182747 46.505527]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.84376  47.166786 47.477726 47.7716  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.09128  48.39927  48.700462 49.016514]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.229187 49.401417 49.537895 49.630123]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.684845 49.703888 49.67173  49.598038]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.493866 49.365894 49.24091  49.096416]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.927296 48.72847  48.514324 48.29774 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.079456 47.859352 47.6419   47.444324]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.253353 47.06687  46.885563 46.73902 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.604206 46.491226 46.39919  46.316563]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.245228 46.217186 46.18784  46.157726]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.141678 46.107193 46.07823  46.057846]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.073345 46.051605 46.033653 46.033783]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.03965  46.040337 46.0384   46.04162 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.045143 46.06549  46.104298 46.168446]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.24274  46.281475 46.31688  46.34862 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.36454  46.39878  46.422832 46.406116]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.380028 46.36743  46.359432 46.34702 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.352867 46.36112  46.371563 46.37352 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.37652  46.385204 46.388428 46.400543]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.41368  46.428463 46.452755 46.494396]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.53269 46.56582 46.59937 46.71855]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.861755 47.020657 47.20968  47.42463 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.66662  47.93873  48.255272 48.611565]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.975403 49.329807 49.717495 50.107292]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.493958 50.87916  51.248676 51.593376]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.903698 52.175797 52.39904  52.58701 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.73297 52.83179 52.84891 52.82285]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.7531   52.6428   52.50158  52.344673]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.17977  52.006535 51.81557  51.637234]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.502884 51.37877  51.28522  51.222248]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.278896 51.419865 51.62497  51.889988]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.213566 52.604294 53.041157 53.518867]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.010353 54.506992 54.991455 55.464214]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.914886 56.34315  56.739025 57.065296]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.316334 57.525    57.69129  57.800472]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.789017 57.6949   57.536903 57.32121 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.068153 56.78544  56.473957 56.15109 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.834213 55.5384   55.277676 55.067284]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.904892 54.805355 54.774918 54.817947]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.93157  55.110363 55.35683  55.668747]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.013573 56.373264 56.761364 57.172935]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.577404 57.938675 58.247032 58.51383 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.726192 58.852493 58.914288 58.913177]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.83985  58.682858 58.464302 58.197166]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.8798   57.53242  57.175053 56.816547]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.455822 56.11665  55.809223 55.543266]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.330307 55.1699   55.09347  55.06886 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.11396  55.307858 55.552326 55.84745 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.21169  56.601185 56.99886  57.38579 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.73241  58.033623 58.290638 58.474323]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.569954 58.58406  58.515568 58.366   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.14135  57.852142 57.50152  57.105946]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.67872  56.238094 55.79531  55.371788]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.968063 54.593185 54.24715  54.03541 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.91035  53.849422 53.868008 53.98324 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.151917 54.383827 54.682903 55.021282]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.38017  55.742348 56.096848 56.431713]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.724964 56.969425 57.157604 57.287556]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.3494   57.31939  57.202877 57.035664]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.829857 56.571915 56.247242 55.883896]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.516693 55.149605 54.81536  54.505497]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.2307   54.039143 53.901207 53.82291 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.834362 53.92787  54.095547 54.344673]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.66631  55.046898 55.486073 55.986496]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.524178 57.079155 57.642216 58.213017]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.786533 59.306664 59.797035 60.25542 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.608234 60.895885 61.13696  61.30915 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.387756 61.41311  61.394665 61.348267]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.28251  61.199642 61.124603 61.057102]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.997437 60.954994 60.928864 60.91778 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.934032 60.97698  61.043987 61.142673]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.26602  61.411522 61.562855 61.729   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.903397 62.091175 62.251415 62.4496  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.64162  62.811    62.959415 63.11038 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.252106 63.3749   63.48831  63.59032 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.68742  63.777393 63.869907 63.96335 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.06027 64.12246 64.18015 64.2421 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.31507  64.397896 64.487366 64.574265]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.662735 64.77522  64.9061   65.041664]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.18962  65.328384 65.46806  65.609276]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.71517  65.8739   66.03446  66.183395]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.34714  66.49181  66.613144 66.70705 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.77025 66.79384 66.77126 66.69959]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.5901   66.42336  66.20886  65.949875]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.64423 65.30625 64.95654 64.60266]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.27679  63.972008 63.690292 63.479084]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.349506 63.277843 63.257183 63.31513 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.43117  63.596546 63.815247 64.07446 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.35945 64.66553 64.98774 65.31372]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.625145 65.915405 66.16848  66.375175]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.526054 66.62589  66.66981  66.65765 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.587074 66.45936  66.22797  65.95287 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.642586 65.27463  64.863075 64.415146]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.940372 63.443554 62.94087  62.44024 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.947857 61.46408  60.996334 60.545326]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.15195  59.78756  59.452663 59.1543  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.883682 58.635254 58.402912 58.185764]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.975445 57.7723   57.568417 57.35619 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.130527 56.8888   56.629436 56.35309 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.001446 55.623226 55.216293 54.74781 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.264942 53.768143 53.262257 52.74976 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.242287 51.751682 51.285614 50.842728]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.44138  50.071262 49.730347 49.430737]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.167793 48.922966 48.691696 48.46744 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.240757 48.0126   47.77859  47.502556]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.201916 46.87703  46.500072 46.09079 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.656673 45.196423 44.719376 44.237354]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.75986  43.289383 42.826633 42.386543]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.97229  41.587433 41.24127  40.942146]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.684326 40.458843 40.265736 40.102352]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.967262 39.857437 39.76865  39.691685]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.626545 39.57846  39.55145  39.53475 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.527214 39.5274   39.527428 39.526875]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.526543 39.526863 39.52557  39.523983]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.523056 39.522892 39.523407 39.525463]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.52981  39.54301  39.561607 39.58485 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.61239  39.650795 39.715103 39.7913  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.880386 40.005936 40.15751  40.32987 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.523396 40.73172  40.95181  41.18469 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.437122 41.69659  41.965023 42.241318]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.516968 42.791183 43.0655   43.340324]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.61565  43.891586 44.167667 44.44282 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.7183   44.9938   45.271336 45.54822 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.822742 46.097546 46.374302 46.65237 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.930916 47.209846 47.48742  47.765194]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.044205 48.32398  48.602215 48.87996 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.156773 49.43161  49.707924 49.986042]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.26487  50.5439   50.821198 51.097626]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.37625  51.65433  51.931072 52.206413]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.481586 52.757812 53.03507  53.309555]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.583572 53.858715 54.134945 54.411427]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.68814  54.965767 55.243298 55.520424]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.797302 56.075325 56.35285  56.629   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.904015 57.176167 57.44375  57.70389 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.951126 58.182632 58.399338 58.601036]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.772697 58.90198  59.00948  59.094524]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.134148 59.14319  59.126953 59.09062 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.04268  58.984623 58.909206 58.813984]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.713615 58.605995 58.488403 58.366154]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.245605 58.1266   58.010197 57.896282]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.78312  57.67018  57.556953 57.4414  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.324177 57.20642  57.090874 56.977863]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.864502 56.750828 56.63436  56.51546 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.398033 56.282326 56.169193 56.056866]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.94339  55.831886 55.72464  55.621746]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.52355 55.43491 55.35708 55.28891]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.230423 55.184723 55.15295  55.135292]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.13054  55.136833 55.152054 55.170784]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.191017 55.21379  55.24044  55.26608 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.29011  55.312626 55.332058 55.3484  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.3611   55.374897 55.38979  55.405193]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.419857 55.434814 55.45152  55.468628]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.486465 55.507034 55.532104 55.565445]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.60965  55.664295 55.728844 55.81143 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.90908 56.0153  56.13288 56.26829]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.418217 56.57581  56.7378   56.90652 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.092464 57.28821  57.483788 57.67718 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.871353 58.06778  58.264267 58.457203]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.650185 58.84204  59.03138  59.220047]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.408684 59.598656 59.79116  59.986084]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.181503 60.37456  60.567356 60.763153]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.963383 61.172432 61.392117 61.616398]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.84376  62.073864 62.307457 62.55879 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.81115  63.061832 63.304974 63.539997]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.764492 63.977226 64.1814   64.37647 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.557945 64.72342  64.87481  65.01391 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.14437  65.26898  65.39142  65.515686]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.646706 65.7864   65.93292  66.08572 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.24285  66.405174 66.5719   66.74265 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.9135  67.0811  67.24525 67.40749]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.567825 67.72637  67.885185 68.04609 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.2074   68.36892  68.52821  68.685394]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.840775 68.995445 69.15048  69.30609 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.462585 69.61837  69.76772  69.908775]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.0444  70.17277 70.29244 70.39394]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.47651 70.55005 70.61612 70.67263]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.71829  70.75635  70.78835  70.816574]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.8418   70.871635 70.90287  70.934204]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.96473 70.99767 71.0282  71.05781]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.089325 71.12083  71.14905  71.17266 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.18802 71.19343 71.18506 71.16237]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.125084 71.07561  71.00325  70.9265  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.84376 70.76107 70.6805  70.60112]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.522964 70.443886 70.36319  70.28062 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.20297  70.13042  70.056465 69.982445]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.906265 69.826775 69.737915 69.62235 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.48774  69.338646 69.17636  69.003   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.81898 68.63279 68.44591 68.25712]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.07022  67.88647  67.705864 67.52779 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.35148  67.17519  66.998634 66.82233 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.64569  66.469536 66.2915   66.11454 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.93833 65.76429 65.59368 65.42145]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.24683 65.06879 64.88955 64.71197]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.53686 64.36385 64.19631 64.04243]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.92214  63.81965  63.736134 63.669434]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.6197   63.584908 63.56007  63.536526]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.50932  63.47893  63.446083 63.412357]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.376247 63.34211  63.309265 63.275364]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.238144 63.201237 63.165386 63.13245 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.102436 63.093212 63.10901  63.14643 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.19664  63.257153 63.323734 63.391598]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.462395 63.53478  63.602707 63.667587]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.730576 63.796703 63.866558 63.932766]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.99322  64.05488  64.115486 64.177376]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.251526 64.34018  64.45515  64.588905]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.7401  64.90259 65.07092 65.24861]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.42764 65.60378 65.77957 65.95583]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.13081  66.304306 66.478325 66.651855]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.82498  66.99766  67.176765 67.36991 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.57234 67.78576 68.00688 68.23222]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.455505 68.67544  68.89341  69.109764]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.32522 69.54166 69.75638 69.97001]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.182304 70.39062  70.59554  70.77759 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.95513 71.12824 71.309   71.48957]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.66691 71.83914 72.00924 72.17726]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.344925 72.51331  72.681816 72.84844 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.015144 73.182076 73.349304 73.51794 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.686295 73.852806 74.0183   74.183   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.33426 74.46959 74.59355 74.71295]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.827324 74.93779  75.04965  75.162025]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.276505 75.39544  75.51683  75.63926 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.75937 75.87829 75.99743 76.11613]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.23508 76.35413 76.47209 76.57891]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.675415 76.761856 76.83847  76.908104]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.976295 77.04541  77.110596 77.169815]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.22504  77.27703  77.32226  77.359886]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.39019 77.41099 77.42222 77.42665]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.4256  77.41216 77.37619 77.32081]\n",
      "<class 'numpy.ndarray'>\n",
      "[77.25008  77.16427  77.06429  76.960075]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.85776  76.77104  76.696175 76.63304 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.59669  76.58493  76.589905 76.610054]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.64333 76.67827 76.71365 76.73619]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.7425   76.72338  76.6796   76.610016]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.51803  76.40988  76.2809   76.142654]\n",
      "<class 'numpy.ndarray'>\n",
      "[76.00329  75.865974 75.73349  75.60418 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.48211  75.390564 75.32083  75.27345 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.24605  75.22092  75.19781  75.168846]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.13354 75.07573 75.00006 74.91023]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.80626 74.66418 74.48372 74.29881]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.11927 73.95521 73.8122  73.69185]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.600945 73.533455 73.47952  73.43771 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.404144 73.376816 73.35257  73.32065 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.27787 73.22053 73.14338 73.05183]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.9498  72.84385 72.73721 72.64086]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.55807  72.49694  72.46721  72.469765]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.506165 72.573524 72.6659   72.76589 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.87179  72.977135 73.08364  73.18828 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.29077  73.34685  73.390175 73.414604]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.40408 73.35763 73.29656 73.20189]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.08651  72.96724  72.847946 72.74511 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.669624 72.616806 72.59362  72.600975]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.62495  72.67114  72.736435 72.81236 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.88921 72.97556 73.06189 73.13938]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.19563  73.222244 73.23204  73.22725 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.195366 73.14772  73.08681  73.03929 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.00796  72.99276  73.00499  73.048096]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.1296   73.241165 73.37367  73.51915 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.701004 73.90822  74.09813  74.261795]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.3878  74.4698  74.49845 74.46752]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.38607  74.26747  74.114944 73.9433  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.757866 73.561966 73.34571  73.134575]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.93249  72.735825 72.54026  72.349655]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.16667  71.990425 71.81904  71.64881 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.47751  71.30232  71.125496 70.952545]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.78813 70.63454 70.50709 70.40615]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.33263  70.28114  70.252396 70.24314 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.288086 70.34386  70.40745  70.48449 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.57249  70.64407  70.698265 70.72394 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.71095  70.670746 70.58877  70.47754 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.34545  70.19765  70.045944 69.90915 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.79351 69.70486 69.64829 69.62749]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.63461  69.6571   69.694046 69.739685]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.79841 69.85682 69.8982  69.91064]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.894745 69.849075 69.75297  69.63202 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.49073 69.3264  69.14872 68.97187]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.79931  68.653    68.54286  68.463326]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.412315 68.394684 68.40229  68.43324 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.48134 68.55218 68.62492 68.70021]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.7767  68.84334 68.89457 68.94368]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.99066  69.05783  69.13922  69.238174]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.37473 69.54568 69.74032 69.9512 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.18529 70.43121 70.6751  70.9076 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.11022  71.27585  71.396126 71.4628  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.477646 71.44096  71.36347  71.256966]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.14251  71.020004 70.89455  70.778244]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.687355 70.6494   70.639305 70.6642  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.740295 70.8407   70.967545 71.116936]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.27014 71.41152 71.5252  71.60305]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.65702 71.67772 71.66092 71.62029]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.56295  71.49359  71.418236 71.35267 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.30344  71.26287  71.244026 71.245285]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.25573  71.282036 71.316475 71.35536 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.39805 71.44172 71.47235 71.48544]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.49784 71.50077 71.49858 71.4894 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.45804  71.42037  71.378426 71.34054 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.304596 71.274    71.24649  71.21993 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.1832   71.15402  71.11971  71.082085]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.050896 71.02758  71.0057   70.995605]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.00318 71.02094 71.03924 71.04521]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.05227  71.069405 71.07756  71.06978 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.04426  71.005104 70.95533  70.89881 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.83552  70.77223  70.718124 70.67924 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.6626  70.65988 70.67815 70.70336]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.744576 70.800804 70.87031  70.9404  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.98314  71.005875 71.008606 70.9785  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.92868 70.86624 70.79666 70.6816 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.57231  70.47216  70.369934 70.25945 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.155975 70.05942  69.95071  69.832825]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.70377 69.56561 69.40648 69.23026]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.04301 68.844   68.63931 68.44075]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.25272 68.0824  67.92608 67.78588]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.66303  67.56008  67.475075 67.39126 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.308266 67.21337  67.10844  66.99358 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.85053  66.681755 66.4924   66.286705]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.073524 65.85769  65.64729  65.45441 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.28187 65.13819 65.02089 64.92971]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.858055 64.79609  64.73221  64.658516]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.56572 64.45991 64.34037 64.19529]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.02288  63.843395 63.65683  63.464474]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.275867 63.097645 62.939564 62.796562]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.6718   62.56298  62.46365  62.366486]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.26924  62.161114 62.038868 61.89602 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.736862 61.55718  61.356903 61.13933 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.92197  60.704315 60.48937  60.292107]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.10739 59.93533 59.81931 59.72574]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.654617 59.62982  59.653255 59.697735]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.762325 59.848183 59.946423 60.050663]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.15735  60.261898 60.362938 60.458916]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.548447 60.625866 60.69318  60.750336]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.77561  60.79859  60.81661  60.826736]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.822643 60.81414  60.802998 60.79373 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.788296 60.78986  60.803917 60.830997]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.884544 60.96367  61.05973  61.159714]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.264687 61.36994  61.474506 61.573723]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.666332 61.748676 61.81953  61.880135]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.930336 61.963226 61.970055 61.963367]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.95278  61.941887 61.940937 61.94649 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.957825 61.97889  62.013508 62.070744]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.153576 62.24872  62.365257 62.49546 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.629864 62.76317  62.89614  63.028805]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.153824 63.271793 63.37799  63.46149 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.52735  63.57923  63.623375 63.66156 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.70212  63.74522  63.786903 63.825214]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.868114 63.91683  63.96702  64.01911 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.06553 64.09695 64.13914 64.18488]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.228516 64.26619  64.29813  64.32404 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.34554 64.36397 64.37589 64.38439]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.39073 64.3925  64.38009 64.38632]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.39289  64.408646 64.43618  64.48314 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.54875  64.63939  64.739265 64.848236]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.96418 65.10669 65.24487 65.36637]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.47025 65.55307 65.62489 65.67127]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.69364 65.70141 65.69651 65.66642]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.635025 65.60206  65.5717   65.54786 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.5484   65.584656 65.65201  65.745995]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.86516  66.007805 66.173225 66.357834]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.544205 66.72784  66.88891  67.018776]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.126076 67.21053  67.2616   67.27422 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.25342 67.21798 67.16909 67.11332]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.05072  66.99971  66.97209  66.969315]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.993675 67.04036  67.10776  67.188   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.427217  2.420146  2.4128668 2.4105668]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.410311  2.4097962 2.4082234 2.4060783]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.4066339 2.4093642 2.4150171 2.4241958]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.4330728 2.441046  2.4504282 2.4601684]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.4691281 2.4733815 2.4729447 2.4758391]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.480208  2.479483  2.4731033 2.4661806]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.4593232 2.4492261 2.4406912 2.4336038]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.4280705 2.4238112 2.4219522 2.4264793]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.4387538 2.456227  2.4826055 2.5467339]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.6479669 2.7886074 2.9734051 3.2159626]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.5111954 3.856552  4.252155  4.696909 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.2073236 5.795456  6.435351  7.1271067]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 7.8773904  8.678933   9.527863  10.425423 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.372845 12.374518 13.412471 14.481566]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.574811 16.68776  17.80269  18.912617]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.012007 21.098448 22.163818 23.194496]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.187431 25.116575 25.986961 26.807005]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.57554  28.293343 28.96421  29.502306]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.990599 30.43332  30.819374 31.166906]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.490034 31.795902 32.088825 32.383205]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.67881  32.977985 33.28838  33.624004]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.99848 34.4236  34.90158 35.43596]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.03509  36.692177 37.39257  38.136658]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.986553 39.87205  40.79085  41.74709 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.74579  43.794098 44.868244 45.962204]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.06751  48.177803 49.285675 50.38777 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.479202 52.55491  53.612656 54.636925]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.62174  56.561245 57.42776  58.216072]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.918526 59.526432 60.028877 60.43226 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.73998  60.95143  61.039047 61.000435]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.846615 60.60931  60.29299  59.892994]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.4296   58.92196  58.369663 57.7832  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.17537  56.573948 55.997704 55.45633 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.95903  54.520027 54.14115  53.83053 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.598988 53.442177 53.369095 53.379955]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.49296 53.69594 53.96935 54.30495]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.694855 55.13173  55.695904 56.30241 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.948204 57.64548  58.36372  59.064484]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.74908  60.40651  60.999363 61.532524]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.004322 62.40691  62.731285 62.965538]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.095364 63.11984  63.047955 62.880314]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.634293 62.335236 61.990196 61.548935]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.084724 60.603603 60.09949  59.643   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.18866  58.753654 58.359768 58.02206 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.738235 57.50911  57.329597 57.203392]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.133816 57.107857 57.131462 57.197483]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.307735 57.45825  57.646847 57.87341 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.131615 58.41802  58.71487  59.012623]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.297466 59.606335 59.928978 60.23335 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.464428 60.659157 60.824883 60.96329 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.071884 61.161232 61.231842 61.27194 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.29388  61.287888 61.256565 61.195183]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.116745 61.026287 61.012238 60.980392]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.941765 60.914734 60.89952  60.883793]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.87918  60.898586 60.910507 60.920376]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.925003 60.92465  60.912525 60.88777 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.84769  60.792427 60.74725  60.70585 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.672733 60.664436 60.658    60.64356 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.652287 60.679325 60.707462 60.70892 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.708263 60.71232  60.722534 60.7383  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.766533 60.812252 60.880028 60.981384]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.110535 61.267567 61.391792 61.54208 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.69541  61.834118 61.986023 62.135532]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.27253  62.39642  62.505756 62.583344]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.63774  62.664288 62.665924 62.643032]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.592575 62.51936  62.424377 62.31625 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.18594  62.045307 61.898605 61.75927 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.634323 61.518906 61.39228  61.354065]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.373543 61.4505   61.586227 61.775368]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.01249  62.284603 62.608723 62.958836]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.32603  63.711742 64.118935 64.52767 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.929565 65.22095  65.49103  65.72621 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.90069 66.00522 66.05913 66.06205]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.00963  65.934265 65.83123  65.70294 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.50361  65.283066 65.045616 64.79667 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.54492  64.26148  63.965916 63.67176 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.382206 63.117966 62.920822 62.758556]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.631172 62.56645  62.61605  62.80173 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.096306 63.473213 63.91984  64.416115]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.9436  65.48396 66.00369 66.50112]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.95999 67.36386 67.68625 67.94324]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.12969  68.206184 68.19153  68.09411 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.92416  67.686844 67.392166 67.03183 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.640755 66.22931  65.823586 65.44024 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.09091  64.78687  64.534615 64.345345]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.27633 64.30586 64.43517 64.66026]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.957306 65.351616 65.79472  66.295296]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.80077 67.29172 67.75548 68.18091]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.56417 68.8779  69.11893 69.28812]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.37166  69.360405 69.26749  69.10316 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.88045  68.602486 68.274445 67.899895]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.49442 67.06155 66.60447 66.12078]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.61546  65.096245 64.57268  64.09672 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.624752 63.155243 62.69412  62.245068]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.835945 61.4449   61.067368 60.708324]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.351395 59.96719  59.559246 59.120007]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.637287 58.096046 57.448177 56.705227]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.895298 55.024513 54.099895 53.129192]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.11552  51.081543 50.03282  49.00212 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.01305  47.08211  46.217976 45.45374 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.792824 44.224308 43.75429  43.378826]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.09013  42.892822 42.797043 42.777615]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.836777 42.944702 43.090904 43.260983]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.446075 43.6412   43.845665 44.00794 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.1513   44.272694 44.35639  44.42512 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.478134 44.512466 44.505253 44.49813 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.491188 44.489025 44.494957 44.51277 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.54725  44.595737 44.663624 44.745113]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.841133 44.939487 45.026608 45.10623 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.184704 45.271526 45.375862 45.49172 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.54719  45.59241  45.63575  45.665905]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.679977 45.685616 45.68667  45.691406]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.70094  45.6682   45.6263   45.576004]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.519222 45.452477 45.377216 45.30313 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.21775  45.13276  45.059227 45.006573]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.97126  44.961105 44.975872 45.006077]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.081562 45.206814 45.39818  45.648388]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.94737  46.305428 46.72476  47.16674 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.62281  48.105892 48.57149  49.018425]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.45669  49.886166 50.306095 50.72123 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.128155 51.53008  51.92739  52.303844]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.658195 52.98986  53.29967  53.584793]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.841152 54.063988 54.248665 54.396317]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.508007 54.584793 54.617626 54.612995]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.568542 54.483517 54.357487 54.188255]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.97436  53.715786 53.411438 53.05858 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.66815  52.23923  51.777554 51.284477]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.7737   50.254734 49.7248   49.174862]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.59885 48.01188 47.41579 46.87909]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.35393  45.848503 45.376278 44.927174]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.520496 44.148155 43.80817  43.503475]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.28883  43.143234 43.077938 43.107758]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.249947 43.490887 43.832375 44.28633 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.836903 45.47917  46.19514  46.976227]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.797012 48.656406 49.561104 50.470688]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.366425 52.222397 53.041534 53.82076 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.543747 55.18866  55.774536 56.297512]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.741215 57.116455 57.43376  57.695602]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.89467 58.03313 58.11038 58.13285]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.057148 57.909634 57.70357  57.449806]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.152214 56.77007  56.337677 55.86229 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.33778  54.764492 54.158245 53.520164]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.821503 52.061836 51.25938  50.44683 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.626713 48.818047 48.03089  47.268963]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.54099 45.85712 45.22441 44.64907]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.14544  43.725357 43.40988  43.20125 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.10929  43.136047 43.285404 43.54909 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.907345 44.349163 44.87662  45.497196]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.182186 46.927105 47.740974 48.591846]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.47204  50.374496 51.293934 52.21087 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.10578  53.950047 54.726276 55.412537]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.016865 56.536743 56.960747 57.28393 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.49856  57.602367 57.60558  57.52081 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.33641  57.049026 56.666992 56.193424]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.63386 54.98816 54.26651 53.46983]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.610523 51.694572 50.73448  49.73669 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.730618 47.716335 46.710335 45.72322 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.765812 43.864826 43.04147  42.346077]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.755405 41.27824  40.906834 40.636894]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.5083   40.48921  40.570568 40.756824]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.037342 41.398563 41.84315  42.401733]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.084267 43.862965 44.704586 45.596626]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.51662  47.457798 48.4148   49.37865 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.336327 51.28051  52.201473 53.085796]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.89982  54.63039  55.27311  55.804752]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.21822  56.512295 56.69769  56.77061 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.747765 56.630585 56.39118  56.042183]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.606915 55.09058  54.50027  53.854736]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.1619   52.43286  51.6758   50.902954]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.133133 49.36905  48.625324 47.919323]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.27804  46.712673 46.246826 45.884865]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.63058  45.481792 45.44663  45.52221 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.701782 45.975513 46.34851  46.817448]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.387768 48.062813 48.818848 49.644936]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.53638  51.477367 52.456722 53.45402 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.46543  55.474606 56.47965  57.461224]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.389034 59.23363  59.986595 60.643906]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.193317 61.638523 61.96253  62.1646  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.2629   62.258858 62.154842 61.954895]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.648518 61.24933  60.757534 60.171696]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.505585 58.767323 57.983715 57.166298]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.341614 55.51499  54.692535 53.878242]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.093185 52.380817 51.73312  51.199203]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.777023 50.450607 50.23552  50.141125]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.158157 50.284748 50.51363  50.83557 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.239513 51.740166 52.34524  53.01925 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.751316 54.532246 55.34734  56.18114 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.022755 57.865047 58.692673 59.498806]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.284435 61.0379   61.73598  62.345043]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.849773 63.228394 63.48246  63.62733 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.66108  63.569393 63.367126 63.057064]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.648453 62.143814 61.552803 60.875202]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.122387 59.319412 58.468983 57.57236 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.635155 55.665524 54.683952 53.703453]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.730206 51.777573 50.86661  50.01486 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.24976  48.614418 48.08376  47.660095]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.34352  47.153835 47.09334  47.131874]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.26733  47.490055 47.810303 48.23524 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.74155  49.32049  49.967793 50.675182]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.42769  52.209995 53.008423 53.795013]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.56605  55.306538 56.00569  56.638653]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.16523  57.590416 57.891518 58.060608]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.10635  58.02407  57.81362  57.485268]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.026344 56.450817 55.78144  55.032383]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.21482  53.336674 52.42388  51.49704 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.567284 49.63555  48.71071  47.835293]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.042896 46.3457   45.74136  45.221245]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.76597  44.384907 44.10627  43.948402]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.945084 44.042973 44.23481  44.524677]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.91302  45.391884 45.94759  46.570488]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.24761  47.97502  48.74774  49.545887]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.367268 51.202312 52.045086 52.887146]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.72465  54.53746  55.311684 56.02958 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.66685  57.195225 57.622524 57.927364]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.05903  58.045822 57.88998  57.596313]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.16849  56.629105 55.995174 55.2852  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.52312  53.725613 52.89635  52.044067]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.202923 50.39568  49.642548 48.955994]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.35474  47.848965 47.44823  47.138134]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.92506  46.811535 46.80265  46.895645]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.08237  47.357292 47.717278 48.16234 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.67598 49.25099 49.88494 50.59215]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.368458 52.188866 53.044437 53.910763]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.774464 55.621178 56.436737 57.210335]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.936935 58.60804  59.193012 59.66063 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.00075  60.220318 60.32408  60.329567]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.179836 59.90563  59.511353 58.977997]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.36141  57.6601   56.884705 56.052353]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.170677 54.24423  53.276115 52.287537]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.29268  50.29985  49.315033 48.34165 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.388294 46.459034 45.56181  44.705753]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.897068 43.147255 42.488914 41.933155]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.52864  41.256653 41.12678  41.13936 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.28508  41.546463 41.915955 42.37955 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.91628  43.537533 44.240883 45.00197 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.781097 46.577087 47.392258 48.19765 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.968697 49.68878  50.351078 50.932903]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.428204 51.834953 52.168766 52.42774 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.61807  52.73449  52.781017 52.75137 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.647243 52.471214 52.22822  51.934494]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.588776 51.181454 50.706768 50.171913]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.579544 48.915977 48.20106  47.4451  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.65763  45.8438   45.03618  44.247948]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.508675 42.810253 42.156803 41.551895]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.993134 40.48423  40.02818  39.688065]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.41563  39.24385  39.163376 39.168064]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.256413 39.412636 39.642883 39.95049 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.34851  40.835163 41.39157  42.010277]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.6957   43.424854 44.196686 45.016785]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.867825 46.73874  47.624237 48.535236]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.460495 50.356453 51.216633 52.007664]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.7263   53.359657 53.912605 54.394997]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.821037 55.18852  55.503365 55.75663 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.933342 56.022743 56.02901  55.972485]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.8563   55.675312 55.430817 55.13652 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.80131  54.42453  53.97596  53.496803]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.989185 52.459106 51.91121  51.346794]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.768734 50.175056 49.587837 49.01618 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.46987  47.944843 47.43939  46.972233]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.555485 46.19369  45.881058 45.626408]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.444263 45.32711  45.262127 45.241734]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.263752 45.303745 45.362755 45.41972 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.47304  45.51712  45.547    45.568485]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.579735 45.579063 45.560272 45.522243]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.42055  45.256252 45.03308  44.75059 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.417233 44.020775 43.562447 43.03718 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.45168  41.816    41.12427  40.370686]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.579033 38.750244 37.895645 37.04036 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.20995  35.406914 34.622612 33.853752]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.119167 32.423733 31.767626 31.156437]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.596489 30.086243 29.626122 29.240925]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.918438 28.657972 28.477674 28.377598]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.344685 28.388128 28.510231 28.699213]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.951828 29.256826 29.605604 29.990057]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.41384  30.904955 31.425882 31.968422]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.526688 33.101517 33.646385 34.173153]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.694332 35.205322 35.679375 36.11053 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.499783 36.851826 37.1536   37.40854 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.621395 37.794098 37.925552 38.02196 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.117264 38.219044 38.334194 38.46722 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.61977  38.801163 39.01913  39.29894 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.639168 40.033165 40.477646 40.99584 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.583603 42.222347 42.909992 43.642303]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.416553 45.25684  46.161846 47.113243]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.09758  49.092304 50.095463 51.09633 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.094543 53.0819   54.04866  54.971394]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.839977 56.642284 57.35358  57.92032 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.367527 58.69428  58.90875  59.009842]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.995476 58.893852 58.713814 58.477264]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.193996 57.86945  57.492046 57.093582]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.715137 56.357628 56.033222 55.74225 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.47519  55.238136 55.040295 54.86215 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.713825 54.591663 54.49129  54.408   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.33189  54.260162 54.17689  54.075764]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.94166  53.815456 53.683    53.52676 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.34491  53.158764 52.97813  52.817303]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.690704 52.612926 52.593826 52.639706]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.776707 53.00431  53.326775 53.717533]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.183342 54.709473 55.28629  55.91558 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.576763 57.23965  57.869774 58.451313]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.988308 59.474693 59.870758 60.1731  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.390484 60.521114 60.564007 60.522476]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.37854  60.126926 59.76043  59.329887]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.863    58.384518 57.910645 57.447735]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.02366  56.64832  56.37281  56.211468]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.153286 56.206486 56.401024 56.71057 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.133045 57.660366 58.28932  58.995445]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.742874 60.508842 61.253044 61.970547]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.642437 63.253384 63.77641  64.180664]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.46143 64.60639 64.62798 64.56478]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.414444 64.194496 63.93557  63.64747 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.35462  63.065327 62.80305  62.5874  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.432045 62.351593 62.347355 62.415947]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.555763 62.761105 63.03017  63.350998]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.72522  64.140366 64.58401  65.02734 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.43765 65.76161 66.00155 66.14042]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.18107 66.13039 65.99007 65.7787 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.50803 65.19126 64.83058 64.43693]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.04291  63.691093 63.40421  63.191307]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.04386  62.977455 62.999138 63.09376 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.25186  63.463947 63.722656 64.00116 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.303764 64.64215  64.97295  65.277084]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.52163 65.69342 65.80839 65.84824]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.81988 65.71208 65.53441 65.294  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.017876 64.73643  64.45463  64.18712 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.94574  63.752792 63.612312 63.527718]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.500652 63.53723  63.61064  63.726818]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.874107 64.06043  64.277054 64.50646 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.740974 64.957726 65.12514  65.25117 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.331085 65.3634   65.380005 65.36583 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.34     65.29358  65.234085 65.16188 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.096565 65.043205 65.019295 65.01397 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.03214  65.057594 65.09113  65.12682 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.1606   65.18446  65.19682  65.211334]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.26019 65.3305  65.40305 65.49233]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.586006 65.69917  65.810715 65.92749 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.04255 66.16272 66.28118 66.40001]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.49025  66.57074  66.66416  66.761406]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.852356 66.89185  66.898674 66.87998 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.85149 66.79903 66.74413 66.65633]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.560715 66.46678  66.38984  66.34557 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.36303 66.42769 66.54383 66.71725]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.94987 67.22515 67.54202 67.90524]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.28998 68.68335 69.04674 69.36201]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.59402 69.76578 69.86153 69.88722]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.85254 69.77294 69.64546 69.48939]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.31023 69.10434 68.88431 68.64313]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.40845 68.18885 67.97326 67.75201]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.53761  67.292564 67.01605  66.69785 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.33474 65.91964 65.44224 64.8957 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.28164  63.604374 62.883774 62.135265]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.38386  60.729507 60.144394 59.634224]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.2048   58.86851  58.625458 58.47928 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.43291  58.485638 58.612717 58.806118]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.04043  59.30618  59.591503 59.892654]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.183052 60.461422 60.70778  60.913876]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.052925 61.095436 61.063107 60.94851 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.794033 60.6082   60.393803 60.140457]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.8678   59.578583 59.318176 59.067272]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.816742 58.55994  58.293324 58.01801 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.732452 57.441788 57.131958 56.787064]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.414932 56.036476 55.64045  55.23699 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.842575 54.47395  54.124317 53.815414]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.579212 53.40185  53.278584 53.205826]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.19551  53.24957  53.35668  53.507126]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.705418 53.948917 54.246765 54.59366 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.969303 55.371258 55.778824 56.17906 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.57472  56.95866  57.317726 57.647793]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.947956 58.221138 58.460564 58.66588 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.843807 58.997135 59.13522  59.258057]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.281986 59.24906  59.164803 59.046833]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.898228 58.739285 58.576706 58.413715]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.26021  58.133636 58.028183 57.933502]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.849304 57.776634 57.716854 57.680267]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.65981  57.651775 57.652565 57.66467 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.68009  57.70167  57.746967 57.78787 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.824516 57.862522 57.90324  57.94026 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.965164 57.97661  57.979073 57.977196]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.973644 57.98493  57.995106 58.01266 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.02744  58.043213 58.058884 58.074062]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.088764 58.09599  58.101658 58.099583]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.107677 58.130024 58.157734 58.160156]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.170677 58.180035 58.190987 58.191242]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.179623 58.156826 58.127716 58.090145]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.04074  57.979645 57.895824 57.798275]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.68007  57.562046 57.440693 57.300724]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.144234 56.963135 56.774055 56.587906]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.410355 56.245255 56.11527  56.01072 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.928856 55.853886 55.78891  55.748898]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.739803 55.7805   55.85704  55.972736]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.112988 56.282066 56.453304 56.624016]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.77819  56.924374 57.06803  57.203484]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.322086 57.4229   57.49819  57.547676]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.57858  57.594845 57.590054 57.564533]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.513557 57.434628 57.346195 57.238712]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.12005  56.998493 56.87714  56.769367]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.673843 56.596317 56.54268  56.530636]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.543747 56.57778  56.62201  56.678722]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.766838 56.88843  57.05936  57.260082]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.476948 57.695778 57.908607 58.097095]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.272907 58.434647 58.578613 58.68838 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.772964 58.82976  58.8681   58.863537]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.82374  58.749393 58.635574 58.48791 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.311222 58.122932 57.93096  57.74023 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.545616 57.363426 57.21762  57.12352 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.095592 57.118717 57.185722 57.288277]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.442215 57.636307 57.8867   58.196293]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.56474  58.97445  59.406403 59.851128]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.28539  60.688286 61.048656 61.348183]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.56517  61.721962 61.804417 61.806755]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.719604 61.54663  61.29346  60.972534]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.587032 60.162693 59.708138 59.242336]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.77896 58.33381 57.92058 57.54272]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.203148 56.915325 56.687466 56.508247]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.376812 56.296085 56.263874 56.268665]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.300007 56.35339  56.409214 56.46601 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.520596 56.568302 56.607243 56.613457]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.581165 56.488365 56.33425  56.128216]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.872314 55.571022 55.22642  54.839916]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.422005 53.976772 53.52052  53.05946 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.59862  52.142082 51.72329  51.34065 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.997147 50.70311  50.458916 50.267754]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.12429  50.02629  49.97796  49.979122]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.013268 50.069016 50.13176  50.19641 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.26308  50.327663 50.387703 50.422718]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.41968  50.361305 50.243465 50.064728]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.839653 49.5707   49.254242 48.89203 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.50302  48.09497  47.68394  47.283375]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.89421  46.532856 46.204563 45.92201 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.689156 45.50734  45.374954 45.29271 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.24347  45.2259   45.22659  45.235794]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.243023 45.24866  45.25429  45.259125]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.26223  45.2629   45.263428 45.26503 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.266544 45.264206 45.262386 45.260742]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.25919  45.258194 45.256233 45.25426 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.25364  45.252514 45.250305 45.24916 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.249653 45.25013  45.250813 45.252594]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.255367 45.257977 45.260803 45.26405 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.26716  45.27127  45.274956 45.278908]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.28373  45.287334 45.28976  45.292175]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.295578 45.300148 45.30353  45.305317]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.30603  45.306488 45.307224 45.306847]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.303917 45.301086 45.30032  45.300224]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.30003  45.300407 45.299976 45.299377]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.300972 45.307617 45.31814  45.330986]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.346275 45.367313 45.401516 45.45086 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.517445 45.601242 45.706013 45.836857]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.998985 46.18491  46.39308  46.62484 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.883106 47.17173  47.48542  47.823124]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.18031  48.55368  48.936016 49.32611 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.71692  50.107273 50.49517  50.87956 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.25319  51.608356 51.94123  52.250652]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.535675 52.785408 53.00898  53.206493]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.383877 53.54359  53.688675 53.823494]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.95135  54.07782  54.203438 54.329506]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.460175 54.597008 54.743286 54.912354]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.110107 55.339397 55.59313  55.870644]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.176884 56.51085  56.868736 57.237022]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.615273 57.99535  58.373226 58.749413]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.124832 59.489136 59.836044 60.156124]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.444294 60.701977 60.939518 61.16045 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.372353 61.581852 61.791527 62.0009  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.21391  62.431942 62.700287 63.01159 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.349907 63.712955 64.09725  64.49786 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.91083 65.32598 65.72725 66.11235]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.46938  66.782234 67.04308  67.239525]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.378975 67.46004  67.47954  67.434555]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.3326   67.178406 66.963196 66.702614]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.4124   66.10109  65.774216 65.43886 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.093056 64.740974 64.38727  64.04073 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.706562 63.383965 63.07362  62.776875]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.504196 62.2473   62.00722  61.77909 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.563293 61.361027 61.169186 60.984413]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.802776 60.63013  60.464607 60.317024]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.1899   60.088806 59.994278 59.90039 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.807404 59.71702  59.623928 59.52263 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.41469  59.309307 59.22366  59.14816 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.09118  59.057194 59.042877 59.04646 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.07721  59.140152 59.235783 59.368835]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.53959  59.73823  59.957016 60.18453 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.41223  60.637028 60.85748  61.065567]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.26103  61.38632  61.447617 61.458584]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.422173 61.34342  61.219967 61.055683]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.858    60.641483 60.406326 60.16207 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.922527 59.695206 59.48759  59.300037]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.135406 58.99618  58.88141  58.779526]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.687668 58.614864 58.549667 58.4827  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.412457 58.343407 58.275635 58.20807 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.137207 58.0596   57.97545  57.887863]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.79951  57.71074  57.624897 57.5426  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.463337 57.38287  57.292423 57.19699 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.09506  56.994072 56.894238 56.79302 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.683006 56.564713 56.429512 56.275387]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.09622  55.90008  55.697536 55.485657]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.271122 55.048824 54.816235 54.57256 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.312332 54.032185 53.74157  53.437645]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.120396 52.79367  52.45833  52.11585 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.768253 51.424503 51.085007 50.74543 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.405373 50.068283 49.737476 49.411934]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.09133  48.780994 48.4845   48.200905]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.939964 47.70046  47.485718 47.293922]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.123608 46.981304 46.865093 46.76844 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.690727 46.636017 46.604816 46.5941  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.599857 46.62028  46.659348 46.71517 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.78685  46.8753   46.978867 47.101376]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.244896 47.4111   47.5981   47.802532]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.018295 48.247314 48.491486 48.751675]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.02655  49.31387  49.613182 49.927555]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.25833  50.600197 50.951935 51.314056]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.68872  52.08311  52.49083  52.909195]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.334755 53.764057 54.198017 54.63615 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.074413 55.50828  55.937424 56.358753]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.77074  57.161488 57.529694 57.872128]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.19978  58.520252 58.83561  59.145008]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.447113 59.743042 60.033066 60.314613]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.589607 60.857582 61.114918 61.361073]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.596    61.82366  62.046654 62.261967]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.47103  62.68422  62.896835 63.108692]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.316315 63.520733 63.725502 63.933727]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.14267  64.34712  64.54746  64.747795]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.94734  65.145256 65.34415  65.54235 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.73424 65.92298 66.10989 66.29097]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.47114  66.64685  66.818405 66.98544 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.148735 67.306564 67.45708  67.60779 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.7577   67.90021  68.03842  68.171326]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.29602  68.417496 68.53101  68.64056 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.74842  68.853485 68.953804 69.04777 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.13727  69.22218  69.29978  69.370926]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.43767  69.49868  69.552765 69.59326 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.62668  69.65366  69.672165 69.686424]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.69637 69.70028 69.69775 69.68897]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.676414 69.660576 69.64287  69.62391 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.599236 69.56894  69.53048  69.48424 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.43381  69.38163  69.32962  69.276886]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.22601  69.179085 69.13499  69.09188 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.04663  69.003456 68.96689  68.93773 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.9076  68.88322 68.86034 68.83797]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.815735 68.791306 68.76604  68.74743 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.72826  68.71036  68.692604 68.67569 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.65675 68.633   68.60749 68.58205]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.54992 68.5122  68.47439 68.43057]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.37291 68.30243 68.21972 68.13097]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.04213 67.95543 67.86571 67.77889]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.699425 67.62803  67.58099  67.55857 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.552635 67.55954  67.56792  67.57343 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.57559 67.56595 67.53998 67.48929]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.42034 67.34658 67.26853 67.19309]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.12519  67.07008  67.032906 67.01062 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.00876  67.03325  67.06902  67.102234]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.13039  67.15298  67.166794 67.15716 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.13013  67.09016  67.045784 67.002396]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.968094 66.947395 66.95814  66.99046 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.042175 67.09254  67.13943  67.173744]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.19551  67.19966  67.184204 67.14312 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.09507 67.03865 66.98025 66.9242 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.88046  66.85567  66.85137  66.855576]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.86566 66.8737  66.87312 66.86464]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.84464  66.807846 66.747795 66.64966 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.51734  66.349976 66.1446   65.925385]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.69317  65.449295 65.197426 64.939354]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.67325 64.40597 64.1305  63.84151]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.534298 63.204678 62.866745 62.52003 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.162193 61.781258 61.382137 60.97863 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.58585  60.233994 59.92947  59.671906]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.466537 59.313    59.221294 59.187187]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.192215 59.233013 59.304443 59.3984  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.500587 59.60501  59.70749  59.797764]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.86994  59.928326 59.98421  60.03751 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.079517 60.10025  60.1097   60.100395]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.06895  60.00682  59.910194 59.77207 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.595474 59.362164 59.05541  58.71805 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.370693 58.02031  57.67649  57.343548]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.031826 56.747555 56.511574 56.30199 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.139862 56.019512 55.929207 55.854004]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.788685 55.72597  55.675915 55.633247]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.603733 55.58219  55.56412  55.550297]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.53383  55.516865 55.506622 55.49905 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.489624 55.484894 55.46616  55.442276]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.412148 55.38769  55.378983 55.396202]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.440926 55.517765 55.654938 55.85782 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.116913 56.407703 56.71639  57.035213]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.352654 57.65025  57.91372  58.12741 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.269917 58.336567 58.34222  58.29901 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.223152 58.11529  57.98323  57.83511 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.689003 57.541046 57.386982 57.214886]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.023342 56.807346 56.56835  56.308525]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.038334 55.76074  55.487747 55.238224]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.02771  54.864105 54.75774  54.707558]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.71126  54.766148 54.844624 54.937634]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.045174 55.17825  55.302723 55.428474]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.548607 55.67501  55.812706 55.958984]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.13597  56.326515 56.52871  56.717743]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.88865  57.031372 57.1489   57.235035]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.291405 57.31601  57.302563 57.239918]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.13262 56.99165 56.80977 56.59776]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.35926  56.113888 55.868286 55.628384]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.393467 55.18077  54.980934 54.801846]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.63204  54.468357 54.30862  54.145615]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.00896  53.878357 53.743935 53.61752 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.51081  53.438236 53.407722 53.41143 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.462887 53.563076 53.71248  53.89993 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.12315  54.360435 54.599857 54.824947]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.021114 55.17321  55.271065 55.301804]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.266506 55.16089  55.005737 54.82001 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.614853 54.396915 54.182278 53.975365]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.77614  53.588055 53.40224  53.221485]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.048817 52.90535  52.79918  52.735657]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.723305 52.78059  52.902073 53.065712]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.270245 53.501617 53.740734 53.9849  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.21036  54.404137 54.553043 54.624622]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.613857 54.527878 54.371983 54.15904 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.904827 53.611137 53.29816  52.98467 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.68134  52.40301  52.174107 52.006844]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.912914 51.90983  52.004704 52.19408 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.465466 52.7939   53.17364  53.588272]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.01608  54.450798 54.873005 55.28003 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.654    55.984905 56.260723 56.451946]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.557167 56.585583 56.537613 56.4135  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.21611  55.957718 55.665215 55.345535]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.01097  54.675694 54.370144 54.099014]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.886555 53.72388  53.60503  53.536507]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.52026  53.547894 53.61462  53.714794]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.854633 54.02752  54.210373 54.39076 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.56153  54.70869  54.84135  54.958477]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.036842 55.096077 55.143353 55.175358]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.168747 55.129406 55.060123 54.978844]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.883636 54.779533 54.67017  54.56202 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.447063 54.331844 54.212887 54.088585]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.955013 53.820267 53.691082 53.56299 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.43431  53.314907 53.197365 53.085625]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.001003 52.935944 52.879387 52.83204 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.810963 52.808815 52.823532 52.843586]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.86007  52.857803 52.827713 52.751648]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.64775  52.515682 52.35212  52.15598 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.925922 51.671787 51.396572 51.118774]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.84876  50.613598 50.419434 50.264957]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.15636  50.104263 50.113644 50.185207]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.31558  50.496567 50.71038  50.94591 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.17554 51.37057 51.51767 51.59403]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.60901  51.55856  51.44347  51.258137]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.023678 50.758392 50.466904 50.161472]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.851067 49.565495 49.317673 49.113735]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.972065 48.88845  48.86967  48.93278 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.060345 49.225456 49.417408 49.60913 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.776943 49.907917 49.966827 49.955273]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.864433 49.712322 49.508038 49.271805]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.011337 48.749218 48.501015 48.281918]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.09782  47.958336 47.88055  47.878307]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.951267 48.109444 48.33344  48.596745]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.864456 49.12608  49.366547 49.557156]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.686726 49.734722 49.697334 49.57607 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.38113  49.134144 48.861835 48.581875]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.310616 48.06813  47.868515 47.729427]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.66423  47.675743 47.764164 47.920517]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.127907 48.377125 48.65375  48.940926]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.219704 49.47491  49.690777 49.857502]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.960545 50.002964 49.988914 49.934174]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.831768 49.697094 49.53828  49.352654]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.16115  48.97772  48.82062  48.691902]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.588585 48.515522 48.467915 48.44475 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.42339  48.446938 48.5279   48.63215 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.749043 48.88334  49.023026 49.151302]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.28363  49.43069  49.607895 49.822563]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.07114  50.363438 50.709892 51.120132]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.58359  52.094112 52.64159  53.217445]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.80526  54.401306 54.999134 55.59646 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.178886 56.737392 57.25253  57.711742]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.098766 58.42666  58.69374  58.934925]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.15936  59.37532  59.593872 59.8134  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.046482 60.300797 60.579258 60.881336]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.197334 61.52187  61.838047 62.12829 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.389435 62.61168  62.78438  62.896687]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.94561  62.926823 62.840202 62.684643]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.462715 62.190624 61.878464 61.533012]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.157288 60.761913 60.34518  59.92002 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.496323 59.07183  58.653694 58.264072]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.89889  57.553715 57.235817 56.9425  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.670345 56.412647 56.16372  55.906925]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.63389  55.341846 55.036785 54.717915]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.33941  53.883442 53.365982 52.794186]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.15652  51.454617 50.706543 49.9135  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.073143 48.19111  47.269993 46.318874]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.335392 44.31701  43.26759  42.206074]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.137074 40.065525 38.995148 37.926167]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.862465 35.80786  34.777412 33.777325]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.82334  31.911367 31.032639 30.193539]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.398834 28.662678 27.98271  27.354074]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.778278 26.259243 25.799953 25.39917 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.061745 24.786028 24.562061 24.389643]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.268436 24.197634 24.181236 24.223154]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.324385 24.475018 24.672873 24.918983]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.213768 25.556282 25.944668 26.375433]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.843311 27.345768 27.882992 28.451439]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.04919  29.675148 30.32617  31.00381 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.707495 32.432102 33.17116  33.923557]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.693336 35.480194 36.278133 37.07657 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.87126  38.660393 39.434486 40.193302]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.9459   41.68246  42.403267 43.108227]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.8027   44.488495 45.1564   45.79549 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.414993 47.011715 47.577682 48.11328 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.61968  49.09661  49.54501  49.968754]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.368923 50.74752  51.099373 51.43119 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.743496 52.01499  52.26344  52.484386]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.672592 52.81695  52.922745 52.97587 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.981766 52.952812 52.887753 52.784138]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.634666 52.437096 52.193962 51.9033  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.561764 51.173737 50.744835 50.267487]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.738823 49.16647  48.550407 47.894268]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.203144 46.47734  45.711243 44.900146]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.061253 43.20121  42.315826 41.40565 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.473312 39.522915 38.559082 37.584023]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.620525 35.672455 34.73737  33.810345]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.89483  32.00293  31.164324 30.3558  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.575369 28.82324  28.105553 27.424635]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.78785  26.199902 25.663965 25.186333]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.772017 24.416967 24.123299 23.872047]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.667057 23.508823 23.417807 23.39968 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.433891 23.514708 23.660429 23.876902]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.145903 24.471624 24.851437 25.28333 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.76677  26.293592 26.861729 27.469242]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.111975 28.784271 29.481518 30.20373 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.963486 31.743225 32.544365 33.366703]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.20203  35.03827  35.869873 36.70115 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.529976 38.347515 39.145596 39.931534]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.70564  41.451607 42.17372  42.872223]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7307596  0.72864443 0.726266   0.72447157]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7224087 0.7205321 0.7204966 0.7220963]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7226861 0.7225289 0.7217294 0.7202309]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.71896136 0.7178154  0.71694684 0.7161236 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7148944  0.71322435 0.7110269  0.7099615 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7099524  0.7101386  0.71102625 0.7121059 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.71290684 0.7143636  0.71702564 0.71918064]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.71976066 0.72028476 0.7221161  0.72301334]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.72230226 0.72091043 0.7190701  0.719688  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7200203  0.7187438  0.71625847 0.71340215]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7136094 0.7155786 0.7166541 0.7172548]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7175435  0.71861845 0.72030115 0.7211395 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7217827  0.7245582  0.72732985 0.7277163 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.72783583 0.7284125  0.72784394 0.7269813 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7266302 0.7264833 0.7276082 0.7284101]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7275327  0.72638303 0.72564787 0.725185  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7251884 0.7248211 0.723534  0.7225737]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.72177887 0.7200523  0.7185486  0.7181186 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7180224 0.7178793 0.7177985 0.7189561]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.72120327 0.7232227  0.72535914 0.7277556 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7296932  0.73151577 0.73243266 0.7324357 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.73289067 0.7343808  0.7369162  0.74009264]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.74295187 0.7442338  0.7438511  0.74272937]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7417108  0.742922   0.74330515 0.7412593 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.73840606 0.7354101  0.73418033 0.73426026]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7336813  0.73340935 0.73453456 0.7368426 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.73928994 0.7406191  0.7424444  0.74487346]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7466668 0.7471455 0.7466875 0.7464017]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7466735  0.7475702  0.7490923  0.74940264]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.74901277 0.74843127 0.7467388  0.7443917 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.741547  0.738883  0.7393704 0.7447461]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7509325 0.7583376 0.7675978 0.7776814]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7895366 0.8046384 0.8238402 0.844691 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.86550343 0.88286054 0.89705175 0.9043895 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.9071199 0.9114487 0.9157582 0.9184726]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.92035234 0.9186731  0.91604084 0.9132108 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.9068349  0.89986396 0.8945585  0.8961121 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.9058999  0.92674565 0.9576747  1.0013999 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.0600151 1.1331458 1.2267041 1.3441154]\n",
      "<class 'numpy.ndarray'>\n",
      "[1.4822718 1.6422329 1.8231097 2.0273383]\n",
      "<class 'numpy.ndarray'>\n",
      "[2.2628622 2.527571  2.8203716 3.1466978]\n",
      "<class 'numpy.ndarray'>\n",
      "[3.5080042 3.901719  4.3274555 4.7942824]\n",
      "<class 'numpy.ndarray'>\n",
      "[5.3032393 5.8556943 6.4219136 7.001168 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[7.5786867 8.14947   8.707619  9.248424 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 9.765051 10.250258 10.703015 11.117642]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.4947405 11.837786  12.148953  12.430576 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.690492 12.930149 13.158267 13.379149]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.600949 13.829215 14.069978 14.327124]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.602088 14.910391 15.25828  15.653311]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.09253  16.57149  17.08232  17.623251]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.19536  18.794561 19.410181 20.032135]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.651096 21.260664 21.860384 22.440968]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.00387  23.548168 24.078695 24.580488]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.040983 25.434164 25.810324 26.169806]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.509127 26.812075 27.10569  27.396671]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.688553 27.98079  28.272354 28.575533]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.891792 29.230486 29.592854 29.982515]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.404514 30.85116  31.305567 31.770172]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.260273 32.76415  33.26996  33.749893]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.20473  34.632465 35.020996 35.35931 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.64225 35.86463 36.01142 36.08026]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.08174  36.047283 35.960766 35.819027]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.650627 35.480984 35.31646  35.170704]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.052494 34.971558 34.944424 34.978466]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.079754 35.24829  35.49379  35.81103 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.20169  36.656185 37.1683   37.716957]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.295834 38.88913  39.492443 40.107655]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.705715 41.267006 41.77402  42.226376]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.611553 42.93264  43.173332 43.337406]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.433746 43.468704 43.448715 43.39972 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.33095  43.261803 43.203243 43.16468 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.150127 43.16564  43.239834 43.389797]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.63889  43.942886 44.306248 44.73021 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.225437 45.760902 46.33446  46.937057]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.563824 48.204037 48.837215 49.45152 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.02712  50.556953 51.025486 51.42504 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.746044 52.003666 52.199482 52.31361 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.358547 52.35218  52.313396 52.239548]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.14872  52.055183 51.96946  51.912014]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.89063  51.914642 51.999542 52.134186]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.318027 52.569485 52.920776 53.36249 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.880066 54.455837 55.072704 55.726894]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.398003 57.07348  57.729694 58.351734]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.92486  59.432617 59.869568 60.222534]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.488514 60.654957 60.744007 60.76572 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.72803  60.627018 60.476704 60.29635 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.111225 59.934555 59.77505  59.65108 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.57892  59.588257 59.689846 59.88107 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.166763 60.550404 61.00697  61.525204]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.083412 62.67711  63.273598 63.867912]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.43332 64.95303 65.41759 65.82135]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.15452  66.40816  66.58706  66.694954]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.7389   66.722595 66.652504 66.53319 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.3909  66.23295 66.07524 65.9398 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.834694 65.78125  65.78941  65.87499 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.036385 66.27258  66.58179  66.95399 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.374725 67.835655 68.32761  68.82901 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.321884 69.80248  70.253494 70.67394 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.060646 71.3863   71.6476   71.85321 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.002914 72.085526 72.0939   72.03251 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.923996 71.78562  71.62592  71.46204 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.30672 71.18161 71.09725 71.06374]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.08447  71.15715  71.284004 71.455864]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.68491  71.9595   72.25668  72.574974]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.91822 73.2663  73.60903 73.93068]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.2254   74.483154 74.69942  74.86662 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.97571  75.023315 75.013016 74.95578 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.86166 74.7428  74.61285 74.47841]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.35369  74.25639  74.178375 74.14032 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.13994  74.162445 74.213615 74.29742 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.39136  74.50065  74.617195 74.734245]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.8495   74.95888  75.06318  75.149025]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.22421  75.282814 75.32002  75.34212 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.34131  75.33072  75.308365 75.27683 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.23649  75.19465  75.156364 75.121284]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.07948 75.04119 75.00629 74.97713]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.95233 74.93281 74.91579 74.89087]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.86781  74.85516  74.85129  74.860466]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.88918 74.93286 74.99952 75.08678]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.194466 75.31552  75.44544  75.57536 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.69939  75.792854 75.85595  75.8834  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.87413  75.82701  75.747086 75.62717 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.47815 75.31355 75.13616 74.94917]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.76084 74.58749 74.43974 74.32557]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.25146  74.230194 74.257034 74.344696]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.47584  74.6395   74.82058  74.998505]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.155365 75.28224  75.373795 75.41166 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.39537  75.33126  75.22022  75.067604]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.894516 74.71337  74.533    74.38421 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.2755   74.21826  74.213455 74.25746 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.3468  74.46773 74.60701 74.74288]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.86921 74.97051 75.03514 75.05971]\n",
      "<class 'numpy.ndarray'>\n",
      "[75.02109  74.89835  74.725555 74.50316 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[74.24654  73.96684  73.68382  73.421715]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.189835 72.984245 72.808205 72.650986]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.51241 72.38308 72.26369 72.14386]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.01879  71.884026 71.74323  71.59058 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.436554 71.271034 71.09643  70.91952 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.742836 70.57017  70.40095  70.23381 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.07564  69.91543  69.74778  69.576256]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.39682 69.2161  69.03667 68.84892]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.64954 68.45205 68.25921 68.08911]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.93377 67.79372 67.6666  67.55271]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.44406 67.33996 67.2035  67.04078]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.84755  66.62211  66.36572  66.085785]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.78153 65.45232 65.11111 64.77827]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.46384  64.1813   63.930515 63.704887]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.5024   63.313084 63.127254 62.929813]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.703617 62.443287 62.147392 61.81137 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.44503  61.057365 60.669125 60.294025]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.947823 59.63703  59.36452  59.142002]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.98784  58.871655 58.768467 58.65671 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.52315  58.335815 58.087875 57.782234]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.430374 57.0443   56.64851  56.25893 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.89132  55.556572 55.265976 55.012566]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.789307 54.574524 54.3509   54.106205]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.833755 53.530148 53.182713 52.78859 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.35795  51.9058   51.433064 50.98479 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.554398 50.154064 49.788982 49.468403]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.19086  48.95912  48.773113 48.623814]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.49813  48.387177 48.265553 48.12366 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.951122 47.73402  47.465897 47.136112]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.74667  46.328747 45.88952  45.438477]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.980534 44.530876 44.095688 43.68734 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.31158  42.96592  42.647263 42.352898]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.076153 41.81479  41.568592 41.336933]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.127716 40.943928 40.785244 40.66651 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.592567 40.554592 40.55189  40.57697 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.622715 40.68014  40.731365 40.76782 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.77443  40.748123 40.678543 40.565506]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.401608 40.22264  40.03554  39.846153]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.676704 39.536224 39.41974  39.33013 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.26595  39.221935 39.193813 39.17766 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.174583 39.188293 39.227833 39.295555]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.39504  39.52786  39.690407 39.876125]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.078217 40.29091  40.503345 40.699566]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.86005  40.984802 41.0266   41.009914]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.94056  40.815475 40.637093 40.411835]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.14452  39.836624 39.503933 39.14998 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.77532  38.38324  37.989845 37.59756 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.216244 36.851734 36.51318  36.199215]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.902706 35.618008 35.34652  35.08792 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.834644 34.58659  34.334137 34.076054]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.81009  33.525276 33.2173   32.878525]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.507435 32.104916 31.666334 31.187284]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.670694 30.117344 29.524982 28.898087]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.247332 27.57965  26.90693  26.235167]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.564375 24.903831 24.261162 23.645903]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.059679 22.500782 21.967773 21.463745]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.975239 20.497879 20.035376 19.584002]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.147175 18.72175  18.310356 17.908543]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.513918 17.119871 16.72559  16.334146]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.947191 15.560578 15.17757  14.807496]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.458804 14.131048 13.837198 13.58807 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.382243 13.231447 13.139802 13.106501]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.128658 13.222182 13.383133 13.599632]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.871873 14.204081 14.593951 15.040916]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.545798 16.106508 16.716059 17.37591 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.081333 18.820839 19.59323  20.397287]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.2297   22.079454 22.941078 23.808733]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.680893 25.549688 26.409811 27.251436]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.076685 28.886171 29.67248  30.434717]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.17136  31.887383 32.583824 33.256924]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.90537  34.531063 35.134613 35.7193  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.28478  36.828304 37.341835 37.821293]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.290077 38.743004 39.176346 39.59478 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.99814  40.386448 40.758877 41.112976]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.45214  41.777924 42.08838  42.38516 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.672516 42.94861  43.213512 43.456253]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.673874 43.85979  44.01286  44.132572]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.218334 44.272404 44.28912  44.264133]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.198925 44.092533 43.948032 43.76991 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.556126 43.299416 42.993584 42.64477 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.253777 41.819057 41.344666 40.832024]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.280704 39.692417 39.0751   38.432053]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.759644 37.05706  36.325974 35.565983]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.77945  33.97176  33.145004 32.306377]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.4595   30.613268 29.773373 28.952238]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.147326 27.362814 26.613455 25.904976]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.239973 24.623852 24.089426 23.642645]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.286747 23.002928 22.792095 22.65753 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.594992 22.598835 22.673935 22.822746]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.035406 23.305439 23.637257 24.038303]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.49124  25.002441 25.576838 26.18147 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.813236 27.468842 28.143381 28.82447 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.502728 30.17502  30.836481 31.483923]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.11683  32.73383  33.328655 33.90067 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[34.450592 34.982754 35.4992   35.988   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.45518  36.917473 37.37436  37.816414]\n",
      "<class 'numpy.ndarray'>\n",
      "[38.233494 38.62213  38.99317  39.35035 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.693985 40.02155  40.33654  40.64481 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.947872 41.24651  41.540306 41.83083 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.118053 42.398415 42.67618  42.95615 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.23642  43.49439  43.741085 43.97584 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.19599  44.4027   44.597645 44.775074]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.93077  45.063137 45.168484 45.24313 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.28182  45.286076 45.244358 45.164215]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.046528 44.882614 44.66918  44.41305 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.11664  43.75589  43.322292 42.8166  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[42.266052 41.674545 41.041218 40.36705 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.6592   38.91793  38.153732 37.376183]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.59572 35.80537 35.00803 34.22813]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.479    32.76448  32.096718 31.490698]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.945967 30.457047 30.042467 29.71495 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.460997 29.283691 29.174402 29.133076]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.15819  29.249575 29.40195  29.597542]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.829565 30.086578 30.38076  30.689034]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.989159 31.275509 31.554955 31.83468 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.09728  32.335823 32.55524  32.754642]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.93126  33.085854 33.218754 33.337128]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.445324 33.556957 33.669086 33.797092]\n",
      "<class 'numpy.ndarray'>\n",
      "[33.95282  34.13924  34.36707  34.661865]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.037758 35.478333 35.981525 36.549477]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.17966  37.856926 38.57564  39.324245]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.09477  40.878017 41.671555 42.467888]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.2443   43.997158 44.733105 45.451244]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.11819  46.742344 47.329006 47.876858]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.391506 48.88873  49.368286 49.834827]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.29815  50.76397  51.236828 51.715115]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.207287 52.701145 53.194557 53.710876]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.261036 54.85355  55.482212 56.09283 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.679924 57.230194 57.722893 58.157005]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.528427 58.82828  59.028046 59.139496]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.154663 59.09209  58.956097 58.739193]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.40421  58.00273  57.556362 57.074528]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.570576 56.049374 55.551224 55.085453]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.66637  54.301834 54.01326  53.81899 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.71322  53.694824 53.768856 53.94407 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.208786 54.545795 54.938194 55.373314]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.820885 56.277065 56.71499  57.120113]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.467644 57.740868 57.91379  57.98235 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.943985 57.81944  57.608185 57.31021 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.9385   56.498474 56.002098 55.454865]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.901447 54.351295 53.82287  53.34534 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.924744 52.574158 52.30994  52.160385]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.114494 52.164917 52.302612 52.513046]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.764893 53.045235 53.34183  53.631294]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.892475 54.11366  54.284397 54.38996 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.42671  54.378746 54.217804 53.942936]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.53659  53.001755 52.388996 51.709625]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.00322  50.288235 49.579273 48.893295]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.25312  47.685944 47.2066   46.83203 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.556717 46.380318 46.310623 46.38602 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.554684 46.80688  47.113    47.466362]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.841858 48.21095  48.55782  48.865803]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.119293 49.290848 49.38115  49.38409 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.29002  49.089584 48.78109  48.374054]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.896244 47.35845  46.78444  46.197964]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.616405 45.061222 44.546158 44.111816]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.76284  43.506294 43.355797 43.34575 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.443115 43.636898 43.913197 44.263474]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.691532 45.195953 45.75257  46.314274]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.864563 47.37498  47.827812 48.214417]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.518883 48.696793 48.76282  48.729824]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.593887 48.358505 48.05348  47.701324]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.333813 46.957787 46.588966 46.269318]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.00564  45.822712 45.729935 45.73658 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[45.860188 46.104664 46.4528   46.899235]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.430115 48.031902 48.671154 49.31996 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.95538  50.561428 51.127296 51.62523 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.042545 52.34201  52.511543 52.57525 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.537495 52.404167 52.17675  51.833607]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.421566 50.970627 50.498512 50.034958]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.59402  49.204445 48.876415 48.622063]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.44761  48.367973 48.389175 48.507137]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.71395  48.99704  49.334217 49.706955]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.103703 50.500652 50.871113 51.207085]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.49885 51.71898 51.826   51.83286]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.735878 51.533943 51.198578 50.745953]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.20043  49.575737 48.8708   48.090794]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.248253 46.377304 45.49315  44.62059 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[43.776005 42.973686 42.22434  41.533646]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.94117  40.444447 40.02779  39.69522 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.47477  39.365253 39.336094 39.370625]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.476982 39.624855 39.77883  39.911366]\n",
      "<class 'numpy.ndarray'>\n",
      "[40.0032  40.04062 40.00759 39.89887]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.699142 39.413998 39.02196  38.532658]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.960533 37.34526  36.687923 36.006485]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.31574  34.640457 33.987293 33.37131 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.825695 32.3578   31.950575 31.599236]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.312086 31.116634 31.029963 31.011917]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.056042 31.133415 31.234531 31.33701 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.42415  31.484644 31.510798 31.49376 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[31.405764 31.24506  31.002363 30.693375]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.32916  29.927235 29.485415 29.018068]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.538107 28.079361 27.645561 27.24406 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.897964 26.631287 26.442787 26.336641]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.314625 26.376186 26.519781 26.727863]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.997465 27.319353 27.68037  28.062979]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.45529  28.840195 29.198769 29.524565]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.804235 30.031054 30.206366 30.32003 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[30.352448 30.322119 30.2483   30.11288 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.90429  29.650196 29.346823 29.007675]\n",
      "<class 'numpy.ndarray'>\n",
      "[28.65718  28.303423 27.975157 27.679955]\n",
      "<class 'numpy.ndarray'>\n",
      "[27.419403 27.196089 26.997545 26.83157 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.690939 26.596449 26.547655 26.522892]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.503933 26.495096 26.49083  26.479212]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.44928  26.404736 26.344837 26.265205]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.174767 26.065784 25.944725 25.808424]\n",
      "<class 'numpy.ndarray'>\n",
      "[25.637115 25.448883 25.24387  25.015902]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.779772 24.539042 24.291191 24.02823 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.756067 23.47811  23.19355  22.915367]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.63825  22.375319 22.122803 21.88444 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[21.661781 21.460133 21.279709 21.12609 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.98799  20.858768 20.737629 20.622633]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.531824 20.45642  20.382807 20.311005]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.246523 20.180044 20.11397  20.026268]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.93422  19.842247 19.744995 19.638298]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.527555 19.414253 19.302155 19.196323]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.097101 19.006796 18.93315  18.899298]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.893402 18.90353  18.92853  18.972675]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.034067 19.1124   19.191135 19.276957]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.351477 19.407143 19.42821  19.419989]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.375933 19.307936 19.217562 19.104803]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.973608 18.826126 18.675232 18.526451]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.389008 18.271175 18.187096 18.130407]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.106873 18.107365 18.138184 18.190065]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.301954 18.44109  18.58646  18.717176]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.824091 18.898506 18.933023 18.913383]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.84227  18.736633 18.606194 18.456953]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.300138 18.15107  18.014803 17.91194 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.849632 17.838434 17.873508 17.966436]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.111435 18.300554 18.509987 18.729471]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.946177 19.150282 19.333994 19.469545]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.546083 19.57133  19.541847 19.452116]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.31432  19.141155 18.960718 18.78166 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.61292 18.4696  18.37394 18.33218]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.334057 18.371668 18.435831 18.52306 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.621325 18.70781  18.764618 18.79869 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.803213 18.74889  18.644945 18.502327]\n",
      "<class 'numpy.ndarray'>\n",
      "[18.33745  18.145212 17.92924  17.715176]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.509533 17.324375 17.179104 17.069706]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.997324 16.952864 16.935112 16.944027]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.978323 17.026443 17.086332 17.122591]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.13849  17.132479 17.11054  17.074753]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.030859 16.976954 16.913712 16.819859]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.71285  16.59862  16.491756 16.3872  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[16.280296 16.172895 16.085245 16.010529]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.938373 15.867264 15.794485 15.712264]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.616913 15.517911 15.404355 15.275603]\n",
      "<class 'numpy.ndarray'>\n",
      "[15.123066 14.943809 14.73558  14.508862]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.269594 14.033736 13.813956 13.607945]\n",
      "<class 'numpy.ndarray'>\n",
      "[13.406576 13.217621 13.046897 12.888063]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.736927  12.592864  12.454133  12.3125515]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.163094  12.003904  11.835827  11.6534395]\n",
      "<class 'numpy.ndarray'>\n",
      "[11.455999 11.238674 11.024603 10.836059]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.690393 10.595876 10.562043 10.602013]\n",
      "<class 'numpy.ndarray'>\n",
      "[10.72705  10.941071 11.228439 11.58748 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[12.046414 12.583493 13.179564 13.814363]\n",
      "<class 'numpy.ndarray'>\n",
      "[14.483052 15.165962 15.849238 16.518852]\n",
      "<class 'numpy.ndarray'>\n",
      "[17.172235 17.776636 18.345226 18.866156]\n",
      "<class 'numpy.ndarray'>\n",
      "[19.343723 19.783474 20.173525 20.52234 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[20.847536 21.146906 21.438684 21.72404 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[22.000973 22.278473 22.549892 22.824673]\n",
      "<class 'numpy.ndarray'>\n",
      "[23.0973   23.388762 23.689476 24.00852 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[24.369028 24.784294 25.260792 25.80099 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[26.405088 27.05989  27.761871 28.510834]\n",
      "<class 'numpy.ndarray'>\n",
      "[29.297667 30.09523  30.896011 31.687548]\n",
      "<class 'numpy.ndarray'>\n",
      "[32.451843 33.184834 33.86823  34.499283]\n",
      "<class 'numpy.ndarray'>\n",
      "[35.075554 35.587597 36.033436 36.417027]\n",
      "<class 'numpy.ndarray'>\n",
      "[36.741257 37.01796  37.2679   37.518383]\n",
      "<class 'numpy.ndarray'>\n",
      "[37.776165 38.053047 38.370766 38.739704]\n",
      "<class 'numpy.ndarray'>\n",
      "[39.17149  39.670303 40.245712 40.889713]\n",
      "<class 'numpy.ndarray'>\n",
      "[41.59691  42.35663  43.1623   43.990917]\n",
      "<class 'numpy.ndarray'>\n",
      "[44.827206 45.628674 46.383736 47.07126 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.68176  48.20243  48.631184 48.970455]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.200977 49.322964 49.360306 49.31804 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.203835 49.03671  48.824764 48.58633 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.334736 48.09084  47.879017 47.725605]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.641754 47.630173 47.704845 47.87165 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.124424 48.465652 48.86957  49.325047]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.82093  50.350033 50.89243  51.4318  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.94509  52.422325 52.849888 53.228474]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.535393 53.76273  53.91308  53.978397]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.95957  53.8648   53.710716 53.52016 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.30432 53.09236 52.89191 52.70944]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.57972  52.530098 52.56432  52.68329 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.88839  53.192833 53.58167  54.03964 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.556507 55.112545 55.696133 56.28824 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.868195 57.42119  57.92969  58.384758]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.782257 59.112587 59.368484 59.54021 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.63823  59.66824  59.640713 59.57108 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.464794 59.340397 59.225754 59.130474]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.082325 59.09182  59.171192 59.329903]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.568993 59.875175 60.23594  60.661926]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.146717 61.65441  62.172897 62.68456 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.173355 63.617973 64.00613  64.33536 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.58738  64.749054 64.80599  64.76979 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.6595   64.48279  64.25422  63.989185]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.70603  63.44253  63.20664  63.029854]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.923157 62.899097 62.9592   63.10293 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.339466 63.672596 64.06132  64.501434]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.97983 65.48018 65.98264 66.46901]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.92721 67.33594 67.68592 67.97824]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.188995 68.31967  68.38256  68.37551 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.30196  68.16865  67.98806  67.774605]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.53932 67.30361 67.08738 66.91614]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.80739 66.76203 66.78888 66.88761]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.04692 67.26352 67.52307 67.80881]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.11074 68.42068 68.73175 69.02812]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.2985   69.522316 69.69215  69.81754 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.88622 69.89957 69.87274 69.80386]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.69408  69.54434  69.374405 69.224815]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.078064 68.95216  68.85962  68.80996 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.80922 68.84624 68.92246 69.03067]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.17562  69.35224  69.545975 69.75797 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.98372 70.21567 70.44877 70.66145]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.846275 71.01321  71.156784 71.28492 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.38361 71.45616 71.49114 71.52024]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.52343 71.51374 71.50932 71.50502]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.492546 71.496254 71.51374  71.54292 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.583275 71.63488  71.6994   71.769966]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.85481 71.94572 72.03689 72.1293 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.22657  72.32793  72.441376 72.5433  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.63986  72.73123  72.808945 72.879166]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.94386 73.02364 73.09752 73.16254]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.220955 73.27731  73.31058  73.33433 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.34205  73.341774 73.332825 73.317665]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.299446 73.26024  73.210846 73.16014 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.12578 73.10867 73.0969  73.0903 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.10293  73.12415  73.155914 73.19659 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.25318 73.31654 73.36487 73.41934]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.473305 73.51652  73.5481   73.56805 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.574554 73.57208  73.56227  73.54941 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.52487  73.49783  73.466515 73.4313  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.387314 73.35608  73.34136  73.34531 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.34741  73.34989  73.34372  73.360214]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.38692  73.41601  73.437935 73.4831  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.5364   73.575714 73.595375 73.59786 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.596176 73.57353  73.53956  73.49002 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.42781 73.35339 73.26404 73.15994]\n",
      "<class 'numpy.ndarray'>\n",
      "[73.03898  72.905754 72.767075 72.616295]\n",
      "<class 'numpy.ndarray'>\n",
      "[72.44016  72.23865  72.018166 71.794334]\n",
      "<class 'numpy.ndarray'>\n",
      "[71.55842 71.3258  71.08794 70.84267]\n",
      "<class 'numpy.ndarray'>\n",
      "[70.62416  70.397736 70.16346  69.925095]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.70687 69.52301 69.36971 69.24425]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.14969 69.07736 69.02848 68.99846]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.00141  69.01567  69.028275 69.03234 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[69.01153 68.96956 68.91001 68.81842]\n",
      "<class 'numpy.ndarray'>\n",
      "[68.692085 68.529724 68.329475 68.10566 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.85619 67.5867  67.30321 67.009  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.72357 66.46535 66.23635 66.05285]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.91932  65.832214 65.77881  65.76576 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.78584  65.832115 65.90927  66.00237 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.081955 66.14198  66.19056  66.22503 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.2324  66.20458 66.12092 65.98847]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.81593  65.5888   65.300095 64.97136 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.61483  64.255875 63.900776 63.554615]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.25043  63.007885 62.838684 62.749744]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.746372 62.831997 62.995365 63.23554 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.53997  63.884476 64.26833  64.66567 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.08027 65.50477 65.90697 66.27275]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.59313  66.86489  67.07893  67.136116]\n",
      "<class 'numpy.ndarray'>\n",
      "[67.0942   66.965256 66.75782  66.44888 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[66.04999  65.58211  65.062126 64.50414 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.934704 63.36799  62.822506 62.309948]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.859497 61.477257 61.185135 60.996975]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.91118  60.932632 61.067852 61.30883 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.638046 62.038216 62.48224  62.95645 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.42018  63.860764 64.263016 64.613106]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.87941 65.06166 65.15452 65.15222]\n",
      "<class 'numpy.ndarray'>\n",
      "[65.06037 64.89175 64.64954 64.3467 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.00938  63.652313 63.29349  62.924873]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.556965 62.19118  61.851498 61.548706]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.290947 61.077385 60.911385 60.79972 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.739166 60.7283   60.760483 60.836617]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.9408   61.072224 61.231876 61.41328 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.603962 61.797436 61.971878 62.116966]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.22772  62.300232 62.3381   62.33807 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.31436  62.261646 62.203537 62.118095]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.999443 61.85645  61.69894  61.51624 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.305473 61.08012  60.89672  60.737827]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.53291  60.35174  60.193413 60.063107]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.95895  59.88383  59.838528 59.82072 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.84015  59.892143 59.983986 60.106808]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.26791  60.440094 60.61954  60.809296]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.995342 61.152706 61.272472 61.35463 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.396614 61.391766 61.336487 61.250633]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.122093 60.962006 60.77203  60.560978]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.329933 60.09872  59.87814  59.675083]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.47043  59.27066  59.091747 58.898197]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.705765 58.54254  58.433155 58.37314 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.362984 58.384586 58.46545  58.651627]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.908154 59.207127 59.5342   59.88377 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.244137 60.608906 60.95902  61.28695 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.560078 61.773064 61.908154 61.955837]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.9148   61.790844 61.59191  61.342186]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.060368 60.74341  60.39942  60.022915]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.632523 59.239204 58.864353 58.517555]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.21483  57.965668 57.78514  57.67916 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.643528 57.68154  57.79391  58.045296]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.356255 58.73084  59.162815 59.641396]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.161125 60.742672 61.346527 61.9335  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.481575 62.98061  63.3992   63.725334]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.96439  64.11329  64.166466 64.072235]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.877983 63.602398 63.247494 62.829468]\n",
      "<class 'numpy.ndarray'>\n",
      "[62.376694 61.903454 61.420002 60.963   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.543552 60.166214 59.842857 59.57831 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.38046  59.26296  59.246796 59.33056 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.512905 59.82171  60.23465  60.713142]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.216587 61.728848 62.243027 62.737995]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.202515 63.59124  63.84969  63.99606 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[64.04761  63.990646 63.82035  63.54121 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[63.165478 62.716698 62.21125  61.675674]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.104237 60.540653 59.997387 59.49569 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.060646 58.70199  58.41587  58.210815]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.10612  58.110237 58.22208  58.42808 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.714752 59.055836 59.433437 59.83307 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.240456 60.62426  60.968483 61.26361 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.487984 61.629745 61.688633 61.664005]\n",
      "<class 'numpy.ndarray'>\n",
      "[61.54977  61.351196 61.08279  60.745117]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.314568 59.831345 59.32598  58.826378]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.33677  57.890877 57.49853  57.177925]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.942223 56.791466 56.764614 56.82451 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.959526 57.174767 57.46281  57.791718]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.148697 58.521614 58.88909  59.22622 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.527996 59.781033 59.99131  60.14382 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[60.22041  60.14846  59.997654 59.77603 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.445374 59.05661  58.62282  58.170868]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.712826 57.271088 56.861225 56.49031 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.18121  55.942535 55.78872  55.72875 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.758785 55.876495 56.07448  56.331974]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.63253  56.95705  57.300472 57.655525]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.989048 58.289936 58.54445  58.7394  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.865433 58.916195 58.895294 58.791485]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.572502 58.2726   57.90883  57.50178 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.06601  56.628544 56.191555 55.79371 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.445778 55.1551   54.941814 54.822273]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.80239  54.88053  55.05449  55.314995]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.64599  56.013386 56.41252  56.821255]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.23177  57.618244 57.96057  58.24908 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.470177 58.614323 58.658665 58.594143]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.42703  58.177544 57.850327 57.460316]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.016132 56.54086  56.05639  55.58153 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.126156 54.70427  54.344883 54.055447]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.854755 53.73859  53.71898  53.791054]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.000317 54.294987 54.660156 55.092167]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.581944 56.099373 56.625572 57.139744]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.615067 58.03987  58.39623  58.675743]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.85928  58.93389  58.894302 58.754707]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.52018  58.202957 57.816376 57.364033]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.866848 56.339493 55.81417  55.298794]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.818577 54.381966 54.00265  53.688934]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.45805  53.306324 53.252182 53.32427 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.498253 53.766872 54.115646 54.532112]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.991673 55.483173 55.96916  56.445633]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.902462 57.312355 57.64     57.881695]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.03454  58.0812   58.02917  57.893764]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.692875 57.426926 57.11038  56.74441 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.348362 55.934383 55.524925 55.136944]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.797245 54.513496 54.29385  54.143322]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.06523  54.06992  54.159103 54.352795]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.63859  55.01063  55.4249   55.877373]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.361683 56.85784  57.343315 57.798462]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.20752  58.53859  58.795418 58.96311 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.047215 59.038963 58.936974 58.745583]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.487076 58.150696 57.7493   57.293167]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.798664 56.285072 55.766506 55.28109 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.84395  54.465412 54.160484 53.949852]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.846725 53.84662  53.95635  54.176792]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.498497 54.903378 55.38463  55.91573 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.49554  57.09296  57.682014 58.230644]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.716736 59.11724  59.430706 59.637035]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.723343 59.705044 59.58952  59.310486]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.94327  58.501392 58.001118 57.456722]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.88286  56.32129  55.785503 55.290325]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.85802  54.524036 54.29159  54.166   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.144836 54.228046 54.42662  54.737366]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.164177 55.67615  56.236946 56.820423]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.413044 57.98427  58.51321  58.988808]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.39604  59.69031  59.853813 59.885036]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.779423 59.571762 59.26785  58.87009 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.391094 57.865307 57.308605 56.751766]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.212204 55.71112  55.277023 54.923286]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.667793 54.510033 54.46411  54.534943]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.723827 55.02545  55.42679  55.90732 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.44941  57.030495 57.615135 58.18038 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.715916 59.181328 59.55851  59.83213 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.999176 60.060493 60.012474 59.85449 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[59.58485  59.220867 58.777977 58.27262 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.729523 57.178154 56.640194 56.138634]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.676643 55.31756  55.051613 54.887413]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.821133 54.849064 55.041737 55.321617]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.671215 56.06805  56.50454  56.958595]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.40296  57.819168 58.18988  58.507114]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.727272 58.84002  58.85021  58.76351 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[58.56896  58.25338  57.806763 57.239388]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.570248 55.828217 55.025898 54.19105 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.359898 52.546753 51.759773 51.006416]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.332115 49.73939  49.237667 48.834377]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.539455 48.355347 48.28613  48.314037]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.421635 48.6226   48.891987 49.213238]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.563572 49.93306  50.28019  50.6002  ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.88673  51.107883 51.2418   51.295017]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.271084 51.167572 50.99117  50.75786 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.47824  50.14299  49.767075 49.356266]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.945156 48.536785 48.1415   47.77158 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.436302 47.13271  46.867374 46.651897]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.48098  46.35702  46.26648  46.216457]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.2025   46.223076 46.2851   46.397617]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.498886 46.614292 46.73594  46.840656]\n",
      "<class 'numpy.ndarray'>\n",
      "[46.920853 46.98405  47.062996 47.15985 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.256584 47.338352 47.414536 47.486267]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.55153  47.61387  47.663494 47.711246]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.758385 47.796177 47.82495  47.86033 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.906387 47.982033 48.072865 48.18439 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.313454 48.460175 48.617878 48.784912]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.96512  49.16294  49.38683  49.627834]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.88135  50.148586 50.442947 50.753666]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.069252 51.36417  51.65026  51.977207]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.301846 52.61016  52.90313  53.171314]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.38803  53.584904 53.739857 53.84328 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.906414 53.945724 53.96643  53.95693 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.933273 53.898067 53.85527  53.80951 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.74799  53.65769  53.577892 53.49881 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.39809  53.288376 53.174732 53.062637]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.949734 52.85028  52.755314 52.65643 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.54753  52.432278 52.312134 52.18327 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.045246 51.88435  51.710964 51.526638]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.34083  51.14578  50.948845 50.781937]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.651287 50.50853  50.359932 50.23284 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.131115 50.060333 50.01056  49.994465]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.004265 50.050663 50.111786 50.187576]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.27741  50.366306 50.451912 50.53663 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.612526 50.67892  50.733074 50.776028]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.80562  50.81885  50.819164 50.80479 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.776783 50.72258  50.66321  50.587826]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.483303 50.363968 50.23002  50.08185 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.939465 49.807014 49.634483 49.444572]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.25941  49.07983  48.902176 48.737335]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.576504 48.432182 48.314373 48.20926 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.11757  48.033497 47.96269  47.901096]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.85094  47.806232 47.765697 47.75996 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[47.788673 47.822006 47.873695 47.98911 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[48.14541  48.342316 48.564224 48.80919 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[49.07166  49.362755 49.695393 50.067753]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.47464  50.90941  51.37708  51.857784]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.34648  52.82539  53.289387 53.711662]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.101753 54.45092  54.72939  54.92456 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.04847  55.100704 55.076115 54.991035]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.850746 54.666306 54.42023  54.141758]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.838066 53.527164 53.21186  52.908   ]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.638138 52.409897 52.231186 52.106888]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.035843 52.01018  52.050476 52.167774]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.35294  52.598785 52.90428  53.26638 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.676857 54.095215 54.529102 55.004166]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.45863  55.88159  56.25725  56.578083]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.82397  56.98937  57.06873  57.052357]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.946495 56.763096 56.51953  56.216217]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.86608  55.477543 55.07481  54.66561 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.269653 53.89728  53.568344 53.287304]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.063286 52.89495  52.783897 52.746235]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.790997 52.912292 53.08575  53.32614 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.615063 53.945763 54.314034 54.702797]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.083046 55.438343 55.755    56.01901 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.22016  56.337376 56.37946  56.336246]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.205616 55.98907  55.704372 55.35371 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.9506   54.504246 54.030144 53.545742]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.06113  52.596584 52.15918  51.757233]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.39996  51.111847 50.883595 50.71663 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[50.6235   50.621975 50.69663  50.839893]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.06046 51.3447  51.67406 52.04753]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.46429  52.889996 53.303112 53.69469 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.049118 54.344936 54.57306  54.736217]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.839344 54.8754   54.814713 54.68765 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.498558 54.26929  54.007633 53.726215]\n",
      "<class 'numpy.ndarray'>\n",
      "[53.41866  53.104733 52.796825 52.500492]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.227318 51.988873 51.800655 51.66267 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.57951  51.5458   51.569065 51.640747]\n",
      "<class 'numpy.ndarray'>\n",
      "[51.775673 51.953426 52.16405  52.410786]\n",
      "<class 'numpy.ndarray'>\n",
      "[52.70573 53.03501 53.3872  53.73472]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.071804 54.39479  54.671455 54.907257]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.096504 55.236217 55.31643  55.352062]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.344364 55.299747 55.22518  55.105843]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.961575 54.80818  54.661026 54.521343]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.39043  54.2732   54.183598 54.126987]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.110733 54.131413 54.193417 54.29984 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[54.44419  54.63503  54.864063 55.129242]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.42939  55.758854 56.108555 56.468113]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.81407  57.140064 57.432354 57.66808 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.84401  57.961502 58.012383 57.99239 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[57.910404 57.76031  57.553036 57.295116]\n",
      "<class 'numpy.ndarray'>\n",
      "[56.998627 56.665176 56.30849  55.94001 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[55.559692 55.18264  54.823395 54.48776 ]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Now we average the data for the purpose of smoothing vectors to achieve higher accuracy\n",
    "# We separate elements into vectors of length 4\n",
    "\n",
    "slowVector = set()\n",
    "regularVector = set()\n",
    "fastVector = set()\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "VECTOR_LENGTH = 4\n",
    "AVERAGE_LENGTH = 75\n",
    "\n",
    "timeMatrix = []\n",
    "s21Matrix = []\n",
    "angleMatrix = []\n",
    "\n",
    "setTimes = []\n",
    "sets21 = []\n",
    "setangle = []\n",
    "\n",
    "for index, bank in enumerate(trialBank1):\n",
    "    setTimes.append([])\n",
    "    sets21.append([])\n",
    "    setangle.append([])\n",
    "\n",
    "    timeVector = []\n",
    "    s21Vector = []\n",
    "    angleVector = []\n",
    "    vectorcount = 0\n",
    "    s21 = moving_average(bank['S21'].to_numpy(), AVERAGE_LENGTH)\n",
    "    angle = moving_average(bank['Angle'].to_numpy(), AVERAGE_LENGTH)\n",
    "    s21 = np.float32(s21)\n",
    "    angle = np.float32(angle)\n",
    "    \n",
    "\n",
    "    while vectorcount + VECTOR_LENGTH - 1 < len(s21):\n",
    "        for i in range(VECTOR_LENGTH):\n",
    "            s21Vector.append(s21[vectorcount])\n",
    "\n",
    "            angleVector.append(angle[vectorcount])\n",
    "\n",
    "            vectorcount += 1\n",
    "\n",
    "        s21Matrix.append(np.array(s21Vector))\n",
    "        s21Vector = []\n",
    "        angleMatrix.append(np.array(angleVector))\n",
    "\n",
    "        print(np.array(angleVector))\n",
    "        print(type(np.array(angleVector)))\n",
    "        \n",
    "        if index < 3:\n",
    "            regularVector.add(tuple(np.array(angleVector)))\n",
    "        elif index < 6:\n",
    "            slowVector.add(tuple(np.array(angleVector)))\n",
    "        else:\n",
    "            fastVector.add(tuple(np.array(angleVector)))\n",
    "        angleVector = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleevelessData(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.as_tensor(x, dtype = torch.float32)\n",
    "        self.y = torch.as_tensor(y, dtype = torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "    \n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(VECTOR_LENGTH, 1500), nn.ReLU(), nn.Linear(1500, 1500), nn.ReLU(), nn.Linear(1500, 1500), nn.ReLU(), nn.Linear(1500, VECTOR_LENGTH))   #   \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 5\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "splits = KFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "\n",
    "results = {}\n",
    "slowResults = {}\n",
    "regularResults = {}\n",
    "fastResults = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = s21Matrix\n",
    "y = angleMatrix\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(s21Matrix, angleMatrix, test_size = 0.25, random_state = 42)\n",
    "scaler = StandardScaler()\n",
    "xTrain = scaler.fit_transform(xTrain)\n",
    "xTest = scaler.transform(xTest)\n",
    "\n",
    "train_dataset =  SleevelessData(xTrain, yTrain)\n",
    "test_dataset =  SleevelessData(xTest, yTest)\n",
    "\n",
    "dataset = ConcatDataset([train_dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Fold 1\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini batch     1: 292.173\n",
      "Loss after mini batch     1: 16.599\n",
      "Loss after mini batch     1: 19.456\n",
      "Loss after mini batch     1: 36.300\n",
      "Loss after mini batch     1: 21.789\n",
      "Loss after mini batch     1: 28.344\n",
      "Loss after mini batch     1: 21.304\n",
      "Loss after mini batch     1: 22.096\n",
      "Loss after mini batch     1: 28.093\n",
      "Loss after mini batch     1: 36.402\n",
      "Loss after mini batch     1: 18.848\n",
      "Starting epoch 2\n",
      "Loss after mini batch     2: 22.020\n",
      "Loss after mini batch     2: 19.167\n",
      "Loss after mini batch     2: 24.175\n",
      "Loss after mini batch     2: 19.521\n",
      "Loss after mini batch     2: 21.070\n",
      "Loss after mini batch     2: 18.073\n",
      "Loss after mini batch     2: 21.600\n",
      "Loss after mini batch     2: 19.328\n",
      "Loss after mini batch     2: 18.300\n",
      "Loss after mini batch     2: 23.084\n",
      "Loss after mini batch     2: 26.712\n",
      "Starting epoch 3\n",
      "Loss after mini batch     3: 15.977\n",
      "Loss after mini batch     3: 16.145\n",
      "Loss after mini batch     3: 28.549\n",
      "Loss after mini batch     3: 15.388\n",
      "Loss after mini batch     3: 15.950\n",
      "Loss after mini batch     3: 17.480\n",
      "Loss after mini batch     3: 15.531\n",
      "Loss after mini batch     3: 17.827\n",
      "Loss after mini batch     3: 15.655\n",
      "Loss after mini batch     3: 19.022\n",
      "Loss after mini batch     3: 19.111\n",
      "Starting epoch 4\n",
      "Loss after mini batch     4: 15.864\n",
      "Loss after mini batch     4: 17.744\n",
      "Loss after mini batch     4: 18.713\n",
      "Loss after mini batch     4: 17.145\n",
      "Loss after mini batch     4: 16.977\n",
      "Loss after mini batch     4: 15.988\n",
      "Loss after mini batch     4: 16.832\n",
      "Loss after mini batch     4: 20.431\n",
      "Loss after mini batch     4: 18.113\n",
      "Loss after mini batch     4: 19.095\n",
      "Loss after mini batch     4: 16.979\n",
      "Starting epoch 5\n",
      "Loss after mini batch     5: 18.947\n",
      "Loss after mini batch     5: 14.310\n",
      "Loss after mini batch     5: 21.261\n",
      "Loss after mini batch     5: 14.732\n",
      "Loss after mini batch     5: 18.147\n",
      "Loss after mini batch     5: 15.299\n",
      "Loss after mini batch     5: 16.633\n",
      "Loss after mini batch     5: 20.737\n",
      "Loss after mini batch     5: 22.354\n",
      "Loss after mini batch     5: 13.301\n",
      "Loss after mini batch     5: 14.651\n",
      "Starting epoch 6\n",
      "Loss after mini batch     6: 15.211\n",
      "Loss after mini batch     6: 23.285\n",
      "Loss after mini batch     6: 17.075\n",
      "Loss after mini batch     6: 13.380\n",
      "Loss after mini batch     6: 15.599\n",
      "Loss after mini batch     6: 14.731\n",
      "Loss after mini batch     6: 14.648\n",
      "Loss after mini batch     6: 15.804\n",
      "Loss after mini batch     6: 17.902\n",
      "Loss after mini batch     6: 16.064\n",
      "Loss after mini batch     6: 16.134\n",
      "Starting epoch 7\n",
      "Loss after mini batch     7: 13.571\n",
      "Loss after mini batch     7: 15.097\n",
      "Loss after mini batch     7: 18.050\n",
      "Loss after mini batch     7: 15.002\n",
      "Loss after mini batch     7: 13.237\n",
      "Loss after mini batch     7: 12.702\n",
      "Loss after mini batch     7: 17.095\n",
      "Loss after mini batch     7: 17.991\n",
      "Loss after mini batch     7: 15.048\n",
      "Loss after mini batch     7: 17.220\n",
      "Loss after mini batch     7: 17.649\n",
      "Starting epoch 8\n",
      "Loss after mini batch     8: 16.253\n",
      "Loss after mini batch     8: 15.967\n",
      "Loss after mini batch     8: 15.675\n",
      "Loss after mini batch     8: 13.568\n",
      "Loss after mini batch     8: 15.243\n",
      "Loss after mini batch     8: 14.172\n",
      "Loss after mini batch     8: 15.469\n",
      "Loss after mini batch     8: 17.397\n",
      "Loss after mini batch     8: 17.387\n",
      "Loss after mini batch     8: 13.860\n",
      "Loss after mini batch     8: 14.872\n",
      "Starting epoch 9\n",
      "Loss after mini batch     9: 17.261\n",
      "Loss after mini batch     9: 13.356\n",
      "Loss after mini batch     9: 14.041\n",
      "Loss after mini batch     9: 15.653\n",
      "Loss after mini batch     9: 13.649\n",
      "Loss after mini batch     9: 14.535\n",
      "Loss after mini batch     9: 14.809\n",
      "Loss after mini batch     9: 19.414\n",
      "Loss after mini batch     9: 15.430\n",
      "Loss after mini batch     9: 14.170\n",
      "Loss after mini batch     9: 14.770\n",
      "Starting epoch 10\n",
      "Loss after mini batch    10: 15.091\n",
      "Loss after mini batch    10: 14.811\n",
      "Loss after mini batch    10: 13.643\n",
      "Loss after mini batch    10: 15.150\n",
      "Loss after mini batch    10: 14.275\n",
      "Loss after mini batch    10: 18.429\n",
      "Loss after mini batch    10: 16.864\n",
      "Loss after mini batch    10: 13.254\n",
      "Loss after mini batch    10: 15.335\n",
      "Loss after mini batch    10: 13.587\n",
      "Loss after mini batch    10: 14.810\n",
      "Starting epoch 11\n",
      "Loss after mini batch    11: 13.642\n",
      "Loss after mini batch    11: 13.800\n",
      "Loss after mini batch    11: 15.849\n",
      "Loss after mini batch    11: 12.895\n",
      "Loss after mini batch    11: 15.822\n",
      "Loss after mini batch    11: 15.619\n",
      "Loss after mini batch    11: 14.101\n",
      "Loss after mini batch    11: 14.653\n",
      "Loss after mini batch    11: 14.603\n",
      "Loss after mini batch    11: 15.830\n",
      "Loss after mini batch    11: 14.429\n",
      "Starting epoch 12\n",
      "Loss after mini batch    12: 15.722\n",
      "Loss after mini batch    12: 14.672\n",
      "Loss after mini batch    12: 13.181\n",
      "Loss after mini batch    12: 13.756\n",
      "Loss after mini batch    12: 15.466\n",
      "Loss after mini batch    12: 13.860\n",
      "Loss after mini batch    12: 20.437\n",
      "Loss after mini batch    12: 13.851\n",
      "Loss after mini batch    12: 13.013\n",
      "Loss after mini batch    12: 14.730\n",
      "Loss after mini batch    12: 16.207\n",
      "Starting epoch 13\n",
      "Loss after mini batch    13: 13.387\n",
      "Loss after mini batch    13: 15.239\n",
      "Loss after mini batch    13: 14.161\n",
      "Loss after mini batch    13: 17.238\n",
      "Loss after mini batch    13: 16.068\n",
      "Loss after mini batch    13: 16.835\n",
      "Loss after mini batch    13: 16.171\n",
      "Loss after mini batch    13: 13.716\n",
      "Loss after mini batch    13: 14.343\n",
      "Loss after mini batch    13: 13.628\n",
      "Loss after mini batch    13: 13.943\n",
      "Starting epoch 14\n",
      "Loss after mini batch    14: 15.582\n",
      "Loss after mini batch    14: 14.218\n",
      "Loss after mini batch    14: 15.117\n",
      "Loss after mini batch    14: 14.236\n",
      "Loss after mini batch    14: 14.073\n",
      "Loss after mini batch    14: 12.914\n",
      "Loss after mini batch    14: 14.650\n",
      "Loss after mini batch    14: 13.201\n",
      "Loss after mini batch    14: 13.893\n",
      "Loss after mini batch    14: 14.381\n",
      "Loss after mini batch    14: 13.422\n",
      "Starting epoch 15\n",
      "Loss after mini batch    15: 13.391\n",
      "Loss after mini batch    15: 13.063\n",
      "Loss after mini batch    15: 16.185\n",
      "Loss after mini batch    15: 13.240\n",
      "Loss after mini batch    15: 16.834\n",
      "Loss after mini batch    15: 14.295\n",
      "Loss after mini batch    15: 14.424\n",
      "Loss after mini batch    15: 14.054\n",
      "Loss after mini batch    15: 14.509\n",
      "Loss after mini batch    15: 12.994\n",
      "Loss after mini batch    15: 14.642\n",
      "Starting epoch 16\n",
      "Loss after mini batch    16: 14.461\n",
      "Loss after mini batch    16: 13.684\n",
      "Loss after mini batch    16: 14.399\n",
      "Loss after mini batch    16: 12.757\n",
      "Loss after mini batch    16: 15.677\n",
      "Loss after mini batch    16: 16.965\n",
      "Loss after mini batch    16: 14.728\n",
      "Loss after mini batch    16: 13.074\n",
      "Loss after mini batch    16: 12.560\n",
      "Loss after mini batch    16: 13.560\n",
      "Loss after mini batch    16: 13.561\n",
      "Starting epoch 17\n",
      "Loss after mini batch    17: 13.456\n",
      "Loss after mini batch    17: 13.915\n",
      "Loss after mini batch    17: 13.567\n",
      "Loss after mini batch    17: 13.606\n",
      "Loss after mini batch    17: 13.228\n",
      "Loss after mini batch    17: 16.414\n",
      "Loss after mini batch    17: 14.129\n",
      "Loss after mini batch    17: 14.342\n",
      "Loss after mini batch    17: 12.980\n",
      "Loss after mini batch    17: 15.250\n",
      "Loss after mini batch    17: 14.119\n",
      "Starting epoch 18\n",
      "Loss after mini batch    18: 13.953\n",
      "Loss after mini batch    18: 13.760\n",
      "Loss after mini batch    18: 13.277\n",
      "Loss after mini batch    18: 12.632\n",
      "Loss after mini batch    18: 14.919\n",
      "Loss after mini batch    18: 13.630\n",
      "Loss after mini batch    18: 14.815\n",
      "Loss after mini batch    18: 15.846\n",
      "Loss after mini batch    18: 12.857\n",
      "Loss after mini batch    18: 12.697\n",
      "Loss after mini batch    18: 12.523\n",
      "Starting epoch 19\n",
      "Loss after mini batch    19: 15.437\n",
      "Loss after mini batch    19: 14.717\n",
      "Loss after mini batch    19: 13.688\n",
      "Loss after mini batch    19: 13.230\n",
      "Loss after mini batch    19: 13.290\n",
      "Loss after mini batch    19: 14.754\n",
      "Loss after mini batch    19: 13.831\n",
      "Loss after mini batch    19: 12.422\n",
      "Loss after mini batch    19: 14.086\n",
      "Loss after mini batch    19: 13.700\n",
      "Loss after mini batch    19: 14.066\n",
      "Starting epoch 20\n",
      "Loss after mini batch    20: 12.154\n",
      "Loss after mini batch    20: 12.904\n",
      "Loss after mini batch    20: 12.879\n",
      "Loss after mini batch    20: 14.000\n",
      "Loss after mini batch    20: 15.593\n",
      "Loss after mini batch    20: 14.696\n",
      "Loss after mini batch    20: 13.349\n",
      "Loss after mini batch    20: 13.451\n",
      "Loss after mini batch    20: 13.032\n",
      "Loss after mini batch    20: 13.932\n",
      "Loss after mini batch    20: 13.998\n",
      "Starting epoch 21\n",
      "Loss after mini batch    21: 12.802\n",
      "Loss after mini batch    21: 13.462\n",
      "Loss after mini batch    21: 12.962\n",
      "Loss after mini batch    21: 12.628\n",
      "Loss after mini batch    21: 13.582\n",
      "Loss after mini batch    21: 14.782\n",
      "Loss after mini batch    21: 13.183\n",
      "Loss after mini batch    21: 14.408\n",
      "Loss after mini batch    21: 13.391\n",
      "Loss after mini batch    21: 13.101\n",
      "Loss after mini batch    21: 13.476\n",
      "Starting epoch 22\n",
      "Loss after mini batch    22: 13.216\n",
      "Loss after mini batch    22: 13.536\n",
      "Loss after mini batch    22: 13.028\n",
      "Loss after mini batch    22: 13.034\n",
      "Loss after mini batch    22: 12.720\n",
      "Loss after mini batch    22: 13.237\n",
      "Loss after mini batch    22: 13.134\n",
      "Loss after mini batch    22: 13.480\n",
      "Loss after mini batch    22: 14.179\n",
      "Loss after mini batch    22: 17.266\n",
      "Loss after mini batch    22: 15.030\n",
      "Starting epoch 23\n",
      "Loss after mini batch    23: 12.228\n",
      "Loss after mini batch    23: 15.840\n",
      "Loss after mini batch    23: 13.049\n",
      "Loss after mini batch    23: 15.054\n",
      "Loss after mini batch    23: 15.047\n",
      "Loss after mini batch    23: 13.094\n",
      "Loss after mini batch    23: 12.319\n",
      "Loss after mini batch    23: 12.295\n",
      "Loss after mini batch    23: 11.437\n",
      "Loss after mini batch    23: 14.391\n",
      "Loss after mini batch    23: 12.857\n",
      "Starting epoch 24\n",
      "Loss after mini batch    24: 13.148\n",
      "Loss after mini batch    24: 13.646\n",
      "Loss after mini batch    24: 12.525\n",
      "Loss after mini batch    24: 14.328\n",
      "Loss after mini batch    24: 13.566\n",
      "Loss after mini batch    24: 12.935\n",
      "Loss after mini batch    24: 11.818\n",
      "Loss after mini batch    24: 13.996\n",
      "Loss after mini batch    24: 13.588\n",
      "Loss after mini batch    24: 13.919\n",
      "Loss after mini batch    24: 13.850\n",
      "Starting epoch 25\n",
      "Loss after mini batch    25: 14.099\n",
      "Loss after mini batch    25: 14.518\n",
      "Loss after mini batch    25: 13.097\n",
      "Loss after mini batch    25: 15.221\n",
      "Loss after mini batch    25: 13.672\n",
      "Loss after mini batch    25: 13.855\n",
      "Loss after mini batch    25: 13.063\n",
      "Loss after mini batch    25: 13.830\n",
      "Loss after mini batch    25: 14.576\n",
      "Loss after mini batch    25: 13.222\n",
      "Loss after mini batch    25: 12.489\n",
      "Starting epoch 26\n",
      "Loss after mini batch    26: 14.840\n",
      "Loss after mini batch    26: 12.569\n",
      "Loss after mini batch    26: 12.871\n",
      "Loss after mini batch    26: 13.498\n",
      "Loss after mini batch    26: 12.759\n",
      "Loss after mini batch    26: 12.938\n",
      "Loss after mini batch    26: 14.148\n",
      "Loss after mini batch    26: 14.517\n",
      "Loss after mini batch    26: 13.584\n",
      "Loss after mini batch    26: 13.703\n",
      "Loss after mini batch    26: 14.215\n",
      "Starting epoch 27\n",
      "Loss after mini batch    27: 13.540\n",
      "Loss after mini batch    27: 13.261\n",
      "Loss after mini batch    27: 14.307\n",
      "Loss after mini batch    27: 12.204\n",
      "Loss after mini batch    27: 13.125\n",
      "Loss after mini batch    27: 13.684\n",
      "Loss after mini batch    27: 13.608\n",
      "Loss after mini batch    27: 13.207\n",
      "Loss after mini batch    27: 13.857\n",
      "Loss after mini batch    27: 11.201\n",
      "Loss after mini batch    27: 13.265\n",
      "Starting epoch 28\n",
      "Loss after mini batch    28: 14.340\n",
      "Loss after mini batch    28: 13.814\n",
      "Loss after mini batch    28: 12.745\n",
      "Loss after mini batch    28: 12.328\n",
      "Loss after mini batch    28: 13.016\n",
      "Loss after mini batch    28: 13.432\n",
      "Loss after mini batch    28: 13.351\n",
      "Loss after mini batch    28: 12.013\n",
      "Loss after mini batch    28: 14.853\n",
      "Loss after mini batch    28: 14.407\n",
      "Loss after mini batch    28: 13.388\n",
      "Starting epoch 29\n",
      "Loss after mini batch    29: 14.107\n",
      "Loss after mini batch    29: 12.638\n",
      "Loss after mini batch    29: 14.659\n",
      "Loss after mini batch    29: 12.056\n",
      "Loss after mini batch    29: 12.615\n",
      "Loss after mini batch    29: 14.059\n",
      "Loss after mini batch    29: 11.894\n",
      "Loss after mini batch    29: 13.337\n",
      "Loss after mini batch    29: 13.179\n",
      "Loss after mini batch    29: 14.544\n",
      "Loss after mini batch    29: 13.386\n",
      "Starting epoch 30\n",
      "Loss after mini batch    30: 12.226\n",
      "Loss after mini batch    30: 12.626\n",
      "Loss after mini batch    30: 13.086\n",
      "Loss after mini batch    30: 12.692\n",
      "Loss after mini batch    30: 13.508\n",
      "Loss after mini batch    30: 13.257\n",
      "Loss after mini batch    30: 14.848\n",
      "Loss after mini batch    30: 13.136\n",
      "Loss after mini batch    30: 12.782\n",
      "Loss after mini batch    30: 14.983\n",
      "Loss after mini batch    30: 13.615\n",
      "Starting epoch 31\n",
      "Loss after mini batch    31: 14.724\n",
      "Loss after mini batch    31: 12.819\n",
      "Loss after mini batch    31: 13.353\n",
      "Loss after mini batch    31: 12.226\n",
      "Loss after mini batch    31: 13.645\n",
      "Loss after mini batch    31: 13.456\n",
      "Loss after mini batch    31: 14.717\n",
      "Loss after mini batch    31: 15.191\n",
      "Loss after mini batch    31: 12.836\n",
      "Loss after mini batch    31: 13.694\n",
      "Loss after mini batch    31: 13.024\n",
      "Starting epoch 32\n",
      "Loss after mini batch    32: 13.509\n",
      "Loss after mini batch    32: 14.449\n",
      "Loss after mini batch    32: 13.125\n",
      "Loss after mini batch    32: 12.659\n",
      "Loss after mini batch    32: 13.393\n",
      "Loss after mini batch    32: 12.608\n",
      "Loss after mini batch    32: 13.214\n",
      "Loss after mini batch    32: 13.010\n",
      "Loss after mini batch    32: 12.897\n",
      "Loss after mini batch    32: 13.438\n",
      "Loss after mini batch    32: 12.709\n",
      "Starting epoch 33\n",
      "Loss after mini batch    33: 13.645\n",
      "Loss after mini batch    33: 13.634\n",
      "Loss after mini batch    33: 12.446\n",
      "Loss after mini batch    33: 12.148\n",
      "Loss after mini batch    33: 13.621\n",
      "Loss after mini batch    33: 15.482\n",
      "Loss after mini batch    33: 13.500\n",
      "Loss after mini batch    33: 12.301\n",
      "Loss after mini batch    33: 13.073\n",
      "Loss after mini batch    33: 12.643\n",
      "Loss after mini batch    33: 12.859\n",
      "Starting epoch 34\n",
      "Loss after mini batch    34: 13.191\n",
      "Loss after mini batch    34: 15.491\n",
      "Loss after mini batch    34: 11.899\n",
      "Loss after mini batch    34: 13.532\n",
      "Loss after mini batch    34: 12.956\n",
      "Loss after mini batch    34: 13.373\n",
      "Loss after mini batch    34: 13.211\n",
      "Loss after mini batch    34: 13.046\n",
      "Loss after mini batch    34: 12.608\n",
      "Loss after mini batch    34: 11.978\n",
      "Loss after mini batch    34: 14.412\n",
      "Starting epoch 35\n",
      "Loss after mini batch    35: 12.328\n",
      "Loss after mini batch    35: 13.216\n",
      "Loss after mini batch    35: 12.640\n",
      "Loss after mini batch    35: 13.317\n",
      "Loss after mini batch    35: 14.716\n",
      "Loss after mini batch    35: 11.487\n",
      "Loss after mini batch    35: 13.500\n",
      "Loss after mini batch    35: 13.043\n",
      "Loss after mini batch    35: 13.185\n",
      "Loss after mini batch    35: 13.121\n",
      "Loss after mini batch    35: 13.141\n",
      "Starting epoch 36\n",
      "Loss after mini batch    36: 12.689\n",
      "Loss after mini batch    36: 13.659\n",
      "Loss after mini batch    36: 12.801\n",
      "Loss after mini batch    36: 13.432\n",
      "Loss after mini batch    36: 13.124\n",
      "Loss after mini batch    36: 14.149\n",
      "Loss after mini batch    36: 12.992\n",
      "Loss after mini batch    36: 12.653\n",
      "Loss after mini batch    36: 12.011\n",
      "Loss after mini batch    36: 12.568\n",
      "Loss after mini batch    36: 13.901\n",
      "Starting epoch 37\n",
      "Loss after mini batch    37: 13.465\n",
      "Loss after mini batch    37: 13.905\n",
      "Loss after mini batch    37: 12.353\n",
      "Loss after mini batch    37: 12.739\n",
      "Loss after mini batch    37: 12.906\n",
      "Loss after mini batch    37: 14.476\n",
      "Loss after mini batch    37: 13.625\n",
      "Loss after mini batch    37: 13.132\n",
      "Loss after mini batch    37: 14.271\n",
      "Loss after mini batch    37: 13.354\n",
      "Loss after mini batch    37: 12.702\n",
      "Starting epoch 38\n",
      "Loss after mini batch    38: 11.995\n",
      "Loss after mini batch    38: 13.220\n",
      "Loss after mini batch    38: 17.390\n",
      "Loss after mini batch    38: 13.733\n",
      "Loss after mini batch    38: 13.905\n",
      "Loss after mini batch    38: 12.558\n",
      "Loss after mini batch    38: 12.827\n",
      "Loss after mini batch    38: 12.670\n",
      "Loss after mini batch    38: 13.970\n",
      "Loss after mini batch    38: 13.469\n",
      "Loss after mini batch    38: 12.686\n",
      "Starting epoch 39\n",
      "Loss after mini batch    39: 14.631\n",
      "Loss after mini batch    39: 11.943\n",
      "Loss after mini batch    39: 13.040\n",
      "Loss after mini batch    39: 13.066\n",
      "Loss after mini batch    39: 13.081\n",
      "Loss after mini batch    39: 15.317\n",
      "Loss after mini batch    39: 12.399\n",
      "Loss after mini batch    39: 13.077\n",
      "Loss after mini batch    39: 11.926\n",
      "Loss after mini batch    39: 14.076\n",
      "Loss after mini batch    39: 13.303\n",
      "Starting epoch 40\n",
      "Loss after mini batch    40: 12.905\n",
      "Loss after mini batch    40: 12.912\n",
      "Loss after mini batch    40: 12.779\n",
      "Loss after mini batch    40: 14.368\n",
      "Loss after mini batch    40: 13.668\n",
      "Loss after mini batch    40: 12.134\n",
      "Loss after mini batch    40: 13.499\n",
      "Loss after mini batch    40: 12.968\n",
      "Loss after mini batch    40: 13.091\n",
      "Loss after mini batch    40: 14.005\n",
      "Loss after mini batch    40: 11.996\n",
      "Starting epoch 41\n",
      "Loss after mini batch    41: 12.694\n",
      "Loss after mini batch    41: 13.367\n",
      "Loss after mini batch    41: 13.025\n",
      "Loss after mini batch    41: 14.130\n",
      "Loss after mini batch    41: 12.716\n",
      "Loss after mini batch    41: 13.917\n",
      "Loss after mini batch    41: 13.967\n",
      "Loss after mini batch    41: 14.038\n",
      "Loss after mini batch    41: 12.917\n",
      "Loss after mini batch    41: 12.170\n",
      "Loss after mini batch    41: 13.108\n",
      "Starting epoch 42\n",
      "Loss after mini batch    42: 13.193\n",
      "Loss after mini batch    42: 14.198\n",
      "Loss after mini batch    42: 12.777\n",
      "Loss after mini batch    42: 12.871\n",
      "Loss after mini batch    42: 13.409\n",
      "Loss after mini batch    42: 12.827\n",
      "Loss after mini batch    42: 12.708\n",
      "Loss after mini batch    42: 12.583\n",
      "Loss after mini batch    42: 13.164\n",
      "Loss after mini batch    42: 13.987\n",
      "Loss after mini batch    42: 12.710\n",
      "Starting epoch 43\n",
      "Loss after mini batch    43: 14.733\n",
      "Loss after mini batch    43: 14.836\n",
      "Loss after mini batch    43: 11.854\n",
      "Loss after mini batch    43: 13.025\n",
      "Loss after mini batch    43: 14.388\n",
      "Loss after mini batch    43: 11.732\n",
      "Loss after mini batch    43: 12.463\n",
      "Loss after mini batch    43: 13.906\n",
      "Loss after mini batch    43: 13.719\n",
      "Loss after mini batch    43: 12.289\n",
      "Loss after mini batch    43: 12.434\n",
      "Starting epoch 44\n",
      "Loss after mini batch    44: 12.727\n",
      "Loss after mini batch    44: 12.844\n",
      "Loss after mini batch    44: 12.647\n",
      "Loss after mini batch    44: 13.220\n",
      "Loss after mini batch    44: 14.872\n",
      "Loss after mini batch    44: 13.598\n",
      "Loss after mini batch    44: 11.656\n",
      "Loss after mini batch    44: 12.720\n",
      "Loss after mini batch    44: 13.628\n",
      "Loss after mini batch    44: 12.989\n",
      "Loss after mini batch    44: 13.658\n",
      "Starting epoch 45\n",
      "Loss after mini batch    45: 13.629\n",
      "Loss after mini batch    45: 12.958\n",
      "Loss after mini batch    45: 13.383\n",
      "Loss after mini batch    45: 13.054\n",
      "Loss after mini batch    45: 11.839\n",
      "Loss after mini batch    45: 14.239\n",
      "Loss after mini batch    45: 14.733\n",
      "Loss after mini batch    45: 12.829\n",
      "Loss after mini batch    45: 13.061\n",
      "Loss after mini batch    45: 12.722\n",
      "Loss after mini batch    45: 13.945\n",
      "Starting epoch 46\n",
      "Loss after mini batch    46: 13.921\n",
      "Loss after mini batch    46: 12.293\n",
      "Loss after mini batch    46: 12.947\n",
      "Loss after mini batch    46: 13.098\n",
      "Loss after mini batch    46: 14.036\n",
      "Loss after mini batch    46: 14.909\n",
      "Loss after mini batch    46: 12.196\n",
      "Loss after mini batch    46: 13.071\n",
      "Loss after mini batch    46: 12.359\n",
      "Loss after mini batch    46: 12.638\n",
      "Loss after mini batch    46: 14.243\n",
      "Starting epoch 47\n",
      "Loss after mini batch    47: 14.463\n",
      "Loss after mini batch    47: 12.162\n",
      "Loss after mini batch    47: 12.411\n",
      "Loss after mini batch    47: 12.981\n",
      "Loss after mini batch    47: 12.757\n",
      "Loss after mini batch    47: 11.974\n",
      "Loss after mini batch    47: 13.703\n",
      "Loss after mini batch    47: 13.453\n",
      "Loss after mini batch    47: 12.144\n",
      "Loss after mini batch    47: 12.347\n",
      "Loss after mini batch    47: 13.082\n",
      "Starting epoch 48\n",
      "Loss after mini batch    48: 12.197\n",
      "Loss after mini batch    48: 12.209\n",
      "Loss after mini batch    48: 12.508\n",
      "Loss after mini batch    48: 12.700\n",
      "Loss after mini batch    48: 13.841\n",
      "Loss after mini batch    48: 13.727\n",
      "Loss after mini batch    48: 13.576\n",
      "Loss after mini batch    48: 13.419\n",
      "Loss after mini batch    48: 13.544\n",
      "Loss after mini batch    48: 13.544\n",
      "Loss after mini batch    48: 12.535\n",
      "Starting epoch 49\n",
      "Loss after mini batch    49: 12.002\n",
      "Loss after mini batch    49: 13.408\n",
      "Loss after mini batch    49: 12.466\n",
      "Loss after mini batch    49: 13.307\n",
      "Loss after mini batch    49: 16.825\n",
      "Loss after mini batch    49: 11.953\n",
      "Loss after mini batch    49: 12.252\n",
      "Loss after mini batch    49: 13.959\n",
      "Loss after mini batch    49: 12.370\n",
      "Loss after mini batch    49: 12.559\n",
      "Loss after mini batch    49: 12.957\n",
      "Starting epoch 50\n",
      "Loss after mini batch    50: 12.788\n",
      "Loss after mini batch    50: 13.214\n",
      "Loss after mini batch    50: 15.589\n",
      "Loss after mini batch    50: 13.943\n",
      "Loss after mini batch    50: 13.050\n",
      "Loss after mini batch    50: 11.700\n",
      "Loss after mini batch    50: 13.643\n",
      "Loss after mini batch    50: 12.283\n",
      "Loss after mini batch    50: 13.029\n",
      "Loss after mini batch    50: 12.560\n",
      "Loss after mini batch    50: 13.352\n",
      "Starting epoch 51\n",
      "Loss after mini batch    51: 12.430\n",
      "Loss after mini batch    51: 12.084\n",
      "Loss after mini batch    51: 11.675\n",
      "Loss after mini batch    51: 13.052\n",
      "Loss after mini batch    51: 13.610\n",
      "Loss after mini batch    51: 11.165\n",
      "Loss after mini batch    51: 13.640\n",
      "Loss after mini batch    51: 14.037\n",
      "Loss after mini batch    51: 13.588\n",
      "Loss after mini batch    51: 13.530\n",
      "Loss after mini batch    51: 13.227\n",
      "Starting epoch 52\n",
      "Loss after mini batch    52: 13.590\n",
      "Loss after mini batch    52: 14.518\n",
      "Loss after mini batch    52: 12.988\n",
      "Loss after mini batch    52: 13.695\n",
      "Loss after mini batch    52: 13.844\n",
      "Loss after mini batch    52: 12.293\n",
      "Loss after mini batch    52: 13.111\n",
      "Loss after mini batch    52: 11.959\n",
      "Loss after mini batch    52: 13.476\n",
      "Loss after mini batch    52: 12.655\n",
      "Loss after mini batch    52: 12.888\n",
      "Starting epoch 53\n",
      "Loss after mini batch    53: 14.228\n",
      "Loss after mini batch    53: 13.557\n",
      "Loss after mini batch    53: 12.044\n",
      "Loss after mini batch    53: 12.712\n",
      "Loss after mini batch    53: 12.229\n",
      "Loss after mini batch    53: 12.185\n",
      "Loss after mini batch    53: 13.430\n",
      "Loss after mini batch    53: 12.626\n",
      "Loss after mini batch    53: 13.062\n",
      "Loss after mini batch    53: 13.299\n",
      "Loss after mini batch    53: 14.751\n",
      "Starting epoch 54\n",
      "Loss after mini batch    54: 13.794\n",
      "Loss after mini batch    54: 11.849\n",
      "Loss after mini batch    54: 14.022\n",
      "Loss after mini batch    54: 12.936\n",
      "Loss after mini batch    54: 12.833\n",
      "Loss after mini batch    54: 12.295\n",
      "Loss after mini batch    54: 12.852\n",
      "Loss after mini batch    54: 13.294\n",
      "Loss after mini batch    54: 12.556\n",
      "Loss after mini batch    54: 11.912\n",
      "Loss after mini batch    54: 13.327\n",
      "Starting epoch 55\n",
      "Loss after mini batch    55: 13.038\n",
      "Loss after mini batch    55: 13.136\n",
      "Loss after mini batch    55: 12.273\n",
      "Loss after mini batch    55: 13.053\n",
      "Loss after mini batch    55: 12.631\n",
      "Loss after mini batch    55: 12.516\n",
      "Loss after mini batch    55: 13.519\n",
      "Loss after mini batch    55: 15.048\n",
      "Loss after mini batch    55: 12.325\n",
      "Loss after mini batch    55: 14.022\n",
      "Loss after mini batch    55: 13.087\n",
      "Starting epoch 56\n",
      "Loss after mini batch    56: 12.052\n",
      "Loss after mini batch    56: 12.731\n",
      "Loss after mini batch    56: 11.719\n",
      "Loss after mini batch    56: 11.955\n",
      "Loss after mini batch    56: 12.137\n",
      "Loss after mini batch    56: 12.464\n",
      "Loss after mini batch    56: 14.731\n",
      "Loss after mini batch    56: 12.246\n",
      "Loss after mini batch    56: 14.331\n",
      "Loss after mini batch    56: 13.168\n",
      "Loss after mini batch    56: 14.445\n",
      "Starting epoch 57\n",
      "Loss after mini batch    57: 12.865\n",
      "Loss after mini batch    57: 12.659\n",
      "Loss after mini batch    57: 12.384\n",
      "Loss after mini batch    57: 12.800\n",
      "Loss after mini batch    57: 13.188\n",
      "Loss after mini batch    57: 13.576\n",
      "Loss after mini batch    57: 12.334\n",
      "Loss after mini batch    57: 13.063\n",
      "Loss after mini batch    57: 11.971\n",
      "Loss after mini batch    57: 12.859\n",
      "Loss after mini batch    57: 12.998\n",
      "Starting epoch 58\n",
      "Loss after mini batch    58: 13.088\n",
      "Loss after mini batch    58: 14.723\n",
      "Loss after mini batch    58: 13.285\n",
      "Loss after mini batch    58: 12.801\n",
      "Loss after mini batch    58: 12.837\n",
      "Loss after mini batch    58: 12.890\n",
      "Loss after mini batch    58: 12.806\n",
      "Loss after mini batch    58: 13.049\n",
      "Loss after mini batch    58: 11.137\n",
      "Loss after mini batch    58: 11.420\n",
      "Loss after mini batch    58: 14.941\n",
      "Starting epoch 59\n",
      "Loss after mini batch    59: 13.031\n",
      "Loss after mini batch    59: 13.184\n",
      "Loss after mini batch    59: 12.636\n",
      "Loss after mini batch    59: 12.812\n",
      "Loss after mini batch    59: 13.057\n",
      "Loss after mini batch    59: 12.576\n",
      "Loss after mini batch    59: 11.129\n",
      "Loss after mini batch    59: 12.416\n",
      "Loss after mini batch    59: 13.802\n",
      "Loss after mini batch    59: 14.219\n",
      "Loss after mini batch    59: 11.799\n",
      "Starting epoch 60\n",
      "Loss after mini batch    60: 12.075\n",
      "Loss after mini batch    60: 12.228\n",
      "Loss after mini batch    60: 12.995\n",
      "Loss after mini batch    60: 12.166\n",
      "Loss after mini batch    60: 15.045\n",
      "Loss after mini batch    60: 13.702\n",
      "Loss after mini batch    60: 11.436\n",
      "Loss after mini batch    60: 12.130\n",
      "Loss after mini batch    60: 13.844\n",
      "Loss after mini batch    60: 12.517\n",
      "Loss after mini batch    60: 13.271\n",
      "Starting epoch 61\n",
      "Loss after mini batch    61: 11.653\n",
      "Loss after mini batch    61: 13.720\n",
      "Loss after mini batch    61: 12.214\n",
      "Loss after mini batch    61: 12.540\n",
      "Loss after mini batch    61: 12.189\n",
      "Loss after mini batch    61: 13.128\n",
      "Loss after mini batch    61: 12.183\n",
      "Loss after mini batch    61: 11.423\n",
      "Loss after mini batch    61: 13.494\n",
      "Loss after mini batch    61: 15.131\n",
      "Loss after mini batch    61: 13.293\n",
      "Starting epoch 62\n",
      "Loss after mini batch    62: 12.468\n",
      "Loss after mini batch    62: 13.855\n",
      "Loss after mini batch    62: 12.894\n",
      "Loss after mini batch    62: 13.798\n",
      "Loss after mini batch    62: 12.418\n",
      "Loss after mini batch    62: 13.522\n",
      "Loss after mini batch    62: 13.577\n",
      "Loss after mini batch    62: 12.138\n",
      "Loss after mini batch    62: 14.527\n",
      "Loss after mini batch    62: 12.122\n",
      "Loss after mini batch    62: 13.638\n",
      "Starting epoch 63\n",
      "Loss after mini batch    63: 12.421\n",
      "Loss after mini batch    63: 13.789\n",
      "Loss after mini batch    63: 12.891\n",
      "Loss after mini batch    63: 13.810\n",
      "Loss after mini batch    63: 12.291\n",
      "Loss after mini batch    63: 13.180\n",
      "Loss after mini batch    63: 13.563\n",
      "Loss after mini batch    63: 13.255\n",
      "Loss after mini batch    63: 12.992\n",
      "Loss after mini batch    63: 12.292\n",
      "Loss after mini batch    63: 12.949\n",
      "Starting epoch 64\n",
      "Loss after mini batch    64: 12.004\n",
      "Loss after mini batch    64: 14.655\n",
      "Loss after mini batch    64: 12.511\n",
      "Loss after mini batch    64: 13.523\n",
      "Loss after mini batch    64: 12.168\n",
      "Loss after mini batch    64: 13.350\n",
      "Loss after mini batch    64: 11.780\n",
      "Loss after mini batch    64: 12.565\n",
      "Loss after mini batch    64: 14.521\n",
      "Loss after mini batch    64: 12.821\n",
      "Loss after mini batch    64: 12.423\n",
      "Starting epoch 65\n",
      "Loss after mini batch    65: 12.665\n",
      "Loss after mini batch    65: 13.563\n",
      "Loss after mini batch    65: 13.773\n",
      "Loss after mini batch    65: 11.893\n",
      "Loss after mini batch    65: 13.846\n",
      "Loss after mini batch    65: 13.364\n",
      "Loss after mini batch    65: 11.132\n",
      "Loss after mini batch    65: 12.182\n",
      "Loss after mini batch    65: 12.301\n",
      "Loss after mini batch    65: 13.500\n",
      "Loss after mini batch    65: 12.405\n",
      "Starting epoch 66\n",
      "Loss after mini batch    66: 12.273\n",
      "Loss after mini batch    66: 12.744\n",
      "Loss after mini batch    66: 11.458\n",
      "Loss after mini batch    66: 12.346\n",
      "Loss after mini batch    66: 12.904\n",
      "Loss after mini batch    66: 12.310\n",
      "Loss after mini batch    66: 12.157\n",
      "Loss after mini batch    66: 11.873\n",
      "Loss after mini batch    66: 12.667\n",
      "Loss after mini batch    66: 13.119\n",
      "Loss after mini batch    66: 12.904\n",
      "Starting epoch 67\n",
      "Loss after mini batch    67: 14.657\n",
      "Loss after mini batch    67: 10.836\n",
      "Loss after mini batch    67: 13.625\n",
      "Loss after mini batch    67: 13.796\n",
      "Loss after mini batch    67: 12.251\n",
      "Loss after mini batch    67: 12.752\n",
      "Loss after mini batch    67: 11.684\n",
      "Loss after mini batch    67: 12.260\n",
      "Loss after mini batch    67: 11.868\n",
      "Loss after mini batch    67: 13.262\n",
      "Loss after mini batch    67: 12.089\n",
      "Starting epoch 68\n",
      "Loss after mini batch    68: 12.367\n",
      "Loss after mini batch    68: 13.658\n",
      "Loss after mini batch    68: 12.388\n",
      "Loss after mini batch    68: 10.321\n",
      "Loss after mini batch    68: 12.636\n",
      "Loss after mini batch    68: 13.281\n",
      "Loss after mini batch    68: 13.258\n",
      "Loss after mini batch    68: 12.748\n",
      "Loss after mini batch    68: 13.474\n",
      "Loss after mini batch    68: 12.681\n",
      "Loss after mini batch    68: 13.659\n",
      "Starting epoch 69\n",
      "Loss after mini batch    69: 12.632\n",
      "Loss after mini batch    69: 13.434\n",
      "Loss after mini batch    69: 13.855\n",
      "Loss after mini batch    69: 12.007\n",
      "Loss after mini batch    69: 12.543\n",
      "Loss after mini batch    69: 12.318\n",
      "Loss after mini batch    69: 11.246\n",
      "Loss after mini batch    69: 12.717\n",
      "Loss after mini batch    69: 13.430\n",
      "Loss after mini batch    69: 13.793\n",
      "Loss after mini batch    69: 13.076\n",
      "Starting epoch 70\n",
      "Loss after mini batch    70: 12.045\n",
      "Loss after mini batch    70: 13.537\n",
      "Loss after mini batch    70: 12.227\n",
      "Loss after mini batch    70: 12.051\n",
      "Loss after mini batch    70: 12.876\n",
      "Loss after mini batch    70: 12.903\n",
      "Loss after mini batch    70: 12.765\n",
      "Loss after mini batch    70: 12.134\n",
      "Loss after mini batch    70: 12.776\n",
      "Loss after mini batch    70: 10.721\n",
      "Loss after mini batch    70: 13.002\n",
      "Starting epoch 71\n",
      "Loss after mini batch    71: 11.186\n",
      "Loss after mini batch    71: 13.154\n",
      "Loss after mini batch    71: 11.558\n",
      "Loss after mini batch    71: 13.489\n",
      "Loss after mini batch    71: 12.279\n",
      "Loss after mini batch    71: 12.257\n",
      "Loss after mini batch    71: 12.570\n",
      "Loss after mini batch    71: 12.701\n",
      "Loss after mini batch    71: 13.142\n",
      "Loss after mini batch    71: 13.046\n",
      "Loss after mini batch    71: 13.684\n",
      "Starting epoch 72\n",
      "Loss after mini batch    72: 13.082\n",
      "Loss after mini batch    72: 14.006\n",
      "Loss after mini batch    72: 12.707\n",
      "Loss after mini batch    72: 12.367\n",
      "Loss after mini batch    72: 12.886\n",
      "Loss after mini batch    72: 12.811\n",
      "Loss after mini batch    72: 11.834\n",
      "Loss after mini batch    72: 11.657\n",
      "Loss after mini batch    72: 13.159\n",
      "Loss after mini batch    72: 12.231\n",
      "Loss after mini batch    72: 11.119\n",
      "Starting epoch 73\n",
      "Loss after mini batch    73: 12.522\n",
      "Loss after mini batch    73: 13.603\n",
      "Loss after mini batch    73: 12.422\n",
      "Loss after mini batch    73: 12.768\n",
      "Loss after mini batch    73: 12.442\n",
      "Loss after mini batch    73: 12.358\n",
      "Loss after mini batch    73: 12.422\n",
      "Loss after mini batch    73: 12.528\n",
      "Loss after mini batch    73: 12.190\n",
      "Loss after mini batch    73: 12.139\n",
      "Loss after mini batch    73: 13.205\n",
      "Starting epoch 74\n",
      "Loss after mini batch    74: 12.986\n",
      "Loss after mini batch    74: 11.810\n",
      "Loss after mini batch    74: 12.549\n",
      "Loss after mini batch    74: 13.208\n",
      "Loss after mini batch    74: 12.671\n",
      "Loss after mini batch    74: 11.553\n",
      "Loss after mini batch    74: 12.718\n",
      "Loss after mini batch    74: 12.152\n",
      "Loss after mini batch    74: 10.787\n",
      "Loss after mini batch    74: 13.887\n",
      "Loss after mini batch    74: 12.474\n",
      "Starting epoch 75\n",
      "Loss after mini batch    75: 11.319\n",
      "Loss after mini batch    75: 12.879\n",
      "Loss after mini batch    75: 13.252\n",
      "Loss after mini batch    75: 12.598\n",
      "Loss after mini batch    75: 12.718\n",
      "Loss after mini batch    75: 12.169\n",
      "Loss after mini batch    75: 12.366\n",
      "Loss after mini batch    75: 12.771\n",
      "Loss after mini batch    75: 12.530\n",
      "Loss after mini batch    75: 12.019\n",
      "Loss after mini batch    75: 12.110\n",
      "Starting epoch 76\n",
      "Loss after mini batch    76: 13.170\n",
      "Loss after mini batch    76: 11.914\n",
      "Loss after mini batch    76: 12.447\n",
      "Loss after mini batch    76: 12.246\n",
      "Loss after mini batch    76: 14.008\n",
      "Loss after mini batch    76: 11.114\n",
      "Loss after mini batch    76: 12.085\n",
      "Loss after mini batch    76: 13.210\n",
      "Loss after mini batch    76: 13.473\n",
      "Loss after mini batch    76: 12.701\n",
      "Loss after mini batch    76: 14.442\n",
      "Starting epoch 77\n",
      "Loss after mini batch    77: 11.247\n",
      "Loss after mini batch    77: 11.866\n",
      "Loss after mini batch    77: 13.423\n",
      "Loss after mini batch    77: 12.721\n",
      "Loss after mini batch    77: 12.678\n",
      "Loss after mini batch    77: 12.984\n",
      "Loss after mini batch    77: 13.016\n",
      "Loss after mini batch    77: 10.938\n",
      "Loss after mini batch    77: 11.997\n",
      "Loss after mini batch    77: 13.225\n",
      "Loss after mini batch    77: 12.980\n",
      "Starting epoch 78\n",
      "Loss after mini batch    78: 11.236\n",
      "Loss after mini batch    78: 11.940\n",
      "Loss after mini batch    78: 13.562\n",
      "Loss after mini batch    78: 12.876\n",
      "Loss after mini batch    78: 13.031\n",
      "Loss after mini batch    78: 12.251\n",
      "Loss after mini batch    78: 12.245\n",
      "Loss after mini batch    78: 13.712\n",
      "Loss after mini batch    78: 12.202\n",
      "Loss after mini batch    78: 12.813\n",
      "Loss after mini batch    78: 13.467\n",
      "Starting epoch 79\n",
      "Loss after mini batch    79: 11.945\n",
      "Loss after mini batch    79: 11.975\n",
      "Loss after mini batch    79: 11.683\n",
      "Loss after mini batch    79: 12.323\n",
      "Loss after mini batch    79: 11.831\n",
      "Loss after mini batch    79: 12.806\n",
      "Loss after mini batch    79: 11.955\n",
      "Loss after mini batch    79: 12.991\n",
      "Loss after mini batch    79: 12.105\n",
      "Loss after mini batch    79: 13.617\n",
      "Loss after mini batch    79: 13.067\n",
      "Starting epoch 80\n",
      "Loss after mini batch    80: 13.259\n",
      "Loss after mini batch    80: 12.650\n",
      "Loss after mini batch    80: 12.257\n",
      "Loss after mini batch    80: 14.154\n",
      "Loss after mini batch    80: 12.667\n",
      "Loss after mini batch    80: 12.457\n",
      "Loss after mini batch    80: 11.150\n",
      "Loss after mini batch    80: 11.751\n",
      "Loss after mini batch    80: 12.170\n",
      "Loss after mini batch    80: 11.765\n",
      "Loss after mini batch    80: 12.743\n",
      "Starting epoch 81\n",
      "Loss after mini batch    81: 12.520\n",
      "Loss after mini batch    81: 12.886\n",
      "Loss after mini batch    81: 13.094\n",
      "Loss after mini batch    81: 12.306\n",
      "Loss after mini batch    81: 12.017\n",
      "Loss after mini batch    81: 12.623\n",
      "Loss after mini batch    81: 11.513\n",
      "Loss after mini batch    81: 12.833\n",
      "Loss after mini batch    81: 12.752\n",
      "Loss after mini batch    81: 11.519\n",
      "Loss after mini batch    81: 13.228\n",
      "Starting epoch 82\n",
      "Loss after mini batch    82: 11.207\n",
      "Loss after mini batch    82: 13.384\n",
      "Loss after mini batch    82: 12.947\n",
      "Loss after mini batch    82: 13.525\n",
      "Loss after mini batch    82: 12.135\n",
      "Loss after mini batch    82: 12.824\n",
      "Loss after mini batch    82: 12.673\n",
      "Loss after mini batch    82: 12.789\n",
      "Loss after mini batch    82: 12.316\n",
      "Loss after mini batch    82: 12.632\n",
      "Loss after mini batch    82: 11.196\n",
      "Starting epoch 83\n",
      "Loss after mini batch    83: 11.086\n",
      "Loss after mini batch    83: 13.342\n",
      "Loss after mini batch    83: 11.758\n",
      "Loss after mini batch    83: 11.745\n",
      "Loss after mini batch    83: 12.868\n",
      "Loss after mini batch    83: 11.789\n",
      "Loss after mini batch    83: 12.770\n",
      "Loss after mini batch    83: 13.477\n",
      "Loss after mini batch    83: 12.561\n",
      "Loss after mini batch    83: 11.761\n",
      "Loss after mini batch    83: 13.310\n",
      "Starting epoch 84\n",
      "Loss after mini batch    84: 11.357\n",
      "Loss after mini batch    84: 11.591\n",
      "Loss after mini batch    84: 12.495\n",
      "Loss after mini batch    84: 12.857\n",
      "Loss after mini batch    84: 10.510\n",
      "Loss after mini batch    84: 13.514\n",
      "Loss after mini batch    84: 12.807\n",
      "Loss after mini batch    84: 12.934\n",
      "Loss after mini batch    84: 13.388\n",
      "Loss after mini batch    84: 14.794\n",
      "Loss after mini batch    84: 12.529\n",
      "Starting epoch 85\n",
      "Loss after mini batch    85: 12.961\n",
      "Loss after mini batch    85: 11.729\n",
      "Loss after mini batch    85: 13.063\n",
      "Loss after mini batch    85: 13.194\n",
      "Loss after mini batch    85: 11.793\n",
      "Loss after mini batch    85: 11.504\n",
      "Loss after mini batch    85: 12.340\n",
      "Loss after mini batch    85: 13.649\n",
      "Loss after mini batch    85: 11.758\n",
      "Loss after mini batch    85: 11.298\n",
      "Loss after mini batch    85: 13.017\n",
      "Starting epoch 86\n",
      "Loss after mini batch    86: 12.249\n",
      "Loss after mini batch    86: 12.654\n",
      "Loss after mini batch    86: 12.424\n",
      "Loss after mini batch    86: 13.272\n",
      "Loss after mini batch    86: 11.670\n",
      "Loss after mini batch    86: 11.990\n",
      "Loss after mini batch    86: 12.928\n",
      "Loss after mini batch    86: 12.457\n",
      "Loss after mini batch    86: 12.752\n",
      "Loss after mini batch    86: 12.855\n",
      "Loss after mini batch    86: 12.383\n",
      "Starting epoch 87\n",
      "Loss after mini batch    87: 13.126\n",
      "Loss after mini batch    87: 12.587\n",
      "Loss after mini batch    87: 11.368\n",
      "Loss after mini batch    87: 11.975\n",
      "Loss after mini batch    87: 12.289\n",
      "Loss after mini batch    87: 13.057\n",
      "Loss after mini batch    87: 13.058\n",
      "Loss after mini batch    87: 10.997\n",
      "Loss after mini batch    87: 13.182\n",
      "Loss after mini batch    87: 12.247\n",
      "Loss after mini batch    87: 13.078\n",
      "Starting epoch 88\n",
      "Loss after mini batch    88: 11.897\n",
      "Loss after mini batch    88: 12.566\n",
      "Loss after mini batch    88: 11.692\n",
      "Loss after mini batch    88: 12.488\n",
      "Loss after mini batch    88: 11.804\n",
      "Loss after mini batch    88: 12.234\n",
      "Loss after mini batch    88: 14.564\n",
      "Loss after mini batch    88: 11.725\n",
      "Loss after mini batch    88: 13.738\n",
      "Loss after mini batch    88: 12.812\n",
      "Loss after mini batch    88: 11.938\n",
      "Starting epoch 89\n",
      "Loss after mini batch    89: 11.943\n",
      "Loss after mini batch    89: 13.274\n",
      "Loss after mini batch    89: 12.030\n",
      "Loss after mini batch    89: 12.796\n",
      "Loss after mini batch    89: 12.599\n",
      "Loss after mini batch    89: 12.922\n",
      "Loss after mini batch    89: 13.003\n",
      "Loss after mini batch    89: 11.385\n",
      "Loss after mini batch    89: 12.690\n",
      "Loss after mini batch    89: 12.065\n",
      "Loss after mini batch    89: 12.337\n",
      "Starting epoch 90\n",
      "Loss after mini batch    90: 12.946\n",
      "Loss after mini batch    90: 12.504\n",
      "Loss after mini batch    90: 12.381\n",
      "Loss after mini batch    90: 11.218\n",
      "Loss after mini batch    90: 11.410\n",
      "Loss after mini batch    90: 11.422\n",
      "Loss after mini batch    90: 11.622\n",
      "Loss after mini batch    90: 13.309\n",
      "Loss after mini batch    90: 12.490\n",
      "Loss after mini batch    90: 12.966\n",
      "Loss after mini batch    90: 12.461\n",
      "Starting epoch 91\n",
      "Loss after mini batch    91: 12.975\n",
      "Loss after mini batch    91: 11.408\n",
      "Loss after mini batch    91: 12.701\n",
      "Loss after mini batch    91: 12.438\n",
      "Loss after mini batch    91: 11.266\n",
      "Loss after mini batch    91: 12.209\n",
      "Loss after mini batch    91: 10.908\n",
      "Loss after mini batch    91: 13.310\n",
      "Loss after mini batch    91: 13.274\n",
      "Loss after mini batch    91: 12.775\n",
      "Loss after mini batch    91: 12.418\n",
      "Starting epoch 92\n",
      "Loss after mini batch    92: 11.555\n",
      "Loss after mini batch    92: 11.977\n",
      "Loss after mini batch    92: 11.910\n",
      "Loss after mini batch    92: 11.891\n",
      "Loss after mini batch    92: 11.423\n",
      "Loss after mini batch    92: 12.825\n",
      "Loss after mini batch    92: 11.911\n",
      "Loss after mini batch    92: 13.233\n",
      "Loss after mini batch    92: 12.307\n",
      "Loss after mini batch    92: 13.060\n",
      "Loss after mini batch    92: 11.562\n",
      "Starting epoch 93\n",
      "Loss after mini batch    93: 11.592\n",
      "Loss after mini batch    93: 13.966\n",
      "Loss after mini batch    93: 11.087\n",
      "Loss after mini batch    93: 11.800\n",
      "Loss after mini batch    93: 11.547\n",
      "Loss after mini batch    93: 12.155\n",
      "Loss after mini batch    93: 13.141\n",
      "Loss after mini batch    93: 11.559\n",
      "Loss after mini batch    93: 12.765\n",
      "Loss after mini batch    93: 12.203\n",
      "Loss after mini batch    93: 12.345\n",
      "Starting epoch 94\n",
      "Loss after mini batch    94: 13.852\n",
      "Loss after mini batch    94: 11.897\n",
      "Loss after mini batch    94: 11.978\n",
      "Loss after mini batch    94: 12.511\n",
      "Loss after mini batch    94: 11.853\n",
      "Loss after mini batch    94: 12.993\n",
      "Loss after mini batch    94: 11.502\n",
      "Loss after mini batch    94: 13.126\n",
      "Loss after mini batch    94: 11.623\n",
      "Loss after mini batch    94: 12.226\n",
      "Loss after mini batch    94: 11.429\n",
      "Starting epoch 95\n",
      "Loss after mini batch    95: 12.160\n",
      "Loss after mini batch    95: 12.433\n",
      "Loss after mini batch    95: 12.491\n",
      "Loss after mini batch    95: 12.032\n",
      "Loss after mini batch    95: 12.361\n",
      "Loss after mini batch    95: 13.308\n",
      "Loss after mini batch    95: 12.072\n",
      "Loss after mini batch    95: 13.261\n",
      "Loss after mini batch    95: 12.441\n",
      "Loss after mini batch    95: 13.168\n",
      "Loss after mini batch    95: 12.231\n",
      "Starting epoch 96\n",
      "Loss after mini batch    96: 12.308\n",
      "Loss after mini batch    96: 12.436\n",
      "Loss after mini batch    96: 12.552\n",
      "Loss after mini batch    96: 11.963\n",
      "Loss after mini batch    96: 12.291\n",
      "Loss after mini batch    96: 10.696\n",
      "Loss after mini batch    96: 12.136\n",
      "Loss after mini batch    96: 12.665\n",
      "Loss after mini batch    96: 12.175\n",
      "Loss after mini batch    96: 12.985\n",
      "Loss after mini batch    96: 12.516\n",
      "Starting epoch 97\n",
      "Loss after mini batch    97: 13.691\n",
      "Loss after mini batch    97: 11.717\n",
      "Loss after mini batch    97: 11.677\n",
      "Loss after mini batch    97: 15.078\n",
      "Loss after mini batch    97: 13.160\n",
      "Loss after mini batch    97: 11.903\n",
      "Loss after mini batch    97: 12.164\n",
      "Loss after mini batch    97: 11.759\n",
      "Loss after mini batch    97: 12.699\n",
      "Loss after mini batch    97: 11.763\n",
      "Loss after mini batch    97: 12.073\n",
      "Starting epoch 98\n",
      "Loss after mini batch    98: 14.290\n",
      "Loss after mini batch    98: 11.381\n",
      "Loss after mini batch    98: 12.463\n",
      "Loss after mini batch    98: 11.598\n",
      "Loss after mini batch    98: 12.346\n",
      "Loss after mini batch    98: 13.710\n",
      "Loss after mini batch    98: 12.712\n",
      "Loss after mini batch    98: 12.425\n",
      "Loss after mini batch    98: 11.726\n",
      "Loss after mini batch    98: 11.698\n",
      "Loss after mini batch    98: 12.977\n",
      "Starting epoch 99\n",
      "Loss after mini batch    99: 12.679\n",
      "Loss after mini batch    99: 13.548\n",
      "Loss after mini batch    99: 11.306\n",
      "Loss after mini batch    99: 12.414\n",
      "Loss after mini batch    99: 12.648\n",
      "Loss after mini batch    99: 11.105\n",
      "Loss after mini batch    99: 12.855\n",
      "Loss after mini batch    99: 13.160\n",
      "Loss after mini batch    99: 11.140\n",
      "Loss after mini batch    99: 12.960\n",
      "Loss after mini batch    99: 11.219\n",
      "Starting epoch 100\n",
      "Loss after mini batch   100: 12.695\n",
      "Loss after mini batch   100: 11.460\n",
      "Loss after mini batch   100: 12.907\n",
      "Loss after mini batch   100: 11.279\n",
      "Loss after mini batch   100: 13.103\n",
      "Loss after mini batch   100: 13.494\n",
      "Loss after mini batch   100: 12.263\n",
      "Loss after mini batch   100: 11.579\n",
      "Loss after mini batch   100: 11.733\n",
      "Loss after mini batch   100: 11.763\n",
      "Loss after mini batch   100: 11.779\n",
      "Training Complete\n",
      "Starting Testing\n",
      "RMSE for fold 1: 3.5177407912832637\n",
      "rRMSE for fold 1: 0.06818291804755172\n",
      "r for fold 1: 0.9908210265577593\n",
      "Fast RMSE for fold 1: 2.8854620937990116\n",
      "Fast rRMSE for fold 1: 0.05770748473684003\n",
      "Fast r for fold 1: 0.9867451451998198\n",
      "Slow RMSE for fold 1: 5.175350623300481\n",
      "Slow rRMSE for fold 1: 0.09334444674232321\n",
      "Slow r for fold 1: 0.997745673551039\n",
      "Regular RMSE for fold 1: 1.983934548825038\n",
      "Regular rRMSE for fold 1: 0.039761393299946074\n",
      "Regular r for fold 1: 0.9975624036253473\n",
      "Fold 2\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini batch     1: 406.241\n",
      "Loss after mini batch     1: 16.610\n",
      "Loss after mini batch     1: 17.522\n",
      "Loss after mini batch     1: 16.106\n",
      "Loss after mini batch     1: 16.958\n",
      "Loss after mini batch     1: 24.090\n",
      "Loss after mini batch     1: 19.467\n",
      "Loss after mini batch     1: 16.850\n",
      "Loss after mini batch     1: 19.676\n",
      "Loss after mini batch     1: 19.980\n",
      "Loss after mini batch     1: 22.804\n",
      "Starting epoch 2\n",
      "Loss after mini batch     2: 20.325\n",
      "Loss after mini batch     2: 20.058\n",
      "Loss after mini batch     2: 28.723\n",
      "Loss after mini batch     2: 25.195\n",
      "Loss after mini batch     2: 25.384\n",
      "Loss after mini batch     2: 43.240\n",
      "Loss after mini batch     2: 26.965\n",
      "Loss after mini batch     2: 15.519\n",
      "Loss after mini batch     2: 21.234\n",
      "Loss after mini batch     2: 21.715\n",
      "Loss after mini batch     2: 21.384\n",
      "Starting epoch 3\n",
      "Loss after mini batch     3: 18.470\n",
      "Loss after mini batch     3: 15.598\n",
      "Loss after mini batch     3: 17.123\n",
      "Loss after mini batch     3: 21.710\n",
      "Loss after mini batch     3: 26.233\n",
      "Loss after mini batch     3: 17.976\n",
      "Loss after mini batch     3: 20.863\n",
      "Loss after mini batch     3: 23.628\n",
      "Loss after mini batch     3: 16.538\n",
      "Loss after mini batch     3: 13.932\n",
      "Loss after mini batch     3: 25.656\n",
      "Starting epoch 4\n",
      "Loss after mini batch     4: 15.425\n",
      "Loss after mini batch     4: 16.601\n",
      "Loss after mini batch     4: 15.270\n",
      "Loss after mini batch     4: 17.122\n",
      "Loss after mini batch     4: 16.476\n",
      "Loss after mini batch     4: 15.827\n",
      "Loss after mini batch     4: 22.569\n",
      "Loss after mini batch     4: 15.734\n",
      "Loss after mini batch     4: 18.294\n",
      "Loss after mini batch     4: 20.801\n",
      "Loss after mini batch     4: 17.427\n",
      "Starting epoch 5\n",
      "Loss after mini batch     5: 17.244\n",
      "Loss after mini batch     5: 23.261\n",
      "Loss after mini batch     5: 17.105\n",
      "Loss after mini batch     5: 16.017\n",
      "Loss after mini batch     5: 28.817\n",
      "Loss after mini batch     5: 13.661\n",
      "Loss after mini batch     5: 14.467\n",
      "Loss after mini batch     5: 16.977\n",
      "Loss after mini batch     5: 19.403\n",
      "Loss after mini batch     5: 15.377\n",
      "Loss after mini batch     5: 15.080\n",
      "Starting epoch 6\n",
      "Loss after mini batch     6: 16.934\n",
      "Loss after mini batch     6: 14.270\n",
      "Loss after mini batch     6: 16.065\n",
      "Loss after mini batch     6: 15.356\n",
      "Loss after mini batch     6: 12.829\n",
      "Loss after mini batch     6: 20.188\n",
      "Loss after mini batch     6: 16.780\n",
      "Loss after mini batch     6: 13.819\n",
      "Loss after mini batch     6: 21.388\n",
      "Loss after mini batch     6: 16.108\n",
      "Loss after mini batch     6: 13.874\n",
      "Starting epoch 7\n",
      "Loss after mini batch     7: 16.006\n",
      "Loss after mini batch     7: 13.924\n",
      "Loss after mini batch     7: 18.337\n",
      "Loss after mini batch     7: 17.025\n",
      "Loss after mini batch     7: 14.687\n",
      "Loss after mini batch     7: 15.456\n",
      "Loss after mini batch     7: 21.096\n",
      "Loss after mini batch     7: 17.397\n",
      "Loss after mini batch     7: 15.313\n",
      "Loss after mini batch     7: 15.327\n",
      "Loss after mini batch     7: 16.506\n",
      "Starting epoch 8\n",
      "Loss after mini batch     8: 13.413\n",
      "Loss after mini batch     8: 13.695\n",
      "Loss after mini batch     8: 15.668\n",
      "Loss after mini batch     8: 14.524\n",
      "Loss after mini batch     8: 15.244\n",
      "Loss after mini batch     8: 17.787\n",
      "Loss after mini batch     8: 16.489\n",
      "Loss after mini batch     8: 15.591\n",
      "Loss after mini batch     8: 15.894\n",
      "Loss after mini batch     8: 14.396\n",
      "Loss after mini batch     8: 15.848\n",
      "Starting epoch 9\n",
      "Loss after mini batch     9: 14.001\n",
      "Loss after mini batch     9: 15.686\n",
      "Loss after mini batch     9: 13.950\n",
      "Loss after mini batch     9: 13.879\n",
      "Loss after mini batch     9: 14.116\n",
      "Loss after mini batch     9: 13.122\n",
      "Loss after mini batch     9: 14.923\n",
      "Loss after mini batch     9: 13.478\n",
      "Loss after mini batch     9: 15.278\n",
      "Loss after mini batch     9: 14.964\n",
      "Loss after mini batch     9: 15.592\n",
      "Starting epoch 10\n",
      "Loss after mini batch    10: 15.442\n",
      "Loss after mini batch    10: 15.268\n",
      "Loss after mini batch    10: 13.665\n",
      "Loss after mini batch    10: 13.930\n",
      "Loss after mini batch    10: 15.965\n",
      "Loss after mini batch    10: 17.376\n",
      "Loss after mini batch    10: 14.804\n",
      "Loss after mini batch    10: 14.802\n",
      "Loss after mini batch    10: 14.154\n",
      "Loss after mini batch    10: 14.535\n",
      "Loss after mini batch    10: 14.689\n",
      "Starting epoch 11\n",
      "Loss after mini batch    11: 13.738\n",
      "Loss after mini batch    11: 16.705\n",
      "Loss after mini batch    11: 16.461\n",
      "Loss after mini batch    11: 15.180\n",
      "Loss after mini batch    11: 13.886\n",
      "Loss after mini batch    11: 15.342\n",
      "Loss after mini batch    11: 15.722\n",
      "Loss after mini batch    11: 16.746\n",
      "Loss after mini batch    11: 14.918\n",
      "Loss after mini batch    11: 17.607\n",
      "Loss after mini batch    11: 13.525\n",
      "Starting epoch 12\n",
      "Loss after mini batch    12: 14.064\n",
      "Loss after mini batch    12: 14.162\n",
      "Loss after mini batch    12: 14.828\n",
      "Loss after mini batch    12: 15.045\n",
      "Loss after mini batch    12: 13.947\n",
      "Loss after mini batch    12: 16.301\n",
      "Loss after mini batch    12: 13.542\n",
      "Loss after mini batch    12: 14.525\n",
      "Loss after mini batch    12: 14.622\n",
      "Loss after mini batch    12: 14.418\n",
      "Loss after mini batch    12: 14.906\n",
      "Starting epoch 13\n",
      "Loss after mini batch    13: 12.078\n",
      "Loss after mini batch    13: 11.977\n",
      "Loss after mini batch    13: 13.287\n",
      "Loss after mini batch    13: 12.568\n",
      "Loss after mini batch    13: 13.910\n",
      "Loss after mini batch    13: 16.231\n",
      "Loss after mini batch    13: 13.551\n",
      "Loss after mini batch    13: 14.602\n",
      "Loss after mini batch    13: 14.476\n",
      "Loss after mini batch    13: 16.502\n",
      "Loss after mini batch    13: 13.991\n",
      "Starting epoch 14\n",
      "Loss after mini batch    14: 14.864\n",
      "Loss after mini batch    14: 13.411\n",
      "Loss after mini batch    14: 12.752\n",
      "Loss after mini batch    14: 14.220\n",
      "Loss after mini batch    14: 14.752\n",
      "Loss after mini batch    14: 15.175\n",
      "Loss after mini batch    14: 14.049\n",
      "Loss after mini batch    14: 15.420\n",
      "Loss after mini batch    14: 15.344\n",
      "Loss after mini batch    14: 14.085\n",
      "Loss after mini batch    14: 13.829\n",
      "Starting epoch 15\n",
      "Loss after mini batch    15: 13.559\n",
      "Loss after mini batch    15: 14.214\n",
      "Loss after mini batch    15: 13.266\n",
      "Loss after mini batch    15: 14.416\n",
      "Loss after mini batch    15: 13.330\n",
      "Loss after mini batch    15: 14.388\n",
      "Loss after mini batch    15: 13.761\n",
      "Loss after mini batch    15: 13.666\n",
      "Loss after mini batch    15: 14.252\n",
      "Loss after mini batch    15: 15.072\n",
      "Loss after mini batch    15: 13.098\n",
      "Starting epoch 16\n",
      "Loss after mini batch    16: 13.438\n",
      "Loss after mini batch    16: 14.925\n",
      "Loss after mini batch    16: 14.013\n",
      "Loss after mini batch    16: 15.525\n",
      "Loss after mini batch    16: 13.843\n",
      "Loss after mini batch    16: 12.552\n",
      "Loss after mini batch    16: 14.456\n",
      "Loss after mini batch    16: 13.864\n",
      "Loss after mini batch    16: 14.487\n",
      "Loss after mini batch    16: 16.687\n",
      "Loss after mini batch    16: 14.364\n",
      "Starting epoch 17\n",
      "Loss after mini batch    17: 12.692\n",
      "Loss after mini batch    17: 13.409\n",
      "Loss after mini batch    17: 13.840\n",
      "Loss after mini batch    17: 13.197\n",
      "Loss after mini batch    17: 14.380\n",
      "Loss after mini batch    17: 14.981\n",
      "Loss after mini batch    17: 14.867\n",
      "Loss after mini batch    17: 14.309\n",
      "Loss after mini batch    17: 13.700\n",
      "Loss after mini batch    17: 14.812\n",
      "Loss after mini batch    17: 13.231\n",
      "Starting epoch 18\n",
      "Loss after mini batch    18: 13.915\n",
      "Loss after mini batch    18: 14.494\n",
      "Loss after mini batch    18: 13.644\n",
      "Loss after mini batch    18: 13.553\n",
      "Loss after mini batch    18: 16.112\n",
      "Loss after mini batch    18: 13.834\n",
      "Loss after mini batch    18: 13.651\n",
      "Loss after mini batch    18: 13.947\n",
      "Loss after mini batch    18: 14.512\n",
      "Loss after mini batch    18: 15.871\n",
      "Loss after mini batch    18: 13.766\n",
      "Starting epoch 19\n",
      "Loss after mini batch    19: 14.227\n",
      "Loss after mini batch    19: 13.498\n",
      "Loss after mini batch    19: 15.543\n",
      "Loss after mini batch    19: 12.417\n",
      "Loss after mini batch    19: 13.962\n",
      "Loss after mini batch    19: 11.715\n",
      "Loss after mini batch    19: 15.077\n",
      "Loss after mini batch    19: 14.333\n",
      "Loss after mini batch    19: 13.415\n",
      "Loss after mini batch    19: 13.763\n",
      "Loss after mini batch    19: 13.163\n",
      "Starting epoch 20\n",
      "Loss after mini batch    20: 14.388\n",
      "Loss after mini batch    20: 12.913\n",
      "Loss after mini batch    20: 13.113\n",
      "Loss after mini batch    20: 14.083\n",
      "Loss after mini batch    20: 13.442\n",
      "Loss after mini batch    20: 15.283\n",
      "Loss after mini batch    20: 13.651\n",
      "Loss after mini batch    20: 13.246\n",
      "Loss after mini batch    20: 14.280\n",
      "Loss after mini batch    20: 14.405\n",
      "Loss after mini batch    20: 14.707\n",
      "Starting epoch 21\n",
      "Loss after mini batch    21: 13.357\n",
      "Loss after mini batch    21: 13.440\n",
      "Loss after mini batch    21: 14.511\n",
      "Loss after mini batch    21: 14.459\n",
      "Loss after mini batch    21: 14.039\n",
      "Loss after mini batch    21: 14.254\n",
      "Loss after mini batch    21: 13.928\n",
      "Loss after mini batch    21: 13.586\n",
      "Loss after mini batch    21: 13.681\n",
      "Loss after mini batch    21: 13.141\n",
      "Loss after mini batch    21: 13.500\n",
      "Starting epoch 22\n",
      "Loss after mini batch    22: 11.923\n",
      "Loss after mini batch    22: 14.067\n",
      "Loss after mini batch    22: 14.625\n",
      "Loss after mini batch    22: 14.613\n",
      "Loss after mini batch    22: 14.488\n",
      "Loss after mini batch    22: 14.155\n",
      "Loss after mini batch    22: 15.033\n",
      "Loss after mini batch    22: 14.809\n",
      "Loss after mini batch    22: 14.250\n",
      "Loss after mini batch    22: 14.372\n",
      "Loss after mini batch    22: 12.705\n",
      "Starting epoch 23\n",
      "Loss after mini batch    23: 14.282\n",
      "Loss after mini batch    23: 13.773\n",
      "Loss after mini batch    23: 15.081\n",
      "Loss after mini batch    23: 12.447\n",
      "Loss after mini batch    23: 15.115\n",
      "Loss after mini batch    23: 15.196\n",
      "Loss after mini batch    23: 13.798\n",
      "Loss after mini batch    23: 13.628\n",
      "Loss after mini batch    23: 15.438\n",
      "Loss after mini batch    23: 13.843\n",
      "Loss after mini batch    23: 12.533\n",
      "Starting epoch 24\n",
      "Loss after mini batch    24: 14.157\n",
      "Loss after mini batch    24: 12.373\n",
      "Loss after mini batch    24: 12.326\n",
      "Loss after mini batch    24: 13.793\n",
      "Loss after mini batch    24: 13.665\n",
      "Loss after mini batch    24: 13.710\n",
      "Loss after mini batch    24: 13.378\n",
      "Loss after mini batch    24: 13.439\n",
      "Loss after mini batch    24: 12.343\n",
      "Loss after mini batch    24: 14.183\n",
      "Loss after mini batch    24: 15.085\n",
      "Starting epoch 25\n",
      "Loss after mini batch    25: 12.365\n",
      "Loss after mini batch    25: 16.956\n",
      "Loss after mini batch    25: 12.873\n",
      "Loss after mini batch    25: 13.085\n",
      "Loss after mini batch    25: 14.793\n",
      "Loss after mini batch    25: 13.542\n",
      "Loss after mini batch    25: 14.587\n",
      "Loss after mini batch    25: 15.706\n",
      "Loss after mini batch    25: 15.472\n",
      "Loss after mini batch    25: 13.281\n",
      "Loss after mini batch    25: 13.917\n",
      "Starting epoch 26\n",
      "Loss after mini batch    26: 12.187\n",
      "Loss after mini batch    26: 16.198\n",
      "Loss after mini batch    26: 13.157\n",
      "Loss after mini batch    26: 13.618\n",
      "Loss after mini batch    26: 12.542\n",
      "Loss after mini batch    26: 12.735\n",
      "Loss after mini batch    26: 12.968\n",
      "Loss after mini batch    26: 15.620\n",
      "Loss after mini batch    26: 13.258\n",
      "Loss after mini batch    26: 14.485\n",
      "Loss after mini batch    26: 13.846\n",
      "Starting epoch 27\n",
      "Loss after mini batch    27: 13.256\n",
      "Loss after mini batch    27: 14.068\n",
      "Loss after mini batch    27: 13.049\n",
      "Loss after mini batch    27: 12.325\n",
      "Loss after mini batch    27: 14.113\n",
      "Loss after mini batch    27: 13.031\n",
      "Loss after mini batch    27: 13.873\n",
      "Loss after mini batch    27: 13.773\n",
      "Loss after mini batch    27: 13.944\n",
      "Loss after mini batch    27: 12.865\n",
      "Loss after mini batch    27: 14.032\n",
      "Starting epoch 28\n",
      "Loss after mini batch    28: 13.071\n",
      "Loss after mini batch    28: 12.104\n",
      "Loss after mini batch    28: 13.883\n",
      "Loss after mini batch    28: 13.000\n",
      "Loss after mini batch    28: 13.498\n",
      "Loss after mini batch    28: 15.157\n",
      "Loss after mini batch    28: 12.404\n",
      "Loss after mini batch    28: 14.203\n",
      "Loss after mini batch    28: 13.981\n",
      "Loss after mini batch    28: 12.714\n",
      "Loss after mini batch    28: 13.185\n",
      "Starting epoch 29\n",
      "Loss after mini batch    29: 12.530\n",
      "Loss after mini batch    29: 13.704\n",
      "Loss after mini batch    29: 14.651\n",
      "Loss after mini batch    29: 12.954\n",
      "Loss after mini batch    29: 12.990\n",
      "Loss after mini batch    29: 13.014\n",
      "Loss after mini batch    29: 13.652\n",
      "Loss after mini batch    29: 11.904\n",
      "Loss after mini batch    29: 14.479\n",
      "Loss after mini batch    29: 13.003\n",
      "Loss after mini batch    29: 12.413\n",
      "Starting epoch 30\n",
      "Loss after mini batch    30: 13.976\n",
      "Loss after mini batch    30: 12.837\n",
      "Loss after mini batch    30: 13.443\n",
      "Loss after mini batch    30: 12.385\n",
      "Loss after mini batch    30: 13.193\n",
      "Loss after mini batch    30: 14.465\n",
      "Loss after mini batch    30: 16.861\n",
      "Loss after mini batch    30: 14.134\n",
      "Loss after mini batch    30: 13.311\n",
      "Loss after mini batch    30: 11.817\n",
      "Loss after mini batch    30: 13.091\n",
      "Starting epoch 31\n",
      "Loss after mini batch    31: 13.361\n",
      "Loss after mini batch    31: 14.287\n",
      "Loss after mini batch    31: 13.784\n",
      "Loss after mini batch    31: 12.023\n",
      "Loss after mini batch    31: 14.357\n",
      "Loss after mini batch    31: 12.751\n",
      "Loss after mini batch    31: 14.399\n",
      "Loss after mini batch    31: 12.604\n",
      "Loss after mini batch    31: 14.375\n",
      "Loss after mini batch    31: 13.621\n",
      "Loss after mini batch    31: 11.897\n",
      "Starting epoch 32\n",
      "Loss after mini batch    32: 13.012\n",
      "Loss after mini batch    32: 14.060\n",
      "Loss after mini batch    32: 13.969\n",
      "Loss after mini batch    32: 13.630\n",
      "Loss after mini batch    32: 13.876\n",
      "Loss after mini batch    32: 12.948\n",
      "Loss after mini batch    32: 12.018\n",
      "Loss after mini batch    32: 13.733\n",
      "Loss after mini batch    32: 12.887\n",
      "Loss after mini batch    32: 14.093\n",
      "Loss after mini batch    32: 14.793\n",
      "Starting epoch 33\n",
      "Loss after mini batch    33: 11.046\n",
      "Loss after mini batch    33: 13.358\n",
      "Loss after mini batch    33: 14.386\n",
      "Loss after mini batch    33: 13.286\n",
      "Loss after mini batch    33: 13.910\n",
      "Loss after mini batch    33: 12.470\n",
      "Loss after mini batch    33: 13.563\n",
      "Loss after mini batch    33: 14.936\n",
      "Loss after mini batch    33: 14.221\n",
      "Loss after mini batch    33: 12.893\n",
      "Loss after mini batch    33: 15.414\n",
      "Starting epoch 34\n",
      "Loss after mini batch    34: 12.537\n",
      "Loss after mini batch    34: 13.718\n",
      "Loss after mini batch    34: 13.167\n",
      "Loss after mini batch    34: 12.540\n",
      "Loss after mini batch    34: 12.907\n",
      "Loss after mini batch    34: 13.373\n",
      "Loss after mini batch    34: 14.425\n",
      "Loss after mini batch    34: 11.844\n",
      "Loss after mini batch    34: 12.906\n",
      "Loss after mini batch    34: 13.540\n",
      "Loss after mini batch    34: 13.399\n",
      "Starting epoch 35\n",
      "Loss after mini batch    35: 12.678\n",
      "Loss after mini batch    35: 13.482\n",
      "Loss after mini batch    35: 15.055\n",
      "Loss after mini batch    35: 14.240\n",
      "Loss after mini batch    35: 14.633\n",
      "Loss after mini batch    35: 12.288\n",
      "Loss after mini batch    35: 12.053\n",
      "Loss after mini batch    35: 13.141\n",
      "Loss after mini batch    35: 11.694\n",
      "Loss after mini batch    35: 13.126\n",
      "Loss after mini batch    35: 14.352\n",
      "Starting epoch 36\n",
      "Loss after mini batch    36: 14.947\n",
      "Loss after mini batch    36: 12.926\n",
      "Loss after mini batch    36: 13.453\n",
      "Loss after mini batch    36: 12.759\n",
      "Loss after mini batch    36: 13.391\n",
      "Loss after mini batch    36: 11.540\n",
      "Loss after mini batch    36: 13.333\n",
      "Loss after mini batch    36: 13.822\n",
      "Loss after mini batch    36: 13.362\n",
      "Loss after mini batch    36: 13.457\n",
      "Loss after mini batch    36: 12.490\n",
      "Starting epoch 37\n",
      "Loss after mini batch    37: 14.255\n",
      "Loss after mini batch    37: 12.676\n",
      "Loss after mini batch    37: 12.813\n",
      "Loss after mini batch    37: 12.902\n",
      "Loss after mini batch    37: 14.237\n",
      "Loss after mini batch    37: 13.781\n",
      "Loss after mini batch    37: 12.341\n",
      "Loss after mini batch    37: 11.783\n",
      "Loss after mini batch    37: 13.007\n",
      "Loss after mini batch    37: 13.821\n",
      "Loss after mini batch    37: 14.796\n",
      "Starting epoch 38\n",
      "Loss after mini batch    38: 13.727\n",
      "Loss after mini batch    38: 13.601\n",
      "Loss after mini batch    38: 13.212\n",
      "Loss after mini batch    38: 14.339\n",
      "Loss after mini batch    38: 13.128\n",
      "Loss after mini batch    38: 13.804\n",
      "Loss after mini batch    38: 12.540\n",
      "Loss after mini batch    38: 12.034\n",
      "Loss after mini batch    38: 13.773\n",
      "Loss after mini batch    38: 14.160\n",
      "Loss after mini batch    38: 14.444\n",
      "Starting epoch 39\n",
      "Loss after mini batch    39: 13.672\n",
      "Loss after mini batch    39: 12.774\n",
      "Loss after mini batch    39: 12.302\n",
      "Loss after mini batch    39: 12.323\n",
      "Loss after mini batch    39: 12.351\n",
      "Loss after mini batch    39: 12.470\n",
      "Loss after mini batch    39: 13.692\n",
      "Loss after mini batch    39: 14.855\n",
      "Loss after mini batch    39: 12.811\n",
      "Loss after mini batch    39: 12.694\n",
      "Loss after mini batch    39: 12.657\n",
      "Starting epoch 40\n",
      "Loss after mini batch    40: 13.417\n",
      "Loss after mini batch    40: 13.099\n",
      "Loss after mini batch    40: 12.952\n",
      "Loss after mini batch    40: 11.562\n",
      "Loss after mini batch    40: 12.512\n",
      "Loss after mini batch    40: 13.846\n",
      "Loss after mini batch    40: 14.194\n",
      "Loss after mini batch    40: 12.096\n",
      "Loss after mini batch    40: 13.352\n",
      "Loss after mini batch    40: 13.518\n",
      "Loss after mini batch    40: 12.532\n",
      "Starting epoch 41\n",
      "Loss after mini batch    41: 13.867\n",
      "Loss after mini batch    41: 14.118\n",
      "Loss after mini batch    41: 11.686\n",
      "Loss after mini batch    41: 13.995\n",
      "Loss after mini batch    41: 13.699\n",
      "Loss after mini batch    41: 12.493\n",
      "Loss after mini batch    41: 13.045\n",
      "Loss after mini batch    41: 14.555\n",
      "Loss after mini batch    41: 12.510\n",
      "Loss after mini batch    41: 13.788\n",
      "Loss after mini batch    41: 13.335\n",
      "Starting epoch 42\n",
      "Loss after mini batch    42: 12.599\n",
      "Loss after mini batch    42: 15.268\n",
      "Loss after mini batch    42: 12.710\n",
      "Loss after mini batch    42: 12.315\n",
      "Loss after mini batch    42: 13.115\n",
      "Loss after mini batch    42: 14.693\n",
      "Loss after mini batch    42: 12.251\n",
      "Loss after mini batch    42: 12.581\n",
      "Loss after mini batch    42: 13.829\n",
      "Loss after mini batch    42: 11.520\n",
      "Loss after mini batch    42: 12.931\n",
      "Starting epoch 43\n",
      "Loss after mini batch    43: 12.572\n",
      "Loss after mini batch    43: 12.929\n",
      "Loss after mini batch    43: 14.251\n",
      "Loss after mini batch    43: 13.827\n",
      "Loss after mini batch    43: 13.715\n",
      "Loss after mini batch    43: 12.784\n",
      "Loss after mini batch    43: 11.682\n",
      "Loss after mini batch    43: 11.413\n",
      "Loss after mini batch    43: 12.186\n",
      "Loss after mini batch    43: 13.887\n",
      "Loss after mini batch    43: 13.648\n",
      "Starting epoch 44\n",
      "Loss after mini batch    44: 13.240\n",
      "Loss after mini batch    44: 13.300\n",
      "Loss after mini batch    44: 14.004\n",
      "Loss after mini batch    44: 12.026\n",
      "Loss after mini batch    44: 12.956\n",
      "Loss after mini batch    44: 14.089\n",
      "Loss after mini batch    44: 14.203\n",
      "Loss after mini batch    44: 12.962\n",
      "Loss after mini batch    44: 13.462\n",
      "Loss after mini batch    44: 13.311\n",
      "Loss after mini batch    44: 12.160\n",
      "Starting epoch 45\n",
      "Loss after mini batch    45: 12.936\n",
      "Loss after mini batch    45: 11.999\n",
      "Loss after mini batch    45: 13.635\n",
      "Loss after mini batch    45: 13.392\n",
      "Loss after mini batch    45: 12.255\n",
      "Loss after mini batch    45: 13.475\n",
      "Loss after mini batch    45: 12.474\n",
      "Loss after mini batch    45: 14.139\n",
      "Loss after mini batch    45: 13.198\n",
      "Loss after mini batch    45: 12.095\n",
      "Loss after mini batch    45: 13.330\n",
      "Starting epoch 46\n",
      "Loss after mini batch    46: 12.711\n",
      "Loss after mini batch    46: 13.129\n",
      "Loss after mini batch    46: 12.819\n",
      "Loss after mini batch    46: 11.468\n",
      "Loss after mini batch    46: 13.216\n",
      "Loss after mini batch    46: 13.427\n",
      "Loss after mini batch    46: 12.555\n",
      "Loss after mini batch    46: 12.621\n",
      "Loss after mini batch    46: 13.430\n",
      "Loss after mini batch    46: 13.737\n",
      "Loss after mini batch    46: 11.538\n",
      "Starting epoch 47\n",
      "Loss after mini batch    47: 12.604\n",
      "Loss after mini batch    47: 13.351\n",
      "Loss after mini batch    47: 12.355\n",
      "Loss after mini batch    47: 13.524\n",
      "Loss after mini batch    47: 12.796\n",
      "Loss after mini batch    47: 12.407\n",
      "Loss after mini batch    47: 11.936\n",
      "Loss after mini batch    47: 13.453\n",
      "Loss after mini batch    47: 13.308\n",
      "Loss after mini batch    47: 12.362\n",
      "Loss after mini batch    47: 12.905\n",
      "Starting epoch 48\n",
      "Loss after mini batch    48: 12.401\n",
      "Loss after mini batch    48: 14.371\n",
      "Loss after mini batch    48: 12.329\n",
      "Loss after mini batch    48: 12.400\n",
      "Loss after mini batch    48: 12.622\n",
      "Loss after mini batch    48: 12.944\n",
      "Loss after mini batch    48: 12.632\n",
      "Loss after mini batch    48: 12.575\n",
      "Loss after mini batch    48: 13.300\n",
      "Loss after mini batch    48: 14.197\n",
      "Loss after mini batch    48: 11.960\n",
      "Starting epoch 49\n",
      "Loss after mini batch    49: 13.424\n",
      "Loss after mini batch    49: 11.923\n",
      "Loss after mini batch    49: 12.670\n",
      "Loss after mini batch    49: 13.098\n",
      "Loss after mini batch    49: 12.436\n",
      "Loss after mini batch    49: 14.016\n",
      "Loss after mini batch    49: 11.938\n",
      "Loss after mini batch    49: 12.667\n",
      "Loss after mini batch    49: 12.283\n",
      "Loss after mini batch    49: 12.144\n",
      "Loss after mini batch    49: 12.209\n",
      "Starting epoch 50\n",
      "Loss after mini batch    50: 12.743\n",
      "Loss after mini batch    50: 12.166\n",
      "Loss after mini batch    50: 12.600\n",
      "Loss after mini batch    50: 13.288\n",
      "Loss after mini batch    50: 13.157\n",
      "Loss after mini batch    50: 13.127\n",
      "Loss after mini batch    50: 12.512\n",
      "Loss after mini batch    50: 12.460\n",
      "Loss after mini batch    50: 13.970\n",
      "Loss after mini batch    50: 13.016\n",
      "Loss after mini batch    50: 11.764\n",
      "Starting epoch 51\n",
      "Loss after mini batch    51: 13.654\n",
      "Loss after mini batch    51: 12.933\n",
      "Loss after mini batch    51: 14.440\n",
      "Loss after mini batch    51: 12.035\n",
      "Loss after mini batch    51: 13.543\n",
      "Loss after mini batch    51: 11.769\n",
      "Loss after mini batch    51: 13.369\n",
      "Loss after mini batch    51: 11.960\n",
      "Loss after mini batch    51: 13.317\n",
      "Loss after mini batch    51: 12.039\n",
      "Loss after mini batch    51: 12.986\n",
      "Starting epoch 52\n",
      "Loss after mini batch    52: 12.699\n",
      "Loss after mini batch    52: 11.819\n",
      "Loss after mini batch    52: 11.798\n",
      "Loss after mini batch    52: 14.084\n",
      "Loss after mini batch    52: 14.126\n",
      "Loss after mini batch    52: 12.209\n",
      "Loss after mini batch    52: 13.036\n",
      "Loss after mini batch    52: 11.458\n",
      "Loss after mini batch    52: 13.413\n",
      "Loss after mini batch    52: 13.840\n",
      "Loss after mini batch    52: 13.062\n",
      "Starting epoch 53\n",
      "Loss after mini batch    53: 11.612\n",
      "Loss after mini batch    53: 12.865\n",
      "Loss after mini batch    53: 12.800\n",
      "Loss after mini batch    53: 12.467\n",
      "Loss after mini batch    53: 11.554\n",
      "Loss after mini batch    53: 12.328\n",
      "Loss after mini batch    53: 12.884\n",
      "Loss after mini batch    53: 14.657\n",
      "Loss after mini batch    53: 13.003\n",
      "Loss after mini batch    53: 13.159\n",
      "Loss after mini batch    53: 11.398\n",
      "Starting epoch 54\n",
      "Loss after mini batch    54: 12.244\n",
      "Loss after mini batch    54: 12.820\n",
      "Loss after mini batch    54: 13.401\n",
      "Loss after mini batch    54: 11.782\n",
      "Loss after mini batch    54: 12.627\n",
      "Loss after mini batch    54: 13.381\n",
      "Loss after mini batch    54: 13.462\n",
      "Loss after mini batch    54: 13.740\n",
      "Loss after mini batch    54: 12.345\n",
      "Loss after mini batch    54: 12.490\n",
      "Loss after mini batch    54: 13.383\n",
      "Starting epoch 55\n",
      "Loss after mini batch    55: 12.028\n",
      "Loss after mini batch    55: 12.426\n",
      "Loss after mini batch    55: 11.537\n",
      "Loss after mini batch    55: 12.502\n",
      "Loss after mini batch    55: 12.829\n",
      "Loss after mini batch    55: 13.470\n",
      "Loss after mini batch    55: 12.820\n",
      "Loss after mini batch    55: 11.866\n",
      "Loss after mini batch    55: 13.718\n",
      "Loss after mini batch    55: 13.734\n",
      "Loss after mini batch    55: 13.662\n",
      "Starting epoch 56\n",
      "Loss after mini batch    56: 13.411\n",
      "Loss after mini batch    56: 13.412\n",
      "Loss after mini batch    56: 13.458\n",
      "Loss after mini batch    56: 11.456\n",
      "Loss after mini batch    56: 12.389\n",
      "Loss after mini batch    56: 12.036\n",
      "Loss after mini batch    56: 11.759\n",
      "Loss after mini batch    56: 12.600\n",
      "Loss after mini batch    56: 12.383\n",
      "Loss after mini batch    56: 12.387\n",
      "Loss after mini batch    56: 11.860\n",
      "Starting epoch 57\n",
      "Loss after mini batch    57: 11.757\n",
      "Loss after mini batch    57: 11.405\n",
      "Loss after mini batch    57: 12.719\n",
      "Loss after mini batch    57: 12.874\n",
      "Loss after mini batch    57: 13.942\n",
      "Loss after mini batch    57: 13.075\n",
      "Loss after mini batch    57: 12.297\n",
      "Loss after mini batch    57: 12.284\n",
      "Loss after mini batch    57: 13.644\n",
      "Loss after mini batch    57: 11.583\n",
      "Loss after mini batch    57: 12.846\n",
      "Starting epoch 58\n",
      "Loss after mini batch    58: 12.772\n",
      "Loss after mini batch    58: 13.619\n",
      "Loss after mini batch    58: 13.250\n",
      "Loss after mini batch    58: 11.923\n",
      "Loss after mini batch    58: 12.460\n",
      "Loss after mini batch    58: 14.852\n",
      "Loss after mini batch    58: 11.799\n",
      "Loss after mini batch    58: 12.789\n",
      "Loss after mini batch    58: 12.749\n",
      "Loss after mini batch    58: 12.484\n",
      "Loss after mini batch    58: 13.245\n",
      "Starting epoch 59\n",
      "Loss after mini batch    59: 11.760\n",
      "Loss after mini batch    59: 11.714\n",
      "Loss after mini batch    59: 12.227\n",
      "Loss after mini batch    59: 13.537\n",
      "Loss after mini batch    59: 12.951\n",
      "Loss after mini batch    59: 12.318\n",
      "Loss after mini batch    59: 13.352\n",
      "Loss after mini batch    59: 13.351\n",
      "Loss after mini batch    59: 13.533\n",
      "Loss after mini batch    59: 11.495\n",
      "Loss after mini batch    59: 12.324\n",
      "Starting epoch 60\n",
      "Loss after mini batch    60: 12.680\n",
      "Loss after mini batch    60: 12.231\n",
      "Loss after mini batch    60: 11.901\n",
      "Loss after mini batch    60: 13.327\n",
      "Loss after mini batch    60: 12.075\n",
      "Loss after mini batch    60: 13.094\n",
      "Loss after mini batch    60: 14.087\n",
      "Loss after mini batch    60: 11.998\n",
      "Loss after mini batch    60: 12.270\n",
      "Loss after mini batch    60: 11.493\n",
      "Loss after mini batch    60: 12.315\n",
      "Starting epoch 61\n",
      "Loss after mini batch    61: 11.655\n",
      "Loss after mini batch    61: 12.714\n",
      "Loss after mini batch    61: 13.804\n",
      "Loss after mini batch    61: 13.180\n",
      "Loss after mini batch    61: 12.427\n",
      "Loss after mini batch    61: 12.451\n",
      "Loss after mini batch    61: 13.521\n",
      "Loss after mini batch    61: 14.248\n",
      "Loss after mini batch    61: 11.894\n",
      "Loss after mini batch    61: 12.583\n",
      "Loss after mini batch    61: 13.693\n",
      "Starting epoch 62\n",
      "Loss after mini batch    62: 12.658\n",
      "Loss after mini batch    62: 13.757\n",
      "Loss after mini batch    62: 12.580\n",
      "Loss after mini batch    62: 11.113\n",
      "Loss after mini batch    62: 12.982\n",
      "Loss after mini batch    62: 12.913\n",
      "Loss after mini batch    62: 12.028\n",
      "Loss after mini batch    62: 13.265\n",
      "Loss after mini batch    62: 12.993\n",
      "Loss after mini batch    62: 11.777\n",
      "Loss after mini batch    62: 11.895\n",
      "Starting epoch 63\n",
      "Loss after mini batch    63: 12.664\n",
      "Loss after mini batch    63: 12.903\n",
      "Loss after mini batch    63: 14.864\n",
      "Loss after mini batch    63: 11.762\n",
      "Loss after mini batch    63: 12.383\n",
      "Loss after mini batch    63: 11.988\n",
      "Loss after mini batch    63: 12.808\n",
      "Loss after mini batch    63: 11.865\n",
      "Loss after mini batch    63: 10.780\n",
      "Loss after mini batch    63: 12.772\n",
      "Loss after mini batch    63: 13.639\n",
      "Starting epoch 64\n",
      "Loss after mini batch    64: 12.030\n",
      "Loss after mini batch    64: 13.549\n",
      "Loss after mini batch    64: 13.249\n",
      "Loss after mini batch    64: 12.389\n",
      "Loss after mini batch    64: 14.073\n",
      "Loss after mini batch    64: 12.788\n",
      "Loss after mini batch    64: 11.503\n",
      "Loss after mini batch    64: 12.692\n",
      "Loss after mini batch    64: 11.990\n",
      "Loss after mini batch    64: 12.663\n",
      "Loss after mini batch    64: 13.212\n",
      "Starting epoch 65\n",
      "Loss after mini batch    65: 13.154\n",
      "Loss after mini batch    65: 12.334\n",
      "Loss after mini batch    65: 11.708\n",
      "Loss after mini batch    65: 12.420\n",
      "Loss after mini batch    65: 12.927\n",
      "Loss after mini batch    65: 12.784\n",
      "Loss after mini batch    65: 11.841\n",
      "Loss after mini batch    65: 12.174\n",
      "Loss after mini batch    65: 12.773\n",
      "Loss after mini batch    65: 12.590\n",
      "Loss after mini batch    65: 11.494\n",
      "Starting epoch 66\n",
      "Loss after mini batch    66: 12.115\n",
      "Loss after mini batch    66: 12.003\n",
      "Loss after mini batch    66: 12.068\n",
      "Loss after mini batch    66: 13.576\n",
      "Loss after mini batch    66: 12.874\n",
      "Loss after mini batch    66: 12.986\n",
      "Loss after mini batch    66: 12.970\n",
      "Loss after mini batch    66: 12.261\n",
      "Loss after mini batch    66: 11.796\n",
      "Loss after mini batch    66: 12.783\n",
      "Loss after mini batch    66: 12.659\n",
      "Starting epoch 67\n",
      "Loss after mini batch    67: 12.222\n",
      "Loss after mini batch    67: 12.056\n",
      "Loss after mini batch    67: 12.723\n",
      "Loss after mini batch    67: 12.888\n",
      "Loss after mini batch    67: 13.280\n",
      "Loss after mini batch    67: 11.741\n",
      "Loss after mini batch    67: 13.157\n",
      "Loss after mini batch    67: 11.816\n",
      "Loss after mini batch    67: 12.683\n",
      "Loss after mini batch    67: 13.013\n",
      "Loss after mini batch    67: 12.461\n",
      "Starting epoch 68\n",
      "Loss after mini batch    68: 11.766\n",
      "Loss after mini batch    68: 14.096\n",
      "Loss after mini batch    68: 11.983\n",
      "Loss after mini batch    68: 12.307\n",
      "Loss after mini batch    68: 12.497\n",
      "Loss after mini batch    68: 12.400\n",
      "Loss after mini batch    68: 11.657\n",
      "Loss after mini batch    68: 13.276\n",
      "Loss after mini batch    68: 13.242\n",
      "Loss after mini batch    68: 12.153\n",
      "Loss after mini batch    68: 12.242\n",
      "Starting epoch 69\n",
      "Loss after mini batch    69: 12.030\n",
      "Loss after mini batch    69: 12.129\n",
      "Loss after mini batch    69: 12.639\n",
      "Loss after mini batch    69: 12.422\n",
      "Loss after mini batch    69: 12.220\n",
      "Loss after mini batch    69: 11.602\n",
      "Loss after mini batch    69: 12.934\n",
      "Loss after mini batch    69: 13.674\n",
      "Loss after mini batch    69: 12.375\n",
      "Loss after mini batch    69: 12.513\n",
      "Loss after mini batch    69: 13.034\n",
      "Starting epoch 70\n",
      "Loss after mini batch    70: 12.403\n",
      "Loss after mini batch    70: 12.365\n",
      "Loss after mini batch    70: 13.046\n",
      "Loss after mini batch    70: 11.980\n",
      "Loss after mini batch    70: 13.736\n",
      "Loss after mini batch    70: 12.139\n",
      "Loss after mini batch    70: 12.610\n",
      "Loss after mini batch    70: 12.891\n",
      "Loss after mini batch    70: 11.965\n",
      "Loss after mini batch    70: 12.197\n",
      "Loss after mini batch    70: 12.702\n",
      "Starting epoch 71\n",
      "Loss after mini batch    71: 11.486\n",
      "Loss after mini batch    71: 12.530\n",
      "Loss after mini batch    71: 11.789\n",
      "Loss after mini batch    71: 11.498\n",
      "Loss after mini batch    71: 12.703\n",
      "Loss after mini batch    71: 12.257\n",
      "Loss after mini batch    71: 13.996\n",
      "Loss after mini batch    71: 11.663\n",
      "Loss after mini batch    71: 12.327\n",
      "Loss after mini batch    71: 12.612\n",
      "Loss after mini batch    71: 13.486\n",
      "Starting epoch 72\n",
      "Loss after mini batch    72: 11.852\n",
      "Loss after mini batch    72: 11.708\n",
      "Loss after mini batch    72: 12.237\n",
      "Loss after mini batch    72: 12.435\n",
      "Loss after mini batch    72: 12.866\n",
      "Loss after mini batch    72: 12.409\n",
      "Loss after mini batch    72: 12.930\n",
      "Loss after mini batch    72: 12.275\n",
      "Loss after mini batch    72: 12.285\n",
      "Loss after mini batch    72: 14.511\n",
      "Loss after mini batch    72: 10.865\n",
      "Starting epoch 73\n",
      "Loss after mini batch    73: 12.123\n",
      "Loss after mini batch    73: 11.810\n",
      "Loss after mini batch    73: 13.138\n",
      "Loss after mini batch    73: 11.902\n",
      "Loss after mini batch    73: 12.690\n",
      "Loss after mini batch    73: 12.048\n",
      "Loss after mini batch    73: 12.072\n",
      "Loss after mini batch    73: 12.500\n",
      "Loss after mini batch    73: 12.928\n",
      "Loss after mini batch    73: 12.762\n",
      "Loss after mini batch    73: 12.978\n",
      "Starting epoch 74\n",
      "Loss after mini batch    74: 11.485\n",
      "Loss after mini batch    74: 11.392\n",
      "Loss after mini batch    74: 13.641\n",
      "Loss after mini batch    74: 11.222\n",
      "Loss after mini batch    74: 14.409\n",
      "Loss after mini batch    74: 13.143\n",
      "Loss after mini batch    74: 13.073\n",
      "Loss after mini batch    74: 12.917\n",
      "Loss after mini batch    74: 11.962\n",
      "Loss after mini batch    74: 12.346\n",
      "Loss after mini batch    74: 11.572\n",
      "Starting epoch 75\n",
      "Loss after mini batch    75: 12.957\n",
      "Loss after mini batch    75: 12.736\n",
      "Loss after mini batch    75: 11.441\n",
      "Loss after mini batch    75: 11.864\n",
      "Loss after mini batch    75: 13.440\n",
      "Loss after mini batch    75: 12.636\n",
      "Loss after mini batch    75: 12.552\n",
      "Loss after mini batch    75: 12.359\n",
      "Loss after mini batch    75: 11.756\n",
      "Loss after mini batch    75: 12.223\n",
      "Loss after mini batch    75: 12.923\n",
      "Starting epoch 76\n",
      "Loss after mini batch    76: 13.038\n",
      "Loss after mini batch    76: 12.690\n",
      "Loss after mini batch    76: 12.647\n",
      "Loss after mini batch    76: 12.547\n",
      "Loss after mini batch    76: 12.953\n",
      "Loss after mini batch    76: 11.768\n",
      "Loss after mini batch    76: 11.721\n",
      "Loss after mini batch    76: 12.621\n",
      "Loss after mini batch    76: 12.586\n",
      "Loss after mini batch    76: 11.661\n",
      "Loss after mini batch    76: 12.414\n",
      "Starting epoch 77\n",
      "Loss after mini batch    77: 12.384\n",
      "Loss after mini batch    77: 13.928\n",
      "Loss after mini batch    77: 12.216\n",
      "Loss after mini batch    77: 11.793\n",
      "Loss after mini batch    77: 13.112\n",
      "Loss after mini batch    77: 13.594\n",
      "Loss after mini batch    77: 12.994\n",
      "Loss after mini batch    77: 11.957\n",
      "Loss after mini batch    77: 12.270\n",
      "Loss after mini batch    77: 13.430\n",
      "Loss after mini batch    77: 11.670\n",
      "Starting epoch 78\n",
      "Loss after mini batch    78: 13.270\n",
      "Loss after mini batch    78: 13.791\n",
      "Loss after mini batch    78: 12.211\n",
      "Loss after mini batch    78: 11.872\n",
      "Loss after mini batch    78: 12.724\n",
      "Loss after mini batch    78: 11.714\n",
      "Loss after mini batch    78: 12.303\n",
      "Loss after mini batch    78: 13.056\n",
      "Loss after mini batch    78: 11.924\n",
      "Loss after mini batch    78: 12.179\n",
      "Loss after mini batch    78: 12.921\n",
      "Starting epoch 79\n",
      "Loss after mini batch    79: 14.094\n",
      "Loss after mini batch    79: 12.299\n",
      "Loss after mini batch    79: 11.368\n",
      "Loss after mini batch    79: 12.080\n",
      "Loss after mini batch    79: 12.886\n",
      "Loss after mini batch    79: 11.511\n",
      "Loss after mini batch    79: 12.398\n",
      "Loss after mini batch    79: 14.182\n",
      "Loss after mini batch    79: 11.739\n",
      "Loss after mini batch    79: 12.158\n",
      "Loss after mini batch    79: 12.003\n",
      "Starting epoch 80\n",
      "Loss after mini batch    80: 13.411\n",
      "Loss after mini batch    80: 11.301\n",
      "Loss after mini batch    80: 13.669\n",
      "Loss after mini batch    80: 13.532\n",
      "Loss after mini batch    80: 12.930\n",
      "Loss after mini batch    80: 12.980\n",
      "Loss after mini batch    80: 11.930\n",
      "Loss after mini batch    80: 12.621\n",
      "Loss after mini batch    80: 13.678\n",
      "Loss after mini batch    80: 13.130\n",
      "Loss after mini batch    80: 11.657\n",
      "Starting epoch 81\n",
      "Loss after mini batch    81: 11.958\n",
      "Loss after mini batch    81: 11.360\n",
      "Loss after mini batch    81: 12.382\n",
      "Loss after mini batch    81: 12.875\n",
      "Loss after mini batch    81: 12.433\n",
      "Loss after mini batch    81: 12.869\n",
      "Loss after mini batch    81: 14.170\n",
      "Loss after mini batch    81: 12.140\n",
      "Loss after mini batch    81: 12.724\n",
      "Loss after mini batch    81: 13.234\n",
      "Loss after mini batch    81: 11.543\n",
      "Starting epoch 82\n",
      "Loss after mini batch    82: 13.037\n",
      "Loss after mini batch    82: 12.703\n",
      "Loss after mini batch    82: 11.879\n",
      "Loss after mini batch    82: 11.979\n",
      "Loss after mini batch    82: 12.353\n",
      "Loss after mini batch    82: 11.822\n",
      "Loss after mini batch    82: 11.914\n",
      "Loss after mini batch    82: 12.347\n",
      "Loss after mini batch    82: 11.579\n",
      "Loss after mini batch    82: 13.446\n",
      "Loss after mini batch    82: 11.884\n",
      "Starting epoch 83\n",
      "Loss after mini batch    83: 12.664\n",
      "Loss after mini batch    83: 12.575\n",
      "Loss after mini batch    83: 13.065\n",
      "Loss after mini batch    83: 11.257\n",
      "Loss after mini batch    83: 12.459\n",
      "Loss after mini batch    83: 12.661\n",
      "Loss after mini batch    83: 12.688\n",
      "Loss after mini batch    83: 11.854\n",
      "Loss after mini batch    83: 12.924\n",
      "Loss after mini batch    83: 11.570\n",
      "Loss after mini batch    83: 12.113\n",
      "Starting epoch 84\n",
      "Loss after mini batch    84: 13.060\n",
      "Loss after mini batch    84: 11.751\n",
      "Loss after mini batch    84: 13.222\n",
      "Loss after mini batch    84: 12.350\n",
      "Loss after mini batch    84: 12.589\n",
      "Loss after mini batch    84: 10.603\n",
      "Loss after mini batch    84: 13.015\n",
      "Loss after mini batch    84: 11.948\n",
      "Loss after mini batch    84: 11.594\n",
      "Loss after mini batch    84: 13.328\n",
      "Loss after mini batch    84: 12.882\n",
      "Starting epoch 85\n",
      "Loss after mini batch    85: 12.992\n",
      "Loss after mini batch    85: 12.573\n",
      "Loss after mini batch    85: 13.264\n",
      "Loss after mini batch    85: 11.626\n",
      "Loss after mini batch    85: 11.567\n",
      "Loss after mini batch    85: 12.628\n",
      "Loss after mini batch    85: 12.363\n",
      "Loss after mini batch    85: 11.286\n",
      "Loss after mini batch    85: 12.305\n",
      "Loss after mini batch    85: 10.696\n",
      "Loss after mini batch    85: 12.351\n",
      "Starting epoch 86\n",
      "Loss after mini batch    86: 11.814\n",
      "Loss after mini batch    86: 11.741\n",
      "Loss after mini batch    86: 12.068\n",
      "Loss after mini batch    86: 11.805\n",
      "Loss after mini batch    86: 11.867\n",
      "Loss after mini batch    86: 13.035\n",
      "Loss after mini batch    86: 11.892\n",
      "Loss after mini batch    86: 12.553\n",
      "Loss after mini batch    86: 11.169\n",
      "Loss after mini batch    86: 13.470\n",
      "Loss after mini batch    86: 12.955\n",
      "Starting epoch 87\n",
      "Loss after mini batch    87: 12.085\n",
      "Loss after mini batch    87: 12.261\n",
      "Loss after mini batch    87: 12.285\n",
      "Loss after mini batch    87: 12.589\n",
      "Loss after mini batch    87: 12.684\n",
      "Loss after mini batch    87: 12.554\n",
      "Loss after mini batch    87: 13.280\n",
      "Loss after mini batch    87: 11.984\n",
      "Loss after mini batch    87: 11.827\n",
      "Loss after mini batch    87: 11.898\n",
      "Loss after mini batch    87: 12.461\n",
      "Starting epoch 88\n",
      "Loss after mini batch    88: 11.104\n",
      "Loss after mini batch    88: 13.706\n",
      "Loss after mini batch    88: 12.206\n",
      "Loss after mini batch    88: 12.753\n",
      "Loss after mini batch    88: 13.221\n",
      "Loss after mini batch    88: 12.665\n",
      "Loss after mini batch    88: 12.406\n",
      "Loss after mini batch    88: 11.779\n",
      "Loss after mini batch    88: 13.127\n",
      "Loss after mini batch    88: 12.591\n",
      "Loss after mini batch    88: 11.891\n",
      "Starting epoch 89\n",
      "Loss after mini batch    89: 13.053\n",
      "Loss after mini batch    89: 11.399\n",
      "Loss after mini batch    89: 11.455\n",
      "Loss after mini batch    89: 12.734\n",
      "Loss after mini batch    89: 11.706\n",
      "Loss after mini batch    89: 13.022\n",
      "Loss after mini batch    89: 13.103\n",
      "Loss after mini batch    89: 13.212\n",
      "Loss after mini batch    89: 11.925\n",
      "Loss after mini batch    89: 11.625\n",
      "Loss after mini batch    89: 12.909\n",
      "Starting epoch 90\n",
      "Loss after mini batch    90: 11.784\n",
      "Loss after mini batch    90: 12.019\n",
      "Loss after mini batch    90: 12.553\n",
      "Loss after mini batch    90: 12.919\n",
      "Loss after mini batch    90: 12.910\n",
      "Loss after mini batch    90: 11.674\n",
      "Loss after mini batch    90: 12.331\n",
      "Loss after mini batch    90: 10.592\n",
      "Loss after mini batch    90: 12.758\n",
      "Loss after mini batch    90: 12.205\n",
      "Loss after mini batch    90: 12.411\n",
      "Starting epoch 91\n",
      "Loss after mini batch    91: 13.438\n",
      "Loss after mini batch    91: 12.320\n",
      "Loss after mini batch    91: 13.238\n",
      "Loss after mini batch    91: 11.853\n",
      "Loss after mini batch    91: 11.594\n",
      "Loss after mini batch    91: 11.920\n",
      "Loss after mini batch    91: 12.258\n",
      "Loss after mini batch    91: 12.702\n",
      "Loss after mini batch    91: 11.075\n",
      "Loss after mini batch    91: 13.084\n",
      "Loss after mini batch    91: 11.939\n",
      "Starting epoch 92\n",
      "Loss after mini batch    92: 12.530\n",
      "Loss after mini batch    92: 11.728\n",
      "Loss after mini batch    92: 12.351\n",
      "Loss after mini batch    92: 12.685\n",
      "Loss after mini batch    92: 13.824\n",
      "Loss after mini batch    92: 13.202\n",
      "Loss after mini batch    92: 14.371\n",
      "Loss after mini batch    92: 11.274\n",
      "Loss after mini batch    92: 11.673\n",
      "Loss after mini batch    92: 11.387\n",
      "Loss after mini batch    92: 12.722\n",
      "Starting epoch 93\n",
      "Loss after mini batch    93: 11.504\n",
      "Loss after mini batch    93: 11.320\n",
      "Loss after mini batch    93: 12.300\n",
      "Loss after mini batch    93: 11.990\n",
      "Loss after mini batch    93: 11.805\n",
      "Loss after mini batch    93: 12.201\n",
      "Loss after mini batch    93: 11.532\n",
      "Loss after mini batch    93: 12.959\n",
      "Loss after mini batch    93: 12.658\n",
      "Loss after mini batch    93: 13.581\n",
      "Loss after mini batch    93: 11.957\n",
      "Starting epoch 94\n",
      "Loss after mini batch    94: 11.637\n",
      "Loss after mini batch    94: 12.236\n",
      "Loss after mini batch    94: 11.731\n",
      "Loss after mini batch    94: 12.086\n",
      "Loss after mini batch    94: 13.589\n",
      "Loss after mini batch    94: 11.976\n",
      "Loss after mini batch    94: 11.492\n",
      "Loss after mini batch    94: 12.305\n",
      "Loss after mini batch    94: 13.281\n",
      "Loss after mini batch    94: 11.592\n",
      "Loss after mini batch    94: 13.718\n",
      "Starting epoch 95\n",
      "Loss after mini batch    95: 12.515\n",
      "Loss after mini batch    95: 12.643\n",
      "Loss after mini batch    95: 12.097\n",
      "Loss after mini batch    95: 13.576\n",
      "Loss after mini batch    95: 13.374\n",
      "Loss after mini batch    95: 12.096\n",
      "Loss after mini batch    95: 13.324\n",
      "Loss after mini batch    95: 11.164\n",
      "Loss after mini batch    95: 12.412\n",
      "Loss after mini batch    95: 11.702\n",
      "Loss after mini batch    95: 11.279\n",
      "Starting epoch 96\n",
      "Loss after mini batch    96: 11.339\n",
      "Loss after mini batch    96: 13.489\n",
      "Loss after mini batch    96: 11.454\n",
      "Loss after mini batch    96: 11.594\n",
      "Loss after mini batch    96: 12.505\n",
      "Loss after mini batch    96: 12.256\n",
      "Loss after mini batch    96: 12.619\n",
      "Loss after mini batch    96: 11.373\n",
      "Loss after mini batch    96: 12.522\n",
      "Loss after mini batch    96: 12.275\n",
      "Loss after mini batch    96: 12.094\n",
      "Starting epoch 97\n",
      "Loss after mini batch    97: 12.266\n",
      "Loss after mini batch    97: 11.584\n",
      "Loss after mini batch    97: 11.825\n",
      "Loss after mini batch    97: 12.729\n",
      "Loss after mini batch    97: 11.695\n",
      "Loss after mini batch    97: 12.067\n",
      "Loss after mini batch    97: 12.653\n",
      "Loss after mini batch    97: 13.069\n",
      "Loss after mini batch    97: 12.877\n",
      "Loss after mini batch    97: 12.417\n",
      "Loss after mini batch    97: 12.112\n",
      "Starting epoch 98\n",
      "Loss after mini batch    98: 10.770\n",
      "Loss after mini batch    98: 12.445\n",
      "Loss after mini batch    98: 13.032\n",
      "Loss after mini batch    98: 12.178\n",
      "Loss after mini batch    98: 12.303\n",
      "Loss after mini batch    98: 11.940\n",
      "Loss after mini batch    98: 12.704\n",
      "Loss after mini batch    98: 12.186\n",
      "Loss after mini batch    98: 12.175\n",
      "Loss after mini batch    98: 12.390\n",
      "Loss after mini batch    98: 12.929\n",
      "Starting epoch 99\n",
      "Loss after mini batch    99: 12.568\n",
      "Loss after mini batch    99: 12.864\n",
      "Loss after mini batch    99: 13.421\n",
      "Loss after mini batch    99: 11.233\n",
      "Loss after mini batch    99: 11.828\n",
      "Loss after mini batch    99: 11.463\n",
      "Loss after mini batch    99: 10.625\n",
      "Loss after mini batch    99: 12.879\n",
      "Loss after mini batch    99: 12.290\n",
      "Loss after mini batch    99: 13.017\n",
      "Loss after mini batch    99: 12.183\n",
      "Starting epoch 100\n",
      "Loss after mini batch   100: 10.729\n",
      "Loss after mini batch   100: 12.193\n",
      "Loss after mini batch   100: 11.436\n",
      "Loss after mini batch   100: 13.300\n",
      "Loss after mini batch   100: 12.021\n",
      "Loss after mini batch   100: 12.287\n",
      "Loss after mini batch   100: 11.887\n",
      "Loss after mini batch   100: 11.869\n",
      "Loss after mini batch   100: 12.015\n",
      "Loss after mini batch   100: 12.963\n",
      "Loss after mini batch   100: 12.875\n",
      "Training Complete\n",
      "Starting Testing\n",
      "RMSE for fold 2: 3.499257962721933\n",
      "rRMSE for fold 2: 0.06772605314725146\n",
      "r for fold 2: 0.9910353694971267\n",
      "Fast RMSE for fold 2: 3.3732230135503207\n",
      "Fast rRMSE for fold 2: 0.06787459849753456\n",
      "Fast r for fold 2: 0.9850175590696935\n",
      "Slow RMSE for fold 2: 4.450572570654819\n",
      "Slow rRMSE for fold 2: 0.07971413374374962\n",
      "Slow r for fold 2: 0.9980630296190753\n",
      "Regular RMSE for fold 2: 2.559795161635253\n",
      "Regular rRMSE for fold 2: 0.05122420620694427\n",
      "Regular r for fold 2: 0.997222781414334\n",
      "Fold 3\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini batch     1: 346.457\n",
      "Loss after mini batch     1: 19.656\n",
      "Loss after mini batch     1: 17.407\n",
      "Loss after mini batch     1: 18.559\n",
      "Loss after mini batch     1: 20.336\n",
      "Loss after mini batch     1: 22.503\n",
      "Loss after mini batch     1: 23.616\n",
      "Loss after mini batch     1: 19.684\n",
      "Loss after mini batch     1: 20.591\n",
      "Loss after mini batch     1: 26.141\n",
      "Loss after mini batch     1: 23.764\n",
      "Starting epoch 2\n",
      "Loss after mini batch     2: 18.718\n",
      "Loss after mini batch     2: 21.084\n",
      "Loss after mini batch     2: 19.928\n",
      "Loss after mini batch     2: 25.580\n",
      "Loss after mini batch     2: 17.856\n",
      "Loss after mini batch     2: 18.053\n",
      "Loss after mini batch     2: 18.648\n",
      "Loss after mini batch     2: 23.289\n",
      "Loss after mini batch     2: 27.617\n",
      "Loss after mini batch     2: 29.148\n",
      "Loss after mini batch     2: 16.837\n",
      "Starting epoch 3\n",
      "Loss after mini batch     3: 22.948\n",
      "Loss after mini batch     3: 19.625\n",
      "Loss after mini batch     3: 22.099\n",
      "Loss after mini batch     3: 16.969\n",
      "Loss after mini batch     3: 20.456\n",
      "Loss after mini batch     3: 17.462\n",
      "Loss after mini batch     3: 21.187\n",
      "Loss after mini batch     3: 23.573\n",
      "Loss after mini batch     3: 18.183\n",
      "Loss after mini batch     3: 19.581\n",
      "Loss after mini batch     3: 23.486\n",
      "Starting epoch 4\n",
      "Loss after mini batch     4: 20.502\n",
      "Loss after mini batch     4: 17.013\n",
      "Loss after mini batch     4: 17.706\n",
      "Loss after mini batch     4: 21.121\n",
      "Loss after mini batch     4: 14.553\n",
      "Loss after mini batch     4: 14.354\n",
      "Loss after mini batch     4: 14.269\n",
      "Loss after mini batch     4: 16.737\n",
      "Loss after mini batch     4: 19.205\n",
      "Loss after mini batch     4: 18.297\n",
      "Loss after mini batch     4: 16.572\n",
      "Starting epoch 5\n",
      "Loss after mini batch     5: 18.678\n",
      "Loss after mini batch     5: 14.909\n",
      "Loss after mini batch     5: 15.600\n",
      "Loss after mini batch     5: 16.888\n",
      "Loss after mini batch     5: 16.059\n",
      "Loss after mini batch     5: 18.464\n",
      "Loss after mini batch     5: 16.103\n",
      "Loss after mini batch     5: 15.947\n",
      "Loss after mini batch     5: 15.305\n",
      "Loss after mini batch     5: 18.319\n",
      "Loss after mini batch     5: 14.796\n",
      "Starting epoch 6\n",
      "Loss after mini batch     6: 14.667\n",
      "Loss after mini batch     6: 14.228\n",
      "Loss after mini batch     6: 16.965\n",
      "Loss after mini batch     6: 14.633\n",
      "Loss after mini batch     6: 16.132\n",
      "Loss after mini batch     6: 15.479\n",
      "Loss after mini batch     6: 15.012\n",
      "Loss after mini batch     6: 19.118\n",
      "Loss after mini batch     6: 15.153\n",
      "Loss after mini batch     6: 12.374\n",
      "Loss after mini batch     6: 14.879\n",
      "Starting epoch 7\n",
      "Loss after mini batch     7: 12.672\n",
      "Loss after mini batch     7: 17.489\n",
      "Loss after mini batch     7: 13.777\n",
      "Loss after mini batch     7: 17.633\n",
      "Loss after mini batch     7: 17.578\n",
      "Loss after mini batch     7: 16.426\n",
      "Loss after mini batch     7: 14.616\n",
      "Loss after mini batch     7: 14.397\n",
      "Loss after mini batch     7: 14.650\n",
      "Loss after mini batch     7: 15.947\n",
      "Loss after mini batch     7: 16.042\n",
      "Starting epoch 8\n",
      "Loss after mini batch     8: 14.884\n",
      "Loss after mini batch     8: 14.022\n",
      "Loss after mini batch     8: 14.623\n",
      "Loss after mini batch     8: 15.632\n",
      "Loss after mini batch     8: 13.909\n",
      "Loss after mini batch     8: 16.986\n",
      "Loss after mini batch     8: 12.160\n",
      "Loss after mini batch     8: 14.786\n",
      "Loss after mini batch     8: 14.935\n",
      "Loss after mini batch     8: 15.993\n",
      "Loss after mini batch     8: 16.223\n",
      "Starting epoch 9\n",
      "Loss after mini batch     9: 14.993\n",
      "Loss after mini batch     9: 16.037\n",
      "Loss after mini batch     9: 14.048\n",
      "Loss after mini batch     9: 15.763\n",
      "Loss after mini batch     9: 15.314\n",
      "Loss after mini batch     9: 16.658\n",
      "Loss after mini batch     9: 16.137\n",
      "Loss after mini batch     9: 14.446\n",
      "Loss after mini batch     9: 13.984\n",
      "Loss after mini batch     9: 18.634\n",
      "Loss after mini batch     9: 13.965\n",
      "Starting epoch 10\n",
      "Loss after mini batch    10: 14.206\n",
      "Loss after mini batch    10: 14.234\n",
      "Loss after mini batch    10: 13.696\n",
      "Loss after mini batch    10: 14.556\n",
      "Loss after mini batch    10: 13.943\n",
      "Loss after mini batch    10: 15.774\n",
      "Loss after mini batch    10: 15.145\n",
      "Loss after mini batch    10: 14.206\n",
      "Loss after mini batch    10: 14.647\n",
      "Loss after mini batch    10: 16.078\n",
      "Loss after mini batch    10: 16.964\n",
      "Starting epoch 11\n",
      "Loss after mini batch    11: 16.725\n",
      "Loss after mini batch    11: 12.881\n",
      "Loss after mini batch    11: 13.274\n",
      "Loss after mini batch    11: 15.805\n",
      "Loss after mini batch    11: 13.915\n",
      "Loss after mini batch    11: 13.612\n",
      "Loss after mini batch    11: 13.837\n",
      "Loss after mini batch    11: 13.178\n",
      "Loss after mini batch    11: 15.645\n",
      "Loss after mini batch    11: 13.563\n",
      "Loss after mini batch    11: 16.794\n",
      "Starting epoch 12\n",
      "Loss after mini batch    12: 14.294\n",
      "Loss after mini batch    12: 12.396\n",
      "Loss after mini batch    12: 13.648\n",
      "Loss after mini batch    12: 14.414\n",
      "Loss after mini batch    12: 14.026\n",
      "Loss after mini batch    12: 12.901\n",
      "Loss after mini batch    12: 16.753\n",
      "Loss after mini batch    12: 14.298\n",
      "Loss after mini batch    12: 12.420\n",
      "Loss after mini batch    12: 13.459\n",
      "Loss after mini batch    12: 14.548\n",
      "Starting epoch 13\n",
      "Loss after mini batch    13: 14.149\n",
      "Loss after mini batch    13: 14.667\n",
      "Loss after mini batch    13: 13.436\n",
      "Loss after mini batch    13: 13.658\n",
      "Loss after mini batch    13: 14.079\n",
      "Loss after mini batch    13: 12.862\n",
      "Loss after mini batch    13: 14.148\n",
      "Loss after mini batch    13: 15.691\n",
      "Loss after mini batch    13: 13.678\n",
      "Loss after mini batch    13: 14.356\n",
      "Loss after mini batch    13: 15.379\n",
      "Starting epoch 14\n",
      "Loss after mini batch    14: 13.718\n",
      "Loss after mini batch    14: 15.389\n",
      "Loss after mini batch    14: 14.225\n",
      "Loss after mini batch    14: 14.790\n",
      "Loss after mini batch    14: 14.324\n",
      "Loss after mini batch    14: 13.835\n",
      "Loss after mini batch    14: 13.155\n",
      "Loss after mini batch    14: 15.225\n",
      "Loss after mini batch    14: 13.627\n",
      "Loss after mini batch    14: 14.030\n",
      "Loss after mini batch    14: 13.582\n",
      "Starting epoch 15\n",
      "Loss after mini batch    15: 13.584\n",
      "Loss after mini batch    15: 12.881\n",
      "Loss after mini batch    15: 14.085\n",
      "Loss after mini batch    15: 14.163\n",
      "Loss after mini batch    15: 13.672\n",
      "Loss after mini batch    15: 15.874\n",
      "Loss after mini batch    15: 13.400\n",
      "Loss after mini batch    15: 14.344\n",
      "Loss after mini batch    15: 13.834\n",
      "Loss after mini batch    15: 13.705\n",
      "Loss after mini batch    15: 14.784\n",
      "Starting epoch 16\n",
      "Loss after mini batch    16: 13.681\n",
      "Loss after mini batch    16: 12.619\n",
      "Loss after mini batch    16: 15.574\n",
      "Loss after mini batch    16: 13.358\n",
      "Loss after mini batch    16: 14.338\n",
      "Loss after mini batch    16: 14.475\n",
      "Loss after mini batch    16: 15.896\n",
      "Loss after mini batch    16: 13.405\n",
      "Loss after mini batch    16: 13.151\n",
      "Loss after mini batch    16: 13.428\n",
      "Loss after mini batch    16: 13.706\n",
      "Starting epoch 17\n",
      "Loss after mini batch    17: 12.946\n",
      "Loss after mini batch    17: 15.215\n",
      "Loss after mini batch    17: 13.017\n",
      "Loss after mini batch    17: 12.734\n",
      "Loss after mini batch    17: 13.021\n",
      "Loss after mini batch    17: 15.222\n",
      "Loss after mini batch    17: 12.356\n",
      "Loss after mini batch    17: 13.781\n",
      "Loss after mini batch    17: 13.347\n",
      "Loss after mini batch    17: 13.392\n",
      "Loss after mini batch    17: 14.807\n",
      "Starting epoch 18\n",
      "Loss after mini batch    18: 13.874\n",
      "Loss after mini batch    18: 15.149\n",
      "Loss after mini batch    18: 15.875\n",
      "Loss after mini batch    18: 12.598\n",
      "Loss after mini batch    18: 14.365\n",
      "Loss after mini batch    18: 14.412\n",
      "Loss after mini batch    18: 13.683\n",
      "Loss after mini batch    18: 14.006\n",
      "Loss after mini batch    18: 14.673\n",
      "Loss after mini batch    18: 14.822\n",
      "Loss after mini batch    18: 12.531\n",
      "Starting epoch 19\n",
      "Loss after mini batch    19: 13.034\n",
      "Loss after mini batch    19: 12.578\n",
      "Loss after mini batch    19: 13.948\n",
      "Loss after mini batch    19: 14.020\n",
      "Loss after mini batch    19: 14.555\n",
      "Loss after mini batch    19: 12.539\n",
      "Loss after mini batch    19: 14.299\n",
      "Loss after mini batch    19: 13.540\n",
      "Loss after mini batch    19: 14.014\n",
      "Loss after mini batch    19: 14.858\n",
      "Loss after mini batch    19: 13.163\n",
      "Starting epoch 20\n",
      "Loss after mini batch    20: 13.155\n",
      "Loss after mini batch    20: 13.975\n",
      "Loss after mini batch    20: 13.704\n",
      "Loss after mini batch    20: 13.608\n",
      "Loss after mini batch    20: 13.399\n",
      "Loss after mini batch    20: 12.929\n",
      "Loss after mini batch    20: 15.045\n",
      "Loss after mini batch    20: 15.741\n",
      "Loss after mini batch    20: 13.894\n",
      "Loss after mini batch    20: 13.587\n",
      "Loss after mini batch    20: 12.743\n",
      "Starting epoch 21\n",
      "Loss after mini batch    21: 12.970\n",
      "Loss after mini batch    21: 12.966\n",
      "Loss after mini batch    21: 12.492\n",
      "Loss after mini batch    21: 14.382\n",
      "Loss after mini batch    21: 15.154\n",
      "Loss after mini batch    21: 13.913\n",
      "Loss after mini batch    21: 14.263\n",
      "Loss after mini batch    21: 12.331\n",
      "Loss after mini batch    21: 13.704\n",
      "Loss after mini batch    21: 15.325\n",
      "Loss after mini batch    21: 13.320\n",
      "Starting epoch 22\n",
      "Loss after mini batch    22: 13.282\n",
      "Loss after mini batch    22: 11.883\n",
      "Loss after mini batch    22: 13.281\n",
      "Loss after mini batch    22: 14.811\n",
      "Loss after mini batch    22: 15.624\n",
      "Loss after mini batch    22: 14.772\n",
      "Loss after mini batch    22: 13.365\n",
      "Loss after mini batch    22: 12.485\n",
      "Loss after mini batch    22: 12.779\n",
      "Loss after mini batch    22: 13.773\n",
      "Loss after mini batch    22: 13.757\n",
      "Starting epoch 23\n",
      "Loss after mini batch    23: 12.903\n",
      "Loss after mini batch    23: 12.842\n",
      "Loss after mini batch    23: 15.429\n",
      "Loss after mini batch    23: 13.942\n",
      "Loss after mini batch    23: 14.798\n",
      "Loss after mini batch    23: 12.856\n",
      "Loss after mini batch    23: 11.707\n",
      "Loss after mini batch    23: 15.362\n",
      "Loss after mini batch    23: 12.410\n",
      "Loss after mini batch    23: 13.318\n",
      "Loss after mini batch    23: 12.796\n",
      "Starting epoch 24\n",
      "Loss after mini batch    24: 13.483\n",
      "Loss after mini batch    24: 12.945\n",
      "Loss after mini batch    24: 11.928\n",
      "Loss after mini batch    24: 14.698\n",
      "Loss after mini batch    24: 13.032\n",
      "Loss after mini batch    24: 13.195\n",
      "Loss after mini batch    24: 14.456\n",
      "Loss after mini batch    24: 14.692\n",
      "Loss after mini batch    24: 13.537\n",
      "Loss after mini batch    24: 13.867\n",
      "Loss after mini batch    24: 14.519\n",
      "Starting epoch 25\n",
      "Loss after mini batch    25: 13.220\n",
      "Loss after mini batch    25: 14.308\n",
      "Loss after mini batch    25: 12.652\n",
      "Loss after mini batch    25: 12.229\n",
      "Loss after mini batch    25: 13.710\n",
      "Loss after mini batch    25: 12.351\n",
      "Loss after mini batch    25: 12.406\n",
      "Loss after mini batch    25: 12.893\n",
      "Loss after mini batch    25: 13.959\n",
      "Loss after mini batch    25: 14.940\n",
      "Loss after mini batch    25: 14.730\n",
      "Starting epoch 26\n",
      "Loss after mini batch    26: 13.046\n",
      "Loss after mini batch    26: 12.519\n",
      "Loss after mini batch    26: 13.957\n",
      "Loss after mini batch    26: 13.565\n",
      "Loss after mini batch    26: 14.229\n",
      "Loss after mini batch    26: 14.267\n",
      "Loss after mini batch    26: 14.354\n",
      "Loss after mini batch    26: 12.717\n",
      "Loss after mini batch    26: 12.558\n",
      "Loss after mini batch    26: 12.869\n",
      "Loss after mini batch    26: 12.735\n",
      "Starting epoch 27\n",
      "Loss after mini batch    27: 14.194\n",
      "Loss after mini batch    27: 13.698\n",
      "Loss after mini batch    27: 13.731\n",
      "Loss after mini batch    27: 11.960\n",
      "Loss after mini batch    27: 16.796\n",
      "Loss after mini batch    27: 15.247\n",
      "Loss after mini batch    27: 13.613\n",
      "Loss after mini batch    27: 12.530\n",
      "Loss after mini batch    27: 13.852\n",
      "Loss after mini batch    27: 12.668\n",
      "Loss after mini batch    27: 13.511\n",
      "Starting epoch 28\n",
      "Loss after mini batch    28: 13.135\n",
      "Loss after mini batch    28: 13.090\n",
      "Loss after mini batch    28: 14.243\n",
      "Loss after mini batch    28: 12.896\n",
      "Loss after mini batch    28: 13.616\n",
      "Loss after mini batch    28: 12.793\n",
      "Loss after mini batch    28: 13.066\n",
      "Loss after mini batch    28: 13.107\n",
      "Loss after mini batch    28: 14.306\n",
      "Loss after mini batch    28: 12.324\n",
      "Loss after mini batch    28: 14.807\n",
      "Starting epoch 29\n",
      "Loss after mini batch    29: 13.102\n",
      "Loss after mini batch    29: 14.063\n",
      "Loss after mini batch    29: 13.535\n",
      "Loss after mini batch    29: 13.241\n",
      "Loss after mini batch    29: 12.522\n",
      "Loss after mini batch    29: 13.364\n",
      "Loss after mini batch    29: 12.819\n",
      "Loss after mini batch    29: 14.473\n",
      "Loss after mini batch    29: 13.146\n",
      "Loss after mini batch    29: 13.490\n",
      "Loss after mini batch    29: 13.812\n",
      "Starting epoch 30\n",
      "Loss after mini batch    30: 12.449\n",
      "Loss after mini batch    30: 11.952\n",
      "Loss after mini batch    30: 13.237\n",
      "Loss after mini batch    30: 12.965\n",
      "Loss after mini batch    30: 13.893\n",
      "Loss after mini batch    30: 13.047\n",
      "Loss after mini batch    30: 13.367\n",
      "Loss after mini batch    30: 13.328\n",
      "Loss after mini batch    30: 15.190\n",
      "Loss after mini batch    30: 13.892\n",
      "Loss after mini batch    30: 13.938\n",
      "Starting epoch 31\n",
      "Loss after mini batch    31: 11.257\n",
      "Loss after mini batch    31: 12.263\n",
      "Loss after mini batch    31: 14.000\n",
      "Loss after mini batch    31: 12.953\n",
      "Loss after mini batch    31: 15.053\n",
      "Loss after mini batch    31: 12.429\n",
      "Loss after mini batch    31: 12.382\n",
      "Loss after mini batch    31: 13.602\n",
      "Loss after mini batch    31: 13.412\n",
      "Loss after mini batch    31: 13.356\n",
      "Loss after mini batch    31: 14.004\n",
      "Starting epoch 32\n",
      "Loss after mini batch    32: 14.051\n",
      "Loss after mini batch    32: 11.460\n",
      "Loss after mini batch    32: 11.680\n",
      "Loss after mini batch    32: 13.515\n",
      "Loss after mini batch    32: 11.633\n",
      "Loss after mini batch    32: 13.696\n",
      "Loss after mini batch    32: 12.801\n",
      "Loss after mini batch    32: 13.551\n",
      "Loss after mini batch    32: 13.884\n",
      "Loss after mini batch    32: 13.622\n",
      "Loss after mini batch    32: 13.852\n",
      "Starting epoch 33\n",
      "Loss after mini batch    33: 11.850\n",
      "Loss after mini batch    33: 13.986\n",
      "Loss after mini batch    33: 13.435\n",
      "Loss after mini batch    33: 13.290\n",
      "Loss after mini batch    33: 14.318\n",
      "Loss after mini batch    33: 12.172\n",
      "Loss after mini batch    33: 12.979\n",
      "Loss after mini batch    33: 13.669\n",
      "Loss after mini batch    33: 14.083\n",
      "Loss after mini batch    33: 14.960\n",
      "Loss after mini batch    33: 12.487\n",
      "Starting epoch 34\n",
      "Loss after mini batch    34: 12.205\n",
      "Loss after mini batch    34: 12.498\n",
      "Loss after mini batch    34: 14.482\n",
      "Loss after mini batch    34: 14.295\n",
      "Loss after mini batch    34: 13.362\n",
      "Loss after mini batch    34: 15.952\n",
      "Loss after mini batch    34: 13.255\n",
      "Loss after mini batch    34: 12.432\n",
      "Loss after mini batch    34: 13.844\n",
      "Loss after mini batch    34: 12.724\n",
      "Loss after mini batch    34: 12.473\n",
      "Starting epoch 35\n",
      "Loss after mini batch    35: 13.400\n",
      "Loss after mini batch    35: 13.239\n",
      "Loss after mini batch    35: 13.456\n",
      "Loss after mini batch    35: 13.601\n",
      "Loss after mini batch    35: 12.052\n",
      "Loss after mini batch    35: 13.656\n",
      "Loss after mini batch    35: 13.109\n",
      "Loss after mini batch    35: 12.738\n",
      "Loss after mini batch    35: 13.704\n",
      "Loss after mini batch    35: 12.002\n",
      "Loss after mini batch    35: 13.596\n",
      "Starting epoch 36\n",
      "Loss after mini batch    36: 13.700\n",
      "Loss after mini batch    36: 13.880\n",
      "Loss after mini batch    36: 11.741\n",
      "Loss after mini batch    36: 12.397\n",
      "Loss after mini batch    36: 14.593\n",
      "Loss after mini batch    36: 14.624\n",
      "Loss after mini batch    36: 12.800\n",
      "Loss after mini batch    36: 12.131\n",
      "Loss after mini batch    36: 14.357\n",
      "Loss after mini batch    36: 14.301\n",
      "Loss after mini batch    36: 12.517\n",
      "Starting epoch 37\n",
      "Loss after mini batch    37: 12.276\n",
      "Loss after mini batch    37: 15.513\n",
      "Loss after mini batch    37: 13.308\n",
      "Loss after mini batch    37: 13.075\n",
      "Loss after mini batch    37: 13.661\n",
      "Loss after mini batch    37: 13.838\n",
      "Loss after mini batch    37: 12.640\n",
      "Loss after mini batch    37: 12.155\n",
      "Loss after mini batch    37: 12.433\n",
      "Loss after mini batch    37: 12.442\n",
      "Loss after mini batch    37: 14.311\n",
      "Starting epoch 38\n",
      "Loss after mini batch    38: 11.733\n",
      "Loss after mini batch    38: 13.256\n",
      "Loss after mini batch    38: 13.175\n",
      "Loss after mini batch    38: 12.038\n",
      "Loss after mini batch    38: 12.373\n",
      "Loss after mini batch    38: 13.079\n",
      "Loss after mini batch    38: 13.112\n",
      "Loss after mini batch    38: 14.403\n",
      "Loss after mini batch    38: 14.978\n",
      "Loss after mini batch    38: 13.358\n",
      "Loss after mini batch    38: 12.050\n",
      "Starting epoch 39\n",
      "Loss after mini batch    39: 12.949\n",
      "Loss after mini batch    39: 12.829\n",
      "Loss after mini batch    39: 13.900\n",
      "Loss after mini batch    39: 13.570\n",
      "Loss after mini batch    39: 14.052\n",
      "Loss after mini batch    39: 12.909\n",
      "Loss after mini batch    39: 13.514\n",
      "Loss after mini batch    39: 12.794\n",
      "Loss after mini batch    39: 13.553\n",
      "Loss after mini batch    39: 11.917\n",
      "Loss after mini batch    39: 12.616\n",
      "Starting epoch 40\n",
      "Loss after mini batch    40: 13.646\n",
      "Loss after mini batch    40: 13.956\n",
      "Loss after mini batch    40: 11.325\n",
      "Loss after mini batch    40: 12.657\n",
      "Loss after mini batch    40: 13.155\n",
      "Loss after mini batch    40: 12.673\n",
      "Loss after mini batch    40: 12.956\n",
      "Loss after mini batch    40: 13.693\n",
      "Loss after mini batch    40: 13.337\n",
      "Loss after mini batch    40: 12.606\n",
      "Loss after mini batch    40: 12.553\n",
      "Starting epoch 41\n",
      "Loss after mini batch    41: 14.059\n",
      "Loss after mini batch    41: 14.985\n",
      "Loss after mini batch    41: 14.329\n",
      "Loss after mini batch    41: 12.974\n",
      "Loss after mini batch    41: 13.162\n",
      "Loss after mini batch    41: 13.116\n",
      "Loss after mini batch    41: 11.753\n",
      "Loss after mini batch    41: 13.114\n",
      "Loss after mini batch    41: 12.144\n",
      "Loss after mini batch    41: 12.329\n",
      "Loss after mini batch    41: 14.465\n",
      "Starting epoch 42\n",
      "Loss after mini batch    42: 11.825\n",
      "Loss after mini batch    42: 13.082\n",
      "Loss after mini batch    42: 12.808\n",
      "Loss after mini batch    42: 13.523\n",
      "Loss after mini batch    42: 13.140\n",
      "Loss after mini batch    42: 14.087\n",
      "Loss after mini batch    42: 14.145\n",
      "Loss after mini batch    42: 13.086\n",
      "Loss after mini batch    42: 14.389\n",
      "Loss after mini batch    42: 12.060\n",
      "Loss after mini batch    42: 13.595\n",
      "Starting epoch 43\n",
      "Loss after mini batch    43: 12.020\n",
      "Loss after mini batch    43: 12.742\n",
      "Loss after mini batch    43: 13.068\n",
      "Loss after mini batch    43: 13.440\n",
      "Loss after mini batch    43: 13.514\n",
      "Loss after mini batch    43: 11.910\n",
      "Loss after mini batch    43: 13.967\n",
      "Loss after mini batch    43: 14.090\n",
      "Loss after mini batch    43: 13.739\n",
      "Loss after mini batch    43: 12.590\n",
      "Loss after mini batch    43: 13.037\n",
      "Starting epoch 44\n",
      "Loss after mini batch    44: 11.684\n",
      "Loss after mini batch    44: 12.890\n",
      "Loss after mini batch    44: 13.441\n",
      "Loss after mini batch    44: 14.150\n",
      "Loss after mini batch    44: 13.822\n",
      "Loss after mini batch    44: 14.059\n",
      "Loss after mini batch    44: 12.382\n",
      "Loss after mini batch    44: 13.656\n",
      "Loss after mini batch    44: 12.654\n",
      "Loss after mini batch    44: 13.102\n",
      "Loss after mini batch    44: 12.268\n",
      "Starting epoch 45\n",
      "Loss after mini batch    45: 13.576\n",
      "Loss after mini batch    45: 13.216\n",
      "Loss after mini batch    45: 13.661\n",
      "Loss after mini batch    45: 12.529\n",
      "Loss after mini batch    45: 12.782\n",
      "Loss after mini batch    45: 13.859\n",
      "Loss after mini batch    45: 12.264\n",
      "Loss after mini batch    45: 13.503\n",
      "Loss after mini batch    45: 12.280\n",
      "Loss after mini batch    45: 13.119\n",
      "Loss after mini batch    45: 12.563\n",
      "Starting epoch 46\n",
      "Loss after mini batch    46: 11.914\n",
      "Loss after mini batch    46: 12.549\n",
      "Loss after mini batch    46: 13.083\n",
      "Loss after mini batch    46: 13.677\n",
      "Loss after mini batch    46: 14.084\n",
      "Loss after mini batch    46: 12.948\n",
      "Loss after mini batch    46: 13.827\n",
      "Loss after mini batch    46: 13.317\n",
      "Loss after mini batch    46: 11.529\n",
      "Loss after mini batch    46: 13.166\n",
      "Loss after mini batch    46: 14.094\n",
      "Starting epoch 47\n",
      "Loss after mini batch    47: 12.911\n",
      "Loss after mini batch    47: 13.538\n",
      "Loss after mini batch    47: 12.765\n",
      "Loss after mini batch    47: 14.359\n",
      "Loss after mini batch    47: 11.657\n",
      "Loss after mini batch    47: 12.069\n",
      "Loss after mini batch    47: 16.453\n",
      "Loss after mini batch    47: 12.633\n",
      "Loss after mini batch    47: 12.259\n",
      "Loss after mini batch    47: 10.574\n",
      "Loss after mini batch    47: 13.295\n",
      "Starting epoch 48\n",
      "Loss after mini batch    48: 13.864\n",
      "Loss after mini batch    48: 12.556\n",
      "Loss after mini batch    48: 11.708\n",
      "Loss after mini batch    48: 12.280\n",
      "Loss after mini batch    48: 14.051\n",
      "Loss after mini batch    48: 12.724\n",
      "Loss after mini batch    48: 14.415\n",
      "Loss after mini batch    48: 13.147\n",
      "Loss after mini batch    48: 11.835\n",
      "Loss after mini batch    48: 12.147\n",
      "Loss after mini batch    48: 12.835\n",
      "Starting epoch 49\n",
      "Loss after mini batch    49: 12.282\n",
      "Loss after mini batch    49: 14.150\n",
      "Loss after mini batch    49: 13.035\n",
      "Loss after mini batch    49: 13.562\n",
      "Loss after mini batch    49: 12.243\n",
      "Loss after mini batch    49: 11.474\n",
      "Loss after mini batch    49: 12.683\n",
      "Loss after mini batch    49: 12.981\n",
      "Loss after mini batch    49: 13.616\n",
      "Loss after mini batch    49: 13.224\n",
      "Loss after mini batch    49: 13.129\n",
      "Starting epoch 50\n",
      "Loss after mini batch    50: 14.083\n",
      "Loss after mini batch    50: 13.616\n",
      "Loss after mini batch    50: 13.819\n",
      "Loss after mini batch    50: 12.238\n",
      "Loss after mini batch    50: 14.627\n",
      "Loss after mini batch    50: 12.262\n",
      "Loss after mini batch    50: 12.308\n",
      "Loss after mini batch    50: 12.166\n",
      "Loss after mini batch    50: 12.996\n",
      "Loss after mini batch    50: 13.302\n",
      "Loss after mini batch    50: 13.599\n",
      "Starting epoch 51\n",
      "Loss after mini batch    51: 12.025\n",
      "Loss after mini batch    51: 12.262\n",
      "Loss after mini batch    51: 16.180\n",
      "Loss after mini batch    51: 13.054\n",
      "Loss after mini batch    51: 11.832\n",
      "Loss after mini batch    51: 13.777\n",
      "Loss after mini batch    51: 12.784\n",
      "Loss after mini batch    51: 12.840\n",
      "Loss after mini batch    51: 13.579\n",
      "Loss after mini batch    51: 13.994\n",
      "Loss after mini batch    51: 11.997\n",
      "Starting epoch 52\n",
      "Loss after mini batch    52: 13.558\n",
      "Loss after mini batch    52: 12.073\n",
      "Loss after mini batch    52: 12.438\n",
      "Loss after mini batch    52: 14.002\n",
      "Loss after mini batch    52: 12.953\n",
      "Loss after mini batch    52: 12.279\n",
      "Loss after mini batch    52: 11.991\n",
      "Loss after mini batch    52: 12.834\n",
      "Loss after mini batch    52: 12.311\n",
      "Loss after mini batch    52: 13.461\n",
      "Loss after mini batch    52: 12.021\n",
      "Starting epoch 53\n",
      "Loss after mini batch    53: 13.406\n",
      "Loss after mini batch    53: 12.701\n",
      "Loss after mini batch    53: 12.715\n",
      "Loss after mini batch    53: 12.360\n",
      "Loss after mini batch    53: 12.820\n",
      "Loss after mini batch    53: 13.852\n",
      "Loss after mini batch    53: 12.564\n",
      "Loss after mini batch    53: 12.693\n",
      "Loss after mini batch    53: 11.574\n",
      "Loss after mini batch    53: 14.032\n",
      "Loss after mini batch    53: 13.415\n",
      "Starting epoch 54\n",
      "Loss after mini batch    54: 12.692\n",
      "Loss after mini batch    54: 13.138\n",
      "Loss after mini batch    54: 12.967\n",
      "Loss after mini batch    54: 12.005\n",
      "Loss after mini batch    54: 13.433\n",
      "Loss after mini batch    54: 12.265\n",
      "Loss after mini batch    54: 12.982\n",
      "Loss after mini batch    54: 13.282\n",
      "Loss after mini batch    54: 12.008\n",
      "Loss after mini batch    54: 12.364\n",
      "Loss after mini batch    54: 12.990\n",
      "Starting epoch 55\n",
      "Loss after mini batch    55: 13.155\n",
      "Loss after mini batch    55: 14.259\n",
      "Loss after mini batch    55: 12.599\n",
      "Loss after mini batch    55: 11.751\n",
      "Loss after mini batch    55: 12.136\n",
      "Loss after mini batch    55: 12.893\n",
      "Loss after mini batch    55: 13.434\n",
      "Loss after mini batch    55: 12.286\n",
      "Loss after mini batch    55: 13.758\n",
      "Loss after mini batch    55: 13.237\n",
      "Loss after mini batch    55: 12.326\n",
      "Starting epoch 56\n",
      "Loss after mini batch    56: 13.901\n",
      "Loss after mini batch    56: 12.375\n",
      "Loss after mini batch    56: 12.012\n",
      "Loss after mini batch    56: 12.048\n",
      "Loss after mini batch    56: 12.552\n",
      "Loss after mini batch    56: 14.891\n",
      "Loss after mini batch    56: 12.463\n",
      "Loss after mini batch    56: 13.261\n",
      "Loss after mini batch    56: 12.487\n",
      "Loss after mini batch    56: 14.306\n",
      "Loss after mini batch    56: 13.909\n",
      "Starting epoch 57\n",
      "Loss after mini batch    57: 13.606\n",
      "Loss after mini batch    57: 12.324\n",
      "Loss after mini batch    57: 12.130\n",
      "Loss after mini batch    57: 12.717\n",
      "Loss after mini batch    57: 13.266\n",
      "Loss after mini batch    57: 12.637\n",
      "Loss after mini batch    57: 13.029\n",
      "Loss after mini batch    57: 13.556\n",
      "Loss after mini batch    57: 13.443\n",
      "Loss after mini batch    57: 12.452\n",
      "Loss after mini batch    57: 12.806\n",
      "Starting epoch 58\n",
      "Loss after mini batch    58: 13.291\n",
      "Loss after mini batch    58: 11.245\n",
      "Loss after mini batch    58: 13.024\n",
      "Loss after mini batch    58: 13.140\n",
      "Loss after mini batch    58: 14.117\n",
      "Loss after mini batch    58: 12.729\n",
      "Loss after mini batch    58: 13.284\n",
      "Loss after mini batch    58: 12.612\n",
      "Loss after mini batch    58: 13.648\n",
      "Loss after mini batch    58: 12.128\n",
      "Loss after mini batch    58: 11.874\n",
      "Starting epoch 59\n",
      "Loss after mini batch    59: 12.612\n",
      "Loss after mini batch    59: 12.049\n",
      "Loss after mini batch    59: 12.593\n",
      "Loss after mini batch    59: 13.040\n",
      "Loss after mini batch    59: 13.295\n",
      "Loss after mini batch    59: 12.964\n",
      "Loss after mini batch    59: 13.556\n",
      "Loss after mini batch    59: 13.265\n",
      "Loss after mini batch    59: 11.902\n",
      "Loss after mini batch    59: 13.568\n",
      "Loss after mini batch    59: 12.026\n",
      "Starting epoch 60\n",
      "Loss after mini batch    60: 12.881\n",
      "Loss after mini batch    60: 12.758\n",
      "Loss after mini batch    60: 13.851\n",
      "Loss after mini batch    60: 11.375\n",
      "Loss after mini batch    60: 11.617\n",
      "Loss after mini batch    60: 12.758\n",
      "Loss after mini batch    60: 13.882\n",
      "Loss after mini batch    60: 12.243\n",
      "Loss after mini batch    60: 12.335\n",
      "Loss after mini batch    60: 13.567\n",
      "Loss after mini batch    60: 13.040\n",
      "Starting epoch 61\n",
      "Loss after mini batch    61: 13.591\n",
      "Loss after mini batch    61: 13.188\n",
      "Loss after mini batch    61: 11.766\n",
      "Loss after mini batch    61: 12.352\n",
      "Loss after mini batch    61: 14.669\n",
      "Loss after mini batch    61: 12.527\n",
      "Loss after mini batch    61: 12.959\n",
      "Loss after mini batch    61: 12.405\n",
      "Loss after mini batch    61: 12.795\n",
      "Loss after mini batch    61: 13.011\n",
      "Loss after mini batch    61: 12.089\n",
      "Starting epoch 62\n",
      "Loss after mini batch    62: 13.636\n",
      "Loss after mini batch    62: 11.439\n",
      "Loss after mini batch    62: 12.067\n",
      "Loss after mini batch    62: 12.651\n",
      "Loss after mini batch    62: 12.643\n",
      "Loss after mini batch    62: 13.209\n",
      "Loss after mini batch    62: 14.585\n",
      "Loss after mini batch    62: 13.203\n",
      "Loss after mini batch    62: 15.732\n",
      "Loss after mini batch    62: 12.406\n",
      "Loss after mini batch    62: 13.331\n",
      "Starting epoch 63\n",
      "Loss after mini batch    63: 12.823\n",
      "Loss after mini batch    63: 12.803\n",
      "Loss after mini batch    63: 12.062\n",
      "Loss after mini batch    63: 12.508\n",
      "Loss after mini batch    63: 11.985\n",
      "Loss after mini batch    63: 11.235\n",
      "Loss after mini batch    63: 14.394\n",
      "Loss after mini batch    63: 12.716\n",
      "Loss after mini batch    63: 12.774\n",
      "Loss after mini batch    63: 13.205\n",
      "Loss after mini batch    63: 12.055\n",
      "Starting epoch 64\n",
      "Loss after mini batch    64: 13.305\n",
      "Loss after mini batch    64: 12.702\n",
      "Loss after mini batch    64: 12.837\n",
      "Loss after mini batch    64: 12.866\n",
      "Loss after mini batch    64: 12.799\n",
      "Loss after mini batch    64: 12.490\n",
      "Loss after mini batch    64: 13.319\n",
      "Loss after mini batch    64: 13.001\n",
      "Loss after mini batch    64: 13.831\n",
      "Loss after mini batch    64: 11.857\n",
      "Loss after mini batch    64: 13.574\n",
      "Starting epoch 65\n",
      "Loss after mini batch    65: 11.715\n",
      "Loss after mini batch    65: 12.199\n",
      "Loss after mini batch    65: 12.299\n",
      "Loss after mini batch    65: 13.248\n",
      "Loss after mini batch    65: 12.978\n",
      "Loss after mini batch    65: 12.070\n",
      "Loss after mini batch    65: 13.673\n",
      "Loss after mini batch    65: 13.430\n",
      "Loss after mini batch    65: 13.083\n",
      "Loss after mini batch    65: 12.498\n",
      "Loss after mini batch    65: 12.756\n",
      "Starting epoch 66\n",
      "Loss after mini batch    66: 12.140\n",
      "Loss after mini batch    66: 12.319\n",
      "Loss after mini batch    66: 11.725\n",
      "Loss after mini batch    66: 11.982\n",
      "Loss after mini batch    66: 12.877\n",
      "Loss after mini batch    66: 12.202\n",
      "Loss after mini batch    66: 13.921\n",
      "Loss after mini batch    66: 13.835\n",
      "Loss after mini batch    66: 12.581\n",
      "Loss after mini batch    66: 13.991\n",
      "Loss after mini batch    66: 12.933\n",
      "Starting epoch 67\n",
      "Loss after mini batch    67: 12.275\n",
      "Loss after mini batch    67: 11.140\n",
      "Loss after mini batch    67: 13.239\n",
      "Loss after mini batch    67: 13.906\n",
      "Loss after mini batch    67: 12.088\n",
      "Loss after mini batch    67: 12.473\n",
      "Loss after mini batch    67: 11.935\n",
      "Loss after mini batch    67: 12.665\n",
      "Loss after mini batch    67: 13.036\n",
      "Loss after mini batch    67: 12.048\n",
      "Loss after mini batch    67: 13.027\n",
      "Starting epoch 68\n",
      "Loss after mini batch    68: 11.992\n",
      "Loss after mini batch    68: 12.709\n",
      "Loss after mini batch    68: 13.473\n",
      "Loss after mini batch    68: 11.204\n",
      "Loss after mini batch    68: 14.640\n",
      "Loss after mini batch    68: 11.217\n",
      "Loss after mini batch    68: 12.789\n",
      "Loss after mini batch    68: 12.438\n",
      "Loss after mini batch    68: 13.283\n",
      "Loss after mini batch    68: 13.217\n",
      "Loss after mini batch    68: 12.964\n",
      "Starting epoch 69\n",
      "Loss after mini batch    69: 12.619\n",
      "Loss after mini batch    69: 12.334\n",
      "Loss after mini batch    69: 12.628\n",
      "Loss after mini batch    69: 12.999\n",
      "Loss after mini batch    69: 12.949\n",
      "Loss after mini batch    69: 12.327\n",
      "Loss after mini batch    69: 12.408\n",
      "Loss after mini batch    69: 12.424\n",
      "Loss after mini batch    69: 14.155\n",
      "Loss after mini batch    69: 12.072\n",
      "Loss after mini batch    69: 13.268\n",
      "Starting epoch 70\n",
      "Loss after mini batch    70: 14.323\n",
      "Loss after mini batch    70: 13.289\n",
      "Loss after mini batch    70: 11.737\n",
      "Loss after mini batch    70: 12.618\n",
      "Loss after mini batch    70: 12.405\n",
      "Loss after mini batch    70: 12.362\n",
      "Loss after mini batch    70: 13.004\n",
      "Loss after mini batch    70: 13.084\n",
      "Loss after mini batch    70: 12.208\n",
      "Loss after mini batch    70: 12.162\n",
      "Loss after mini batch    70: 10.812\n",
      "Starting epoch 71\n",
      "Loss after mini batch    71: 11.580\n",
      "Loss after mini batch    71: 13.633\n",
      "Loss after mini batch    71: 13.583\n",
      "Loss after mini batch    71: 12.979\n",
      "Loss after mini batch    71: 11.721\n",
      "Loss after mini batch    71: 11.833\n",
      "Loss after mini batch    71: 12.463\n",
      "Loss after mini batch    71: 12.874\n",
      "Loss after mini batch    71: 13.868\n",
      "Loss after mini batch    71: 11.573\n",
      "Loss after mini batch    71: 12.221\n",
      "Starting epoch 72\n",
      "Loss after mini batch    72: 12.648\n",
      "Loss after mini batch    72: 13.291\n",
      "Loss after mini batch    72: 13.270\n",
      "Loss after mini batch    72: 11.512\n",
      "Loss after mini batch    72: 12.156\n",
      "Loss after mini batch    72: 12.416\n",
      "Loss after mini batch    72: 12.125\n",
      "Loss after mini batch    72: 13.187\n",
      "Loss after mini batch    72: 12.132\n",
      "Loss after mini batch    72: 11.085\n",
      "Loss after mini batch    72: 10.838\n",
      "Starting epoch 73\n",
      "Loss after mini batch    73: 12.414\n",
      "Loss after mini batch    73: 13.426\n",
      "Loss after mini batch    73: 12.996\n",
      "Loss after mini batch    73: 13.647\n",
      "Loss after mini batch    73: 11.367\n",
      "Loss after mini batch    73: 13.431\n",
      "Loss after mini batch    73: 13.219\n",
      "Loss after mini batch    73: 11.329\n",
      "Loss after mini batch    73: 12.255\n",
      "Loss after mini batch    73: 12.703\n",
      "Loss after mini batch    73: 11.911\n",
      "Starting epoch 74\n",
      "Loss after mini batch    74: 12.771\n",
      "Loss after mini batch    74: 13.894\n",
      "Loss after mini batch    74: 13.935\n",
      "Loss after mini batch    74: 12.289\n",
      "Loss after mini batch    74: 11.933\n",
      "Loss after mini batch    74: 11.884\n",
      "Loss after mini batch    74: 12.537\n",
      "Loss after mini batch    74: 11.310\n",
      "Loss after mini batch    74: 11.844\n",
      "Loss after mini batch    74: 14.801\n",
      "Loss after mini batch    74: 11.341\n",
      "Starting epoch 75\n",
      "Loss after mini batch    75: 11.380\n",
      "Loss after mini batch    75: 13.121\n",
      "Loss after mini batch    75: 12.750\n",
      "Loss after mini batch    75: 11.650\n",
      "Loss after mini batch    75: 12.275\n",
      "Loss after mini batch    75: 14.277\n",
      "Loss after mini batch    75: 11.599\n",
      "Loss after mini batch    75: 12.078\n",
      "Loss after mini batch    75: 12.388\n",
      "Loss after mini batch    75: 13.433\n",
      "Loss after mini batch    75: 13.944\n",
      "Starting epoch 76\n",
      "Loss after mini batch    76: 12.172\n",
      "Loss after mini batch    76: 12.834\n",
      "Loss after mini batch    76: 11.885\n",
      "Loss after mini batch    76: 15.561\n",
      "Loss after mini batch    76: 11.564\n",
      "Loss after mini batch    76: 12.403\n",
      "Loss after mini batch    76: 12.018\n",
      "Loss after mini batch    76: 12.571\n",
      "Loss after mini batch    76: 12.583\n",
      "Loss after mini batch    76: 11.261\n",
      "Loss after mini batch    76: 12.186\n",
      "Starting epoch 77\n",
      "Loss after mini batch    77: 13.574\n",
      "Loss after mini batch    77: 12.379\n",
      "Loss after mini batch    77: 14.075\n",
      "Loss after mini batch    77: 13.723\n",
      "Loss after mini batch    77: 11.108\n",
      "Loss after mini batch    77: 12.779\n",
      "Loss after mini batch    77: 12.240\n",
      "Loss after mini batch    77: 12.400\n",
      "Loss after mini batch    77: 11.908\n",
      "Loss after mini batch    77: 11.398\n",
      "Loss after mini batch    77: 11.714\n",
      "Starting epoch 78\n",
      "Loss after mini batch    78: 11.813\n",
      "Loss after mini batch    78: 11.242\n",
      "Loss after mini batch    78: 11.782\n",
      "Loss after mini batch    78: 12.279\n",
      "Loss after mini batch    78: 12.740\n",
      "Loss after mini batch    78: 12.050\n",
      "Loss after mini batch    78: 12.607\n",
      "Loss after mini batch    78: 13.314\n",
      "Loss after mini batch    78: 12.227\n",
      "Loss after mini batch    78: 13.791\n",
      "Loss after mini batch    78: 13.350\n",
      "Starting epoch 79\n",
      "Loss after mini batch    79: 13.725\n",
      "Loss after mini batch    79: 12.044\n",
      "Loss after mini batch    79: 12.558\n",
      "Loss after mini batch    79: 13.069\n",
      "Loss after mini batch    79: 12.124\n",
      "Loss after mini batch    79: 13.225\n",
      "Loss after mini batch    79: 11.884\n",
      "Loss after mini batch    79: 12.626\n",
      "Loss after mini batch    79: 12.678\n",
      "Loss after mini batch    79: 12.856\n",
      "Loss after mini batch    79: 12.370\n",
      "Starting epoch 80\n",
      "Loss after mini batch    80: 12.049\n",
      "Loss after mini batch    80: 13.262\n",
      "Loss after mini batch    80: 13.302\n",
      "Loss after mini batch    80: 12.083\n",
      "Loss after mini batch    80: 11.769\n",
      "Loss after mini batch    80: 11.513\n",
      "Loss after mini batch    80: 13.176\n",
      "Loss after mini batch    80: 12.767\n",
      "Loss after mini batch    80: 12.439\n",
      "Loss after mini batch    80: 12.572\n",
      "Loss after mini batch    80: 12.375\n",
      "Starting epoch 81\n",
      "Loss after mini batch    81: 13.037\n",
      "Loss after mini batch    81: 12.907\n",
      "Loss after mini batch    81: 11.555\n",
      "Loss after mini batch    81: 13.257\n",
      "Loss after mini batch    81: 13.482\n",
      "Loss after mini batch    81: 12.054\n",
      "Loss after mini batch    81: 12.012\n",
      "Loss after mini batch    81: 11.783\n",
      "Loss after mini batch    81: 11.427\n",
      "Loss after mini batch    81: 11.372\n",
      "Loss after mini batch    81: 12.307\n",
      "Starting epoch 82\n",
      "Loss after mini batch    82: 12.582\n",
      "Loss after mini batch    82: 12.949\n",
      "Loss after mini batch    82: 12.722\n",
      "Loss after mini batch    82: 12.081\n",
      "Loss after mini batch    82: 12.231\n",
      "Loss after mini batch    82: 13.270\n",
      "Loss after mini batch    82: 12.508\n",
      "Loss after mini batch    82: 12.226\n",
      "Loss after mini batch    82: 12.894\n",
      "Loss after mini batch    82: 13.920\n",
      "Loss after mini batch    82: 13.529\n",
      "Starting epoch 83\n",
      "Loss after mini batch    83: 13.088\n",
      "Loss after mini batch    83: 12.897\n",
      "Loss after mini batch    83: 11.794\n",
      "Loss after mini batch    83: 12.444\n",
      "Loss after mini batch    83: 14.381\n",
      "Loss after mini batch    83: 12.217\n",
      "Loss after mini batch    83: 11.914\n",
      "Loss after mini batch    83: 13.030\n",
      "Loss after mini batch    83: 12.062\n",
      "Loss after mini batch    83: 11.525\n",
      "Loss after mini batch    83: 13.361\n",
      "Starting epoch 84\n",
      "Loss after mini batch    84: 12.418\n",
      "Loss after mini batch    84: 11.791\n",
      "Loss after mini batch    84: 13.028\n",
      "Loss after mini batch    84: 12.762\n",
      "Loss after mini batch    84: 11.383\n",
      "Loss after mini batch    84: 12.158\n",
      "Loss after mini batch    84: 12.626\n",
      "Loss after mini batch    84: 12.597\n",
      "Loss after mini batch    84: 12.239\n",
      "Loss after mini batch    84: 12.692\n",
      "Loss after mini batch    84: 11.723\n",
      "Starting epoch 85\n",
      "Loss after mini batch    85: 12.424\n",
      "Loss after mini batch    85: 12.247\n",
      "Loss after mini batch    85: 12.942\n",
      "Loss after mini batch    85: 11.538\n",
      "Loss after mini batch    85: 12.363\n",
      "Loss after mini batch    85: 12.009\n",
      "Loss after mini batch    85: 11.864\n",
      "Loss after mini batch    85: 12.840\n",
      "Loss after mini batch    85: 13.778\n",
      "Loss after mini batch    85: 12.322\n",
      "Loss after mini batch    85: 14.193\n",
      "Starting epoch 86\n",
      "Loss after mini batch    86: 12.430\n",
      "Loss after mini batch    86: 12.284\n",
      "Loss after mini batch    86: 12.166\n",
      "Loss after mini batch    86: 11.783\n",
      "Loss after mini batch    86: 12.548\n",
      "Loss after mini batch    86: 12.329\n",
      "Loss after mini batch    86: 11.894\n",
      "Loss after mini batch    86: 12.766\n",
      "Loss after mini batch    86: 13.236\n",
      "Loss after mini batch    86: 12.025\n",
      "Loss after mini batch    86: 12.050\n",
      "Starting epoch 87\n",
      "Loss after mini batch    87: 12.764\n",
      "Loss after mini batch    87: 12.522\n",
      "Loss after mini batch    87: 11.765\n",
      "Loss after mini batch    87: 13.262\n",
      "Loss after mini batch    87: 13.621\n",
      "Loss after mini batch    87: 12.846\n",
      "Loss after mini batch    87: 12.417\n",
      "Loss after mini batch    87: 10.678\n",
      "Loss after mini batch    87: 11.625\n",
      "Loss after mini batch    87: 12.027\n",
      "Loss after mini batch    87: 12.555\n",
      "Starting epoch 88\n",
      "Loss after mini batch    88: 13.106\n",
      "Loss after mini batch    88: 11.998\n",
      "Loss after mini batch    88: 12.132\n",
      "Loss after mini batch    88: 12.382\n",
      "Loss after mini batch    88: 10.658\n",
      "Loss after mini batch    88: 13.404\n",
      "Loss after mini batch    88: 12.439\n",
      "Loss after mini batch    88: 10.993\n",
      "Loss after mini batch    88: 13.821\n",
      "Loss after mini batch    88: 14.203\n",
      "Loss after mini batch    88: 12.804\n",
      "Starting epoch 89\n",
      "Loss after mini batch    89: 11.997\n",
      "Loss after mini batch    89: 14.806\n",
      "Loss after mini batch    89: 12.506\n",
      "Loss after mini batch    89: 11.381\n",
      "Loss after mini batch    89: 13.249\n",
      "Loss after mini batch    89: 11.325\n",
      "Loss after mini batch    89: 13.285\n",
      "Loss after mini batch    89: 11.541\n",
      "Loss after mini batch    89: 12.398\n",
      "Loss after mini batch    89: 11.714\n",
      "Loss after mini batch    89: 14.272\n",
      "Starting epoch 90\n",
      "Loss after mini batch    90: 11.648\n",
      "Loss after mini batch    90: 11.889\n",
      "Loss after mini batch    90: 12.850\n",
      "Loss after mini batch    90: 12.200\n",
      "Loss after mini batch    90: 13.657\n",
      "Loss after mini batch    90: 11.464\n",
      "Loss after mini batch    90: 12.787\n",
      "Loss after mini batch    90: 11.571\n",
      "Loss after mini batch    90: 12.204\n",
      "Loss after mini batch    90: 12.643\n",
      "Loss after mini batch    90: 12.249\n",
      "Starting epoch 91\n",
      "Loss after mini batch    91: 11.319\n",
      "Loss after mini batch    91: 11.973\n",
      "Loss after mini batch    91: 13.125\n",
      "Loss after mini batch    91: 13.617\n",
      "Loss after mini batch    91: 12.316\n",
      "Loss after mini batch    91: 12.009\n",
      "Loss after mini batch    91: 12.178\n",
      "Loss after mini batch    91: 12.886\n",
      "Loss after mini batch    91: 12.777\n",
      "Loss after mini batch    91: 12.441\n",
      "Loss after mini batch    91: 10.854\n",
      "Starting epoch 92\n",
      "Loss after mini batch    92: 12.750\n",
      "Loss after mini batch    92: 11.439\n",
      "Loss after mini batch    92: 11.504\n",
      "Loss after mini batch    92: 12.078\n",
      "Loss after mini batch    92: 14.137\n",
      "Loss after mini batch    92: 12.108\n",
      "Loss after mini batch    92: 11.929\n",
      "Loss after mini batch    92: 14.872\n",
      "Loss after mini batch    92: 11.328\n",
      "Loss after mini batch    92: 12.616\n",
      "Loss after mini batch    92: 12.785\n",
      "Starting epoch 93\n",
      "Loss after mini batch    93: 11.950\n",
      "Loss after mini batch    93: 13.127\n",
      "Loss after mini batch    93: 12.732\n",
      "Loss after mini batch    93: 12.020\n",
      "Loss after mini batch    93: 12.723\n",
      "Loss after mini batch    93: 11.001\n",
      "Loss after mini batch    93: 13.356\n",
      "Loss after mini batch    93: 12.710\n",
      "Loss after mini batch    93: 11.262\n",
      "Loss after mini batch    93: 14.163\n",
      "Loss after mini batch    93: 11.590\n",
      "Starting epoch 94\n",
      "Loss after mini batch    94: 12.824\n",
      "Loss after mini batch    94: 12.131\n",
      "Loss after mini batch    94: 13.075\n",
      "Loss after mini batch    94: 12.268\n",
      "Loss after mini batch    94: 11.742\n",
      "Loss after mini batch    94: 12.173\n",
      "Loss after mini batch    94: 10.905\n",
      "Loss after mini batch    94: 11.663\n",
      "Loss after mini batch    94: 13.533\n",
      "Loss after mini batch    94: 12.402\n",
      "Loss after mini batch    94: 12.559\n",
      "Starting epoch 95\n",
      "Loss after mini batch    95: 12.346\n",
      "Loss after mini batch    95: 11.933\n",
      "Loss after mini batch    95: 12.082\n",
      "Loss after mini batch    95: 12.176\n",
      "Loss after mini batch    95: 11.180\n",
      "Loss after mini batch    95: 13.374\n",
      "Loss after mini batch    95: 13.068\n",
      "Loss after mini batch    95: 11.584\n",
      "Loss after mini batch    95: 12.592\n",
      "Loss after mini batch    95: 13.330\n",
      "Loss after mini batch    95: 11.689\n",
      "Starting epoch 96\n",
      "Loss after mini batch    96: 11.414\n",
      "Loss after mini batch    96: 13.017\n",
      "Loss after mini batch    96: 12.904\n",
      "Loss after mini batch    96: 12.225\n",
      "Loss after mini batch    96: 13.063\n",
      "Loss after mini batch    96: 13.256\n",
      "Loss after mini batch    96: 12.607\n",
      "Loss after mini batch    96: 11.627\n",
      "Loss after mini batch    96: 11.198\n",
      "Loss after mini batch    96: 13.069\n",
      "Loss after mini batch    96: 11.395\n",
      "Starting epoch 97\n",
      "Loss after mini batch    97: 12.067\n",
      "Loss after mini batch    97: 13.251\n",
      "Loss after mini batch    97: 12.804\n",
      "Loss after mini batch    97: 12.860\n",
      "Loss after mini batch    97: 11.338\n",
      "Loss after mini batch    97: 11.987\n",
      "Loss after mini batch    97: 13.671\n",
      "Loss after mini batch    97: 13.280\n",
      "Loss after mini batch    97: 11.881\n",
      "Loss after mini batch    97: 10.445\n",
      "Loss after mini batch    97: 11.670\n",
      "Starting epoch 98\n",
      "Loss after mini batch    98: 12.478\n",
      "Loss after mini batch    98: 11.623\n",
      "Loss after mini batch    98: 12.077\n",
      "Loss after mini batch    98: 11.682\n",
      "Loss after mini batch    98: 11.663\n",
      "Loss after mini batch    98: 13.402\n",
      "Loss after mini batch    98: 13.673\n",
      "Loss after mini batch    98: 11.541\n",
      "Loss after mini batch    98: 12.218\n",
      "Loss after mini batch    98: 12.751\n",
      "Loss after mini batch    98: 13.314\n",
      "Starting epoch 99\n",
      "Loss after mini batch    99: 11.798\n",
      "Loss after mini batch    99: 12.465\n",
      "Loss after mini batch    99: 12.459\n",
      "Loss after mini batch    99: 13.138\n",
      "Loss after mini batch    99: 12.894\n",
      "Loss after mini batch    99: 11.839\n",
      "Loss after mini batch    99: 12.596\n",
      "Loss after mini batch    99: 11.222\n",
      "Loss after mini batch    99: 11.991\n",
      "Loss after mini batch    99: 12.889\n",
      "Loss after mini batch    99: 12.511\n",
      "Starting epoch 100\n",
      "Loss after mini batch   100: 12.448\n",
      "Loss after mini batch   100: 13.451\n",
      "Loss after mini batch   100: 13.193\n",
      "Loss after mini batch   100: 13.134\n",
      "Loss after mini batch   100: 10.485\n",
      "Loss after mini batch   100: 12.023\n",
      "Loss after mini batch   100: 11.829\n",
      "Loss after mini batch   100: 12.633\n",
      "Loss after mini batch   100: 12.058\n",
      "Loss after mini batch   100: 13.196\n",
      "Loss after mini batch   100: 12.982\n",
      "Training Complete\n",
      "Starting Testing\n",
      "RMSE for fold 3: 3.531483060184505\n",
      "rRMSE for fold 3: 0.06964784901981873\n",
      "r for fold 3: 0.9900056435084453\n",
      "Fast RMSE for fold 3: 3.2976330794058337\n",
      "Fast rRMSE for fold 3: 0.06565432435100639\n",
      "Fast r for fold 3: 0.9815382868014074\n",
      "Slow RMSE for fold 3: 4.576151660153282\n",
      "Slow rRMSE for fold 3: 0.08980221176763709\n",
      "Slow r for fold 3: 0.9975971528220822\n",
      "Regular RMSE for fold 3: 2.6203150040052137\n",
      "Regular rRMSE for fold 3: 0.05141586756410306\n",
      "Regular r for fold 3: 0.9963610330003572\n",
      "Fold 4\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini batch     1: 280.134\n",
      "Loss after mini batch     1: 20.346\n",
      "Loss after mini batch     1: 18.383\n",
      "Loss after mini batch     1: 20.009\n",
      "Loss after mini batch     1: 21.275\n",
      "Loss after mini batch     1: 22.170\n",
      "Loss after mini batch     1: 18.540\n",
      "Loss after mini batch     1: 30.081\n",
      "Loss after mini batch     1: 29.376\n",
      "Loss after mini batch     1: 18.277\n",
      "Loss after mini batch     1: 22.617\n",
      "Starting epoch 2\n",
      "Loss after mini batch     2: 23.847\n",
      "Loss after mini batch     2: 21.880\n",
      "Loss after mini batch     2: 23.675\n",
      "Loss after mini batch     2: 19.361\n",
      "Loss after mini batch     2: 18.697\n",
      "Loss after mini batch     2: 18.107\n",
      "Loss after mini batch     2: 16.952\n",
      "Loss after mini batch     2: 18.168\n",
      "Loss after mini batch     2: 21.978\n",
      "Loss after mini batch     2: 22.511\n",
      "Loss after mini batch     2: 23.831\n",
      "Starting epoch 3\n",
      "Loss after mini batch     3: 23.877\n",
      "Loss after mini batch     3: 26.827\n",
      "Loss after mini batch     3: 18.444\n",
      "Loss after mini batch     3: 17.839\n",
      "Loss after mini batch     3: 17.956\n",
      "Loss after mini batch     3: 28.388\n",
      "Loss after mini batch     3: 15.657\n",
      "Loss after mini batch     3: 19.357\n",
      "Loss after mini batch     3: 18.505\n",
      "Loss after mini batch     3: 14.853\n",
      "Loss after mini batch     3: 19.385\n",
      "Starting epoch 4\n",
      "Loss after mini batch     4: 13.779\n",
      "Loss after mini batch     4: 14.659\n",
      "Loss after mini batch     4: 18.679\n",
      "Loss after mini batch     4: 14.376\n",
      "Loss after mini batch     4: 20.434\n",
      "Loss after mini batch     4: 18.007\n",
      "Loss after mini batch     4: 14.621\n",
      "Loss after mini batch     4: 15.548\n",
      "Loss after mini batch     4: 17.802\n",
      "Loss after mini batch     4: 13.226\n",
      "Loss after mini batch     4: 17.119\n",
      "Starting epoch 5\n",
      "Loss after mini batch     5: 18.055\n",
      "Loss after mini batch     5: 19.449\n",
      "Loss after mini batch     5: 15.677\n",
      "Loss after mini batch     5: 14.769\n",
      "Loss after mini batch     5: 17.369\n",
      "Loss after mini batch     5: 15.740\n",
      "Loss after mini batch     5: 17.798\n",
      "Loss after mini batch     5: 12.849\n",
      "Loss after mini batch     5: 15.853\n",
      "Loss after mini batch     5: 19.825\n",
      "Loss after mini batch     5: 15.287\n",
      "Starting epoch 6\n",
      "Loss after mini batch     6: 13.688\n",
      "Loss after mini batch     6: 16.815\n",
      "Loss after mini batch     6: 17.514\n",
      "Loss after mini batch     6: 14.878\n",
      "Loss after mini batch     6: 15.068\n",
      "Loss after mini batch     6: 14.840\n",
      "Loss after mini batch     6: 14.084\n",
      "Loss after mini batch     6: 12.989\n",
      "Loss after mini batch     6: 15.352\n",
      "Loss after mini batch     6: 13.364\n",
      "Loss after mini batch     6: 17.224\n",
      "Starting epoch 7\n",
      "Loss after mini batch     7: 13.226\n",
      "Loss after mini batch     7: 17.951\n",
      "Loss after mini batch     7: 15.980\n",
      "Loss after mini batch     7: 16.649\n",
      "Loss after mini batch     7: 19.208\n",
      "Loss after mini batch     7: 15.464\n",
      "Loss after mini batch     7: 16.680\n",
      "Loss after mini batch     7: 13.498\n",
      "Loss after mini batch     7: 19.201\n",
      "Loss after mini batch     7: 16.334\n",
      "Loss after mini batch     7: 14.231\n",
      "Starting epoch 8\n",
      "Loss after mini batch     8: 12.759\n",
      "Loss after mini batch     8: 15.338\n",
      "Loss after mini batch     8: 13.022\n",
      "Loss after mini batch     8: 14.329\n",
      "Loss after mini batch     8: 15.105\n",
      "Loss after mini batch     8: 13.594\n",
      "Loss after mini batch     8: 13.610\n",
      "Loss after mini batch     8: 15.767\n",
      "Loss after mini batch     8: 14.335\n",
      "Loss after mini batch     8: 14.530\n",
      "Loss after mini batch     8: 19.712\n",
      "Starting epoch 9\n",
      "Loss after mini batch     9: 19.304\n",
      "Loss after mini batch     9: 13.787\n",
      "Loss after mini batch     9: 16.374\n",
      "Loss after mini batch     9: 15.381\n",
      "Loss after mini batch     9: 15.459\n",
      "Loss after mini batch     9: 15.814\n",
      "Loss after mini batch     9: 12.431\n",
      "Loss after mini batch     9: 14.706\n",
      "Loss after mini batch     9: 14.901\n",
      "Loss after mini batch     9: 16.380\n",
      "Loss after mini batch     9: 14.379\n",
      "Starting epoch 10\n",
      "Loss after mini batch    10: 14.184\n",
      "Loss after mini batch    10: 14.982\n",
      "Loss after mini batch    10: 15.454\n",
      "Loss after mini batch    10: 14.109\n",
      "Loss after mini batch    10: 17.222\n",
      "Loss after mini batch    10: 13.262\n",
      "Loss after mini batch    10: 15.853\n",
      "Loss after mini batch    10: 15.932\n",
      "Loss after mini batch    10: 14.829\n",
      "Loss after mini batch    10: 13.212\n",
      "Loss after mini batch    10: 15.772\n",
      "Starting epoch 11\n",
      "Loss after mini batch    11: 16.918\n",
      "Loss after mini batch    11: 16.126\n",
      "Loss after mini batch    11: 16.031\n",
      "Loss after mini batch    11: 15.017\n",
      "Loss after mini batch    11: 13.226\n",
      "Loss after mini batch    11: 12.991\n",
      "Loss after mini batch    11: 13.659\n",
      "Loss after mini batch    11: 14.411\n",
      "Loss after mini batch    11: 13.780\n",
      "Loss after mini batch    11: 13.731\n",
      "Loss after mini batch    11: 14.551\n",
      "Starting epoch 12\n",
      "Loss after mini batch    12: 14.625\n",
      "Loss after mini batch    12: 14.616\n",
      "Loss after mini batch    12: 15.606\n",
      "Loss after mini batch    12: 13.829\n",
      "Loss after mini batch    12: 16.702\n",
      "Loss after mini batch    12: 16.160\n",
      "Loss after mini batch    12: 15.031\n",
      "Loss after mini batch    12: 16.313\n",
      "Loss after mini batch    12: 15.425\n",
      "Loss after mini batch    12: 14.285\n",
      "Loss after mini batch    12: 13.396\n",
      "Starting epoch 13\n",
      "Loss after mini batch    13: 13.993\n",
      "Loss after mini batch    13: 14.544\n",
      "Loss after mini batch    13: 14.840\n",
      "Loss after mini batch    13: 15.541\n",
      "Loss after mini batch    13: 13.495\n",
      "Loss after mini batch    13: 12.452\n",
      "Loss after mini batch    13: 13.500\n",
      "Loss after mini batch    13: 14.538\n",
      "Loss after mini batch    13: 16.008\n",
      "Loss after mini batch    13: 13.876\n",
      "Loss after mini batch    13: 13.964\n",
      "Starting epoch 14\n",
      "Loss after mini batch    14: 15.368\n",
      "Loss after mini batch    14: 13.658\n",
      "Loss after mini batch    14: 14.109\n",
      "Loss after mini batch    14: 15.413\n",
      "Loss after mini batch    14: 12.395\n",
      "Loss after mini batch    14: 14.197\n",
      "Loss after mini batch    14: 12.281\n",
      "Loss after mini batch    14: 14.013\n",
      "Loss after mini batch    14: 17.513\n",
      "Loss after mini batch    14: 13.946\n",
      "Loss after mini batch    14: 15.506\n",
      "Starting epoch 15\n",
      "Loss after mini batch    15: 13.840\n",
      "Loss after mini batch    15: 15.523\n",
      "Loss after mini batch    15: 12.985\n",
      "Loss after mini batch    15: 12.869\n",
      "Loss after mini batch    15: 15.483\n",
      "Loss after mini batch    15: 14.203\n",
      "Loss after mini batch    15: 13.614\n",
      "Loss after mini batch    15: 14.221\n",
      "Loss after mini batch    15: 15.187\n",
      "Loss after mini batch    15: 12.848\n",
      "Loss after mini batch    15: 14.525\n",
      "Starting epoch 16\n",
      "Loss after mini batch    16: 13.520\n",
      "Loss after mini batch    16: 18.124\n",
      "Loss after mini batch    16: 15.834\n",
      "Loss after mini batch    16: 13.310\n",
      "Loss after mini batch    16: 14.857\n",
      "Loss after mini batch    16: 14.641\n",
      "Loss after mini batch    16: 13.226\n",
      "Loss after mini batch    16: 14.694\n",
      "Loss after mini batch    16: 13.114\n",
      "Loss after mini batch    16: 14.635\n",
      "Loss after mini batch    16: 14.368\n",
      "Starting epoch 17\n",
      "Loss after mini batch    17: 15.425\n",
      "Loss after mini batch    17: 14.928\n",
      "Loss after mini batch    17: 15.816\n",
      "Loss after mini batch    17: 13.849\n",
      "Loss after mini batch    17: 14.391\n",
      "Loss after mini batch    17: 14.559\n",
      "Loss after mini batch    17: 13.095\n",
      "Loss after mini batch    17: 12.731\n",
      "Loss after mini batch    17: 16.355\n",
      "Loss after mini batch    17: 14.029\n",
      "Loss after mini batch    17: 13.619\n",
      "Starting epoch 18\n",
      "Loss after mini batch    18: 14.043\n",
      "Loss after mini batch    18: 14.134\n",
      "Loss after mini batch    18: 13.948\n",
      "Loss after mini batch    18: 16.207\n",
      "Loss after mini batch    18: 13.564\n",
      "Loss after mini batch    18: 13.187\n",
      "Loss after mini batch    18: 14.021\n",
      "Loss after mini batch    18: 13.369\n",
      "Loss after mini batch    18: 15.217\n",
      "Loss after mini batch    18: 14.720\n",
      "Loss after mini batch    18: 14.173\n",
      "Starting epoch 19\n",
      "Loss after mini batch    19: 15.077\n",
      "Loss after mini batch    19: 13.967\n",
      "Loss after mini batch    19: 14.790\n",
      "Loss after mini batch    19: 13.991\n",
      "Loss after mini batch    19: 13.470\n",
      "Loss after mini batch    19: 14.198\n",
      "Loss after mini batch    19: 14.066\n",
      "Loss after mini batch    19: 14.706\n",
      "Loss after mini batch    19: 13.862\n",
      "Loss after mini batch    19: 13.293\n",
      "Loss after mini batch    19: 13.241\n",
      "Starting epoch 20\n",
      "Loss after mini batch    20: 14.813\n",
      "Loss after mini batch    20: 13.589\n",
      "Loss after mini batch    20: 15.702\n",
      "Loss after mini batch    20: 14.111\n",
      "Loss after mini batch    20: 13.364\n",
      "Loss after mini batch    20: 14.603\n",
      "Loss after mini batch    20: 13.747\n",
      "Loss after mini batch    20: 16.116\n",
      "Loss after mini batch    20: 16.065\n",
      "Loss after mini batch    20: 14.706\n",
      "Loss after mini batch    20: 12.177\n",
      "Starting epoch 21\n",
      "Loss after mini batch    21: 13.588\n",
      "Loss after mini batch    21: 14.210\n",
      "Loss after mini batch    21: 13.194\n",
      "Loss after mini batch    21: 15.425\n",
      "Loss after mini batch    21: 15.828\n",
      "Loss after mini batch    21: 16.756\n",
      "Loss after mini batch    21: 13.351\n",
      "Loss after mini batch    21: 12.696\n",
      "Loss after mini batch    21: 14.390\n",
      "Loss after mini batch    21: 12.185\n",
      "Loss after mini batch    21: 13.648\n",
      "Starting epoch 22\n",
      "Loss after mini batch    22: 14.011\n",
      "Loss after mini batch    22: 13.734\n",
      "Loss after mini batch    22: 13.368\n",
      "Loss after mini batch    22: 13.591\n",
      "Loss after mini batch    22: 13.855\n",
      "Loss after mini batch    22: 13.565\n",
      "Loss after mini batch    22: 15.828\n",
      "Loss after mini batch    22: 14.073\n",
      "Loss after mini batch    22: 13.142\n",
      "Loss after mini batch    22: 14.512\n",
      "Loss after mini batch    22: 12.967\n",
      "Starting epoch 23\n",
      "Loss after mini batch    23: 14.594\n",
      "Loss after mini batch    23: 14.088\n",
      "Loss after mini batch    23: 13.333\n",
      "Loss after mini batch    23: 13.478\n",
      "Loss after mini batch    23: 14.124\n",
      "Loss after mini batch    23: 14.276\n",
      "Loss after mini batch    23: 12.998\n",
      "Loss after mini batch    23: 14.053\n",
      "Loss after mini batch    23: 13.024\n",
      "Loss after mini batch    23: 14.739\n",
      "Loss after mini batch    23: 13.978\n",
      "Starting epoch 24\n",
      "Loss after mini batch    24: 13.240\n",
      "Loss after mini batch    24: 13.757\n",
      "Loss after mini batch    24: 13.523\n",
      "Loss after mini batch    24: 12.973\n",
      "Loss after mini batch    24: 14.418\n",
      "Loss after mini batch    24: 13.930\n",
      "Loss after mini batch    24: 13.933\n",
      "Loss after mini batch    24: 14.264\n",
      "Loss after mini batch    24: 13.356\n",
      "Loss after mini batch    24: 13.508\n",
      "Loss after mini batch    24: 14.683\n",
      "Starting epoch 25\n",
      "Loss after mini batch    25: 13.918\n",
      "Loss after mini batch    25: 14.971\n",
      "Loss after mini batch    25: 13.078\n",
      "Loss after mini batch    25: 14.680\n",
      "Loss after mini batch    25: 12.712\n",
      "Loss after mini batch    25: 13.854\n",
      "Loss after mini batch    25: 14.910\n",
      "Loss after mini batch    25: 14.627\n",
      "Loss after mini batch    25: 14.710\n",
      "Loss after mini batch    25: 14.450\n",
      "Loss after mini batch    25: 13.341\n",
      "Starting epoch 26\n",
      "Loss after mini batch    26: 13.331\n",
      "Loss after mini batch    26: 14.665\n",
      "Loss after mini batch    26: 13.257\n",
      "Loss after mini batch    26: 13.836\n",
      "Loss after mini batch    26: 15.762\n",
      "Loss after mini batch    26: 12.026\n",
      "Loss after mini batch    26: 13.526\n",
      "Loss after mini batch    26: 14.550\n",
      "Loss after mini batch    26: 13.953\n",
      "Loss after mini batch    26: 12.938\n",
      "Loss after mini batch    26: 14.870\n",
      "Starting epoch 27\n",
      "Loss after mini batch    27: 12.758\n",
      "Loss after mini batch    27: 13.367\n",
      "Loss after mini batch    27: 12.770\n",
      "Loss after mini batch    27: 14.166\n",
      "Loss after mini batch    27: 16.031\n",
      "Loss after mini batch    27: 14.383\n",
      "Loss after mini batch    27: 14.255\n",
      "Loss after mini batch    27: 13.718\n",
      "Loss after mini batch    27: 13.486\n",
      "Loss after mini batch    27: 13.143\n",
      "Loss after mini batch    27: 13.044\n",
      "Starting epoch 28\n",
      "Loss after mini batch    28: 13.901\n",
      "Loss after mini batch    28: 12.397\n",
      "Loss after mini batch    28: 12.968\n",
      "Loss after mini batch    28: 13.333\n",
      "Loss after mini batch    28: 13.252\n",
      "Loss after mini batch    28: 12.859\n",
      "Loss after mini batch    28: 14.365\n",
      "Loss after mini batch    28: 14.602\n",
      "Loss after mini batch    28: 13.804\n",
      "Loss after mini batch    28: 13.510\n",
      "Loss after mini batch    28: 14.631\n",
      "Starting epoch 29\n",
      "Loss after mini batch    29: 13.033\n",
      "Loss after mini batch    29: 13.376\n",
      "Loss after mini batch    29: 12.278\n",
      "Loss after mini batch    29: 15.452\n",
      "Loss after mini batch    29: 12.758\n",
      "Loss after mini batch    29: 13.490\n",
      "Loss after mini batch    29: 13.756\n",
      "Loss after mini batch    29: 11.577\n",
      "Loss after mini batch    29: 14.532\n",
      "Loss after mini batch    29: 12.379\n",
      "Loss after mini batch    29: 12.940\n",
      "Starting epoch 30\n",
      "Loss after mini batch    30: 12.320\n",
      "Loss after mini batch    30: 12.917\n",
      "Loss after mini batch    30: 14.130\n",
      "Loss after mini batch    30: 12.772\n",
      "Loss after mini batch    30: 13.828\n",
      "Loss after mini batch    30: 14.240\n",
      "Loss after mini batch    30: 13.262\n",
      "Loss after mini batch    30: 14.140\n",
      "Loss after mini batch    30: 12.852\n",
      "Loss after mini batch    30: 13.819\n",
      "Loss after mini batch    30: 13.297\n",
      "Starting epoch 31\n",
      "Loss after mini batch    31: 12.825\n",
      "Loss after mini batch    31: 13.802\n",
      "Loss after mini batch    31: 13.249\n",
      "Loss after mini batch    31: 12.411\n",
      "Loss after mini batch    31: 13.308\n",
      "Loss after mini batch    31: 12.840\n",
      "Loss after mini batch    31: 14.624\n",
      "Loss after mini batch    31: 13.667\n",
      "Loss after mini batch    31: 15.770\n",
      "Loss after mini batch    31: 12.742\n",
      "Loss after mini batch    31: 13.263\n",
      "Starting epoch 32\n",
      "Loss after mini batch    32: 14.478\n",
      "Loss after mini batch    32: 11.694\n",
      "Loss after mini batch    32: 13.069\n",
      "Loss after mini batch    32: 12.960\n",
      "Loss after mini batch    32: 13.022\n",
      "Loss after mini batch    32: 12.939\n",
      "Loss after mini batch    32: 13.245\n",
      "Loss after mini batch    32: 12.392\n",
      "Loss after mini batch    32: 12.835\n",
      "Loss after mini batch    32: 13.818\n",
      "Loss after mini batch    32: 16.773\n",
      "Starting epoch 33\n",
      "Loss after mini batch    33: 14.019\n",
      "Loss after mini batch    33: 13.428\n",
      "Loss after mini batch    33: 12.263\n",
      "Loss after mini batch    33: 12.756\n",
      "Loss after mini batch    33: 14.036\n",
      "Loss after mini batch    33: 14.096\n",
      "Loss after mini batch    33: 12.812\n",
      "Loss after mini batch    33: 12.111\n",
      "Loss after mini batch    33: 13.109\n",
      "Loss after mini batch    33: 13.793\n",
      "Loss after mini batch    33: 14.263\n",
      "Starting epoch 34\n",
      "Loss after mini batch    34: 12.089\n",
      "Loss after mini batch    34: 14.528\n",
      "Loss after mini batch    34: 14.728\n",
      "Loss after mini batch    34: 13.094\n",
      "Loss after mini batch    34: 14.064\n",
      "Loss after mini batch    34: 13.453\n",
      "Loss after mini batch    34: 13.982\n",
      "Loss after mini batch    34: 13.279\n",
      "Loss after mini batch    34: 13.814\n",
      "Loss after mini batch    34: 12.611\n",
      "Loss after mini batch    34: 13.283\n",
      "Starting epoch 35\n",
      "Loss after mini batch    35: 12.789\n",
      "Loss after mini batch    35: 14.586\n",
      "Loss after mini batch    35: 13.814\n",
      "Loss after mini batch    35: 14.769\n",
      "Loss after mini batch    35: 12.145\n",
      "Loss after mini batch    35: 14.263\n",
      "Loss after mini batch    35: 13.112\n",
      "Loss after mini batch    35: 13.716\n",
      "Loss after mini batch    35: 11.801\n",
      "Loss after mini batch    35: 12.378\n",
      "Loss after mini batch    35: 12.235\n",
      "Starting epoch 36\n",
      "Loss after mini batch    36: 12.680\n",
      "Loss after mini batch    36: 12.113\n",
      "Loss after mini batch    36: 12.942\n",
      "Loss after mini batch    36: 12.857\n",
      "Loss after mini batch    36: 13.467\n",
      "Loss after mini batch    36: 12.619\n",
      "Loss after mini batch    36: 13.276\n",
      "Loss after mini batch    36: 12.438\n",
      "Loss after mini batch    36: 15.177\n",
      "Loss after mini batch    36: 14.164\n",
      "Loss after mini batch    36: 13.170\n",
      "Starting epoch 37\n",
      "Loss after mini batch    37: 13.048\n",
      "Loss after mini batch    37: 12.199\n",
      "Loss after mini batch    37: 14.105\n",
      "Loss after mini batch    37: 14.503\n",
      "Loss after mini batch    37: 14.645\n",
      "Loss after mini batch    37: 13.146\n",
      "Loss after mini batch    37: 14.029\n",
      "Loss after mini batch    37: 14.077\n",
      "Loss after mini batch    37: 13.009\n",
      "Loss after mini batch    37: 12.587\n",
      "Loss after mini batch    37: 13.624\n",
      "Starting epoch 38\n",
      "Loss after mini batch    38: 12.481\n",
      "Loss after mini batch    38: 12.723\n",
      "Loss after mini batch    38: 13.844\n",
      "Loss after mini batch    38: 14.567\n",
      "Loss after mini batch    38: 13.495\n",
      "Loss after mini batch    38: 14.312\n",
      "Loss after mini batch    38: 14.658\n",
      "Loss after mini batch    38: 12.629\n",
      "Loss after mini batch    38: 12.900\n",
      "Loss after mini batch    38: 12.828\n",
      "Loss after mini batch    38: 12.627\n",
      "Starting epoch 39\n",
      "Loss after mini batch    39: 12.951\n",
      "Loss after mini batch    39: 13.103\n",
      "Loss after mini batch    39: 13.705\n",
      "Loss after mini batch    39: 13.397\n",
      "Loss after mini batch    39: 13.481\n",
      "Loss after mini batch    39: 14.521\n",
      "Loss after mini batch    39: 11.952\n",
      "Loss after mini batch    39: 14.437\n",
      "Loss after mini batch    39: 12.374\n",
      "Loss after mini batch    39: 13.383\n",
      "Loss after mini batch    39: 12.453\n",
      "Starting epoch 40\n",
      "Loss after mini batch    40: 12.942\n",
      "Loss after mini batch    40: 13.024\n",
      "Loss after mini batch    40: 12.662\n",
      "Loss after mini batch    40: 11.614\n",
      "Loss after mini batch    40: 13.691\n",
      "Loss after mini batch    40: 12.027\n",
      "Loss after mini batch    40: 13.253\n",
      "Loss after mini batch    40: 13.793\n",
      "Loss after mini batch    40: 13.059\n",
      "Loss after mini batch    40: 14.207\n",
      "Loss after mini batch    40: 11.970\n",
      "Starting epoch 41\n",
      "Loss after mini batch    41: 13.729\n",
      "Loss after mini batch    41: 14.838\n",
      "Loss after mini batch    41: 13.507\n",
      "Loss after mini batch    41: 12.342\n",
      "Loss after mini batch    41: 12.893\n",
      "Loss after mini batch    41: 14.910\n",
      "Loss after mini batch    41: 15.085\n",
      "Loss after mini batch    41: 14.745\n",
      "Loss after mini batch    41: 13.490\n",
      "Loss after mini batch    41: 13.803\n",
      "Loss after mini batch    41: 12.377\n",
      "Starting epoch 42\n",
      "Loss after mini batch    42: 12.218\n",
      "Loss after mini batch    42: 13.116\n",
      "Loss after mini batch    42: 12.767\n",
      "Loss after mini batch    42: 12.984\n",
      "Loss after mini batch    42: 12.603\n",
      "Loss after mini batch    42: 14.156\n",
      "Loss after mini batch    42: 11.934\n",
      "Loss after mini batch    42: 14.061\n",
      "Loss after mini batch    42: 12.961\n",
      "Loss after mini batch    42: 14.045\n",
      "Loss after mini batch    42: 12.667\n",
      "Starting epoch 43\n",
      "Loss after mini batch    43: 13.213\n",
      "Loss after mini batch    43: 12.443\n",
      "Loss after mini batch    43: 12.732\n",
      "Loss after mini batch    43: 13.242\n",
      "Loss after mini batch    43: 13.671\n",
      "Loss after mini batch    43: 12.733\n",
      "Loss after mini batch    43: 13.958\n",
      "Loss after mini batch    43: 11.376\n",
      "Loss after mini batch    43: 12.975\n",
      "Loss after mini batch    43: 14.825\n",
      "Loss after mini batch    43: 12.390\n",
      "Starting epoch 44\n",
      "Loss after mini batch    44: 12.314\n",
      "Loss after mini batch    44: 13.293\n",
      "Loss after mini batch    44: 14.646\n",
      "Loss after mini batch    44: 15.318\n",
      "Loss after mini batch    44: 12.894\n",
      "Loss after mini batch    44: 12.493\n",
      "Loss after mini batch    44: 12.775\n",
      "Loss after mini batch    44: 12.673\n",
      "Loss after mini batch    44: 12.251\n",
      "Loss after mini batch    44: 13.195\n",
      "Loss after mini batch    44: 12.945\n",
      "Starting epoch 45\n",
      "Loss after mini batch    45: 14.310\n",
      "Loss after mini batch    45: 12.845\n",
      "Loss after mini batch    45: 13.407\n",
      "Loss after mini batch    45: 13.546\n",
      "Loss after mini batch    45: 12.967\n",
      "Loss after mini batch    45: 12.817\n",
      "Loss after mini batch    45: 13.058\n",
      "Loss after mini batch    45: 13.772\n",
      "Loss after mini batch    45: 13.143\n",
      "Loss after mini batch    45: 13.772\n",
      "Loss after mini batch    45: 13.108\n",
      "Starting epoch 46\n",
      "Loss after mini batch    46: 12.987\n",
      "Loss after mini batch    46: 12.964\n",
      "Loss after mini batch    46: 11.386\n",
      "Loss after mini batch    46: 14.666\n",
      "Loss after mini batch    46: 13.203\n",
      "Loss after mini batch    46: 13.328\n",
      "Loss after mini batch    46: 14.098\n",
      "Loss after mini batch    46: 13.214\n",
      "Loss after mini batch    46: 12.110\n",
      "Loss after mini batch    46: 13.606\n",
      "Loss after mini batch    46: 12.509\n",
      "Starting epoch 47\n",
      "Loss after mini batch    47: 13.281\n",
      "Loss after mini batch    47: 12.216\n",
      "Loss after mini batch    47: 12.936\n",
      "Loss after mini batch    47: 12.354\n",
      "Loss after mini batch    47: 13.358\n",
      "Loss after mini batch    47: 12.595\n",
      "Loss after mini batch    47: 12.951\n",
      "Loss after mini batch    47: 14.211\n",
      "Loss after mini batch    47: 13.580\n",
      "Loss after mini batch    47: 13.852\n",
      "Loss after mini batch    47: 13.467\n",
      "Starting epoch 48\n",
      "Loss after mini batch    48: 14.695\n",
      "Loss after mini batch    48: 12.871\n",
      "Loss after mini batch    48: 12.670\n",
      "Loss after mini batch    48: 13.206\n",
      "Loss after mini batch    48: 13.252\n",
      "Loss after mini batch    48: 14.747\n",
      "Loss after mini batch    48: 13.904\n",
      "Loss after mini batch    48: 13.699\n",
      "Loss after mini batch    48: 12.941\n",
      "Loss after mini batch    48: 12.779\n",
      "Loss after mini batch    48: 11.749\n",
      "Starting epoch 49\n",
      "Loss after mini batch    49: 12.290\n",
      "Loss after mini batch    49: 13.029\n",
      "Loss after mini batch    49: 15.320\n",
      "Loss after mini batch    49: 12.545\n",
      "Loss after mini batch    49: 12.744\n",
      "Loss after mini batch    49: 13.726\n",
      "Loss after mini batch    49: 13.543\n",
      "Loss after mini batch    49: 13.393\n",
      "Loss after mini batch    49: 12.903\n",
      "Loss after mini batch    49: 12.204\n",
      "Loss after mini batch    49: 13.046\n",
      "Starting epoch 50\n",
      "Loss after mini batch    50: 12.913\n",
      "Loss after mini batch    50: 12.736\n",
      "Loss after mini batch    50: 13.005\n",
      "Loss after mini batch    50: 12.784\n",
      "Loss after mini batch    50: 12.451\n",
      "Loss after mini batch    50: 12.778\n",
      "Loss after mini batch    50: 14.590\n",
      "Loss after mini batch    50: 13.771\n",
      "Loss after mini batch    50: 11.206\n",
      "Loss after mini batch    50: 14.463\n",
      "Loss after mini batch    50: 12.795\n",
      "Starting epoch 51\n",
      "Loss after mini batch    51: 12.927\n",
      "Loss after mini batch    51: 14.173\n",
      "Loss after mini batch    51: 11.335\n",
      "Loss after mini batch    51: 12.304\n",
      "Loss after mini batch    51: 11.765\n",
      "Loss after mini batch    51: 13.511\n",
      "Loss after mini batch    51: 12.995\n",
      "Loss after mini batch    51: 13.150\n",
      "Loss after mini batch    51: 13.200\n",
      "Loss after mini batch    51: 14.028\n",
      "Loss after mini batch    51: 13.474\n",
      "Starting epoch 52\n",
      "Loss after mini batch    52: 13.497\n",
      "Loss after mini batch    52: 13.218\n",
      "Loss after mini batch    52: 14.195\n",
      "Loss after mini batch    52: 11.501\n",
      "Loss after mini batch    52: 14.846\n",
      "Loss after mini batch    52: 13.720\n",
      "Loss after mini batch    52: 12.892\n",
      "Loss after mini batch    52: 13.734\n",
      "Loss after mini batch    52: 12.451\n",
      "Loss after mini batch    52: 12.585\n",
      "Loss after mini batch    52: 12.720\n",
      "Starting epoch 53\n",
      "Loss after mini batch    53: 12.311\n",
      "Loss after mini batch    53: 12.736\n",
      "Loss after mini batch    53: 13.562\n",
      "Loss after mini batch    53: 13.271\n",
      "Loss after mini batch    53: 13.035\n",
      "Loss after mini batch    53: 14.919\n",
      "Loss after mini batch    53: 13.364\n",
      "Loss after mini batch    53: 12.797\n",
      "Loss after mini batch    53: 13.989\n",
      "Loss after mini batch    53: 12.648\n",
      "Loss after mini batch    53: 13.220\n",
      "Starting epoch 54\n",
      "Loss after mini batch    54: 13.146\n",
      "Loss after mini batch    54: 14.008\n",
      "Loss after mini batch    54: 11.870\n",
      "Loss after mini batch    54: 13.523\n",
      "Loss after mini batch    54: 12.730\n",
      "Loss after mini batch    54: 13.596\n",
      "Loss after mini batch    54: 12.759\n",
      "Loss after mini batch    54: 12.356\n",
      "Loss after mini batch    54: 12.695\n",
      "Loss after mini batch    54: 12.243\n",
      "Loss after mini batch    54: 12.356\n",
      "Starting epoch 55\n",
      "Loss after mini batch    55: 13.458\n",
      "Loss after mini batch    55: 14.023\n",
      "Loss after mini batch    55: 14.277\n",
      "Loss after mini batch    55: 13.534\n",
      "Loss after mini batch    55: 11.759\n",
      "Loss after mini batch    55: 11.756\n",
      "Loss after mini batch    55: 11.906\n",
      "Loss after mini batch    55: 14.704\n",
      "Loss after mini batch    55: 14.330\n",
      "Loss after mini batch    55: 12.895\n",
      "Loss after mini batch    55: 13.917\n",
      "Starting epoch 56\n",
      "Loss after mini batch    56: 13.207\n",
      "Loss after mini batch    56: 13.111\n",
      "Loss after mini batch    56: 12.897\n",
      "Loss after mini batch    56: 11.710\n",
      "Loss after mini batch    56: 13.134\n",
      "Loss after mini batch    56: 13.379\n",
      "Loss after mini batch    56: 12.999\n",
      "Loss after mini batch    56: 12.563\n",
      "Loss after mini batch    56: 13.911\n",
      "Loss after mini batch    56: 13.228\n",
      "Loss after mini batch    56: 12.583\n",
      "Starting epoch 57\n",
      "Loss after mini batch    57: 14.178\n",
      "Loss after mini batch    57: 14.620\n",
      "Loss after mini batch    57: 11.593\n",
      "Loss after mini batch    57: 11.841\n",
      "Loss after mini batch    57: 13.591\n",
      "Loss after mini batch    57: 14.078\n",
      "Loss after mini batch    57: 12.479\n",
      "Loss after mini batch    57: 12.941\n",
      "Loss after mini batch    57: 12.911\n",
      "Loss after mini batch    57: 12.248\n",
      "Loss after mini batch    57: 13.055\n",
      "Starting epoch 58\n",
      "Loss after mini batch    58: 11.400\n",
      "Loss after mini batch    58: 12.643\n",
      "Loss after mini batch    58: 12.633\n",
      "Loss after mini batch    58: 13.537\n",
      "Loss after mini batch    58: 12.536\n",
      "Loss after mini batch    58: 13.531\n",
      "Loss after mini batch    58: 12.872\n",
      "Loss after mini batch    58: 11.987\n",
      "Loss after mini batch    58: 12.371\n",
      "Loss after mini batch    58: 14.619\n",
      "Loss after mini batch    58: 13.208\n",
      "Starting epoch 59\n",
      "Loss after mini batch    59: 14.533\n",
      "Loss after mini batch    59: 13.197\n",
      "Loss after mini batch    59: 13.031\n",
      "Loss after mini batch    59: 14.077\n",
      "Loss after mini batch    59: 12.620\n",
      "Loss after mini batch    59: 14.394\n",
      "Loss after mini batch    59: 13.274\n",
      "Loss after mini batch    59: 12.489\n",
      "Loss after mini batch    59: 11.653\n",
      "Loss after mini batch    59: 13.166\n",
      "Loss after mini batch    59: 13.303\n",
      "Starting epoch 60\n",
      "Loss after mini batch    60: 13.249\n",
      "Loss after mini batch    60: 14.103\n",
      "Loss after mini batch    60: 13.678\n",
      "Loss after mini batch    60: 11.535\n",
      "Loss after mini batch    60: 12.471\n",
      "Loss after mini batch    60: 13.148\n",
      "Loss after mini batch    60: 13.452\n",
      "Loss after mini batch    60: 13.419\n",
      "Loss after mini batch    60: 13.024\n",
      "Loss after mini batch    60: 13.222\n",
      "Loss after mini batch    60: 12.573\n",
      "Starting epoch 61\n",
      "Loss after mini batch    61: 13.350\n",
      "Loss after mini batch    61: 11.594\n",
      "Loss after mini batch    61: 12.902\n",
      "Loss after mini batch    61: 12.332\n",
      "Loss after mini batch    61: 13.836\n",
      "Loss after mini batch    61: 15.577\n",
      "Loss after mini batch    61: 12.799\n",
      "Loss after mini batch    61: 13.841\n",
      "Loss after mini batch    61: 12.871\n",
      "Loss after mini batch    61: 12.101\n",
      "Loss after mini batch    61: 12.643\n",
      "Starting epoch 62\n",
      "Loss after mini batch    62: 12.920\n",
      "Loss after mini batch    62: 12.879\n",
      "Loss after mini batch    62: 13.732\n",
      "Loss after mini batch    62: 13.379\n",
      "Loss after mini batch    62: 13.420\n",
      "Loss after mini batch    62: 12.106\n",
      "Loss after mini batch    62: 12.728\n",
      "Loss after mini batch    62: 12.875\n",
      "Loss after mini batch    62: 13.113\n",
      "Loss after mini batch    62: 11.883\n",
      "Loss after mini batch    62: 13.342\n",
      "Starting epoch 63\n",
      "Loss after mini batch    63: 12.447\n",
      "Loss after mini batch    63: 12.413\n",
      "Loss after mini batch    63: 12.446\n",
      "Loss after mini batch    63: 12.582\n",
      "Loss after mini batch    63: 13.970\n",
      "Loss after mini batch    63: 13.804\n",
      "Loss after mini batch    63: 14.584\n",
      "Loss after mini batch    63: 12.995\n",
      "Loss after mini batch    63: 12.053\n",
      "Loss after mini batch    63: 12.778\n",
      "Loss after mini batch    63: 13.042\n",
      "Starting epoch 64\n",
      "Loss after mini batch    64: 12.687\n",
      "Loss after mini batch    64: 12.010\n",
      "Loss after mini batch    64: 13.248\n",
      "Loss after mini batch    64: 11.894\n",
      "Loss after mini batch    64: 11.989\n",
      "Loss after mini batch    64: 13.100\n",
      "Loss after mini batch    64: 11.827\n",
      "Loss after mini batch    64: 13.936\n",
      "Loss after mini batch    64: 12.330\n",
      "Loss after mini batch    64: 14.123\n",
      "Loss after mini batch    64: 12.247\n",
      "Starting epoch 65\n",
      "Loss after mini batch    65: 13.401\n",
      "Loss after mini batch    65: 12.292\n",
      "Loss after mini batch    65: 12.924\n",
      "Loss after mini batch    65: 12.027\n",
      "Loss after mini batch    65: 13.789\n",
      "Loss after mini batch    65: 13.978\n",
      "Loss after mini batch    65: 12.607\n",
      "Loss after mini batch    65: 13.264\n",
      "Loss after mini batch    65: 13.926\n",
      "Loss after mini batch    65: 12.418\n",
      "Loss after mini batch    65: 12.451\n",
      "Starting epoch 66\n",
      "Loss after mini batch    66: 12.146\n",
      "Loss after mini batch    66: 13.281\n",
      "Loss after mini batch    66: 13.610\n",
      "Loss after mini batch    66: 13.717\n",
      "Loss after mini batch    66: 12.708\n",
      "Loss after mini batch    66: 11.941\n",
      "Loss after mini batch    66: 12.802\n",
      "Loss after mini batch    66: 12.077\n",
      "Loss after mini batch    66: 13.961\n",
      "Loss after mini batch    66: 13.419\n",
      "Loss after mini batch    66: 11.458\n",
      "Starting epoch 67\n",
      "Loss after mini batch    67: 12.205\n",
      "Loss after mini batch    67: 13.424\n",
      "Loss after mini batch    67: 13.175\n",
      "Loss after mini batch    67: 14.184\n",
      "Loss after mini batch    67: 12.003\n",
      "Loss after mini batch    67: 11.093\n",
      "Loss after mini batch    67: 13.009\n",
      "Loss after mini batch    67: 14.127\n",
      "Loss after mini batch    67: 12.424\n",
      "Loss after mini batch    67: 13.815\n",
      "Loss after mini batch    67: 12.144\n",
      "Starting epoch 68\n",
      "Loss after mini batch    68: 12.731\n",
      "Loss after mini batch    68: 12.436\n",
      "Loss after mini batch    68: 13.387\n",
      "Loss after mini batch    68: 12.708\n",
      "Loss after mini batch    68: 12.052\n",
      "Loss after mini batch    68: 11.859\n",
      "Loss after mini batch    68: 13.623\n",
      "Loss after mini batch    68: 12.632\n",
      "Loss after mini batch    68: 13.071\n",
      "Loss after mini batch    68: 12.835\n",
      "Loss after mini batch    68: 11.672\n",
      "Starting epoch 69\n",
      "Loss after mini batch    69: 12.494\n",
      "Loss after mini batch    69: 13.479\n",
      "Loss after mini batch    69: 11.434\n",
      "Loss after mini batch    69: 11.492\n",
      "Loss after mini batch    69: 14.412\n",
      "Loss after mini batch    69: 12.007\n",
      "Loss after mini batch    69: 12.905\n",
      "Loss after mini batch    69: 13.093\n",
      "Loss after mini batch    69: 13.079\n",
      "Loss after mini batch    69: 12.864\n",
      "Loss after mini batch    69: 12.059\n",
      "Starting epoch 70\n",
      "Loss after mini batch    70: 13.324\n",
      "Loss after mini batch    70: 13.149\n",
      "Loss after mini batch    70: 12.088\n",
      "Loss after mini batch    70: 11.727\n",
      "Loss after mini batch    70: 12.458\n",
      "Loss after mini batch    70: 12.099\n",
      "Loss after mini batch    70: 12.599\n",
      "Loss after mini batch    70: 12.543\n",
      "Loss after mini batch    70: 12.860\n",
      "Loss after mini batch    70: 12.448\n",
      "Loss after mini batch    70: 12.786\n",
      "Starting epoch 71\n",
      "Loss after mini batch    71: 11.350\n",
      "Loss after mini batch    71: 12.291\n",
      "Loss after mini batch    71: 13.339\n",
      "Loss after mini batch    71: 13.727\n",
      "Loss after mini batch    71: 12.109\n",
      "Loss after mini batch    71: 12.619\n",
      "Loss after mini batch    71: 12.219\n",
      "Loss after mini batch    71: 12.728\n",
      "Loss after mini batch    71: 12.566\n",
      "Loss after mini batch    71: 12.796\n",
      "Loss after mini batch    71: 13.891\n",
      "Starting epoch 72\n",
      "Loss after mini batch    72: 13.123\n",
      "Loss after mini batch    72: 12.615\n",
      "Loss after mini batch    72: 11.739\n",
      "Loss after mini batch    72: 11.985\n",
      "Loss after mini batch    72: 12.185\n",
      "Loss after mini batch    72: 13.650\n",
      "Loss after mini batch    72: 14.076\n",
      "Loss after mini batch    72: 11.439\n",
      "Loss after mini batch    72: 13.834\n",
      "Loss after mini batch    72: 12.993\n",
      "Loss after mini batch    72: 12.287\n",
      "Starting epoch 73\n",
      "Loss after mini batch    73: 12.559\n",
      "Loss after mini batch    73: 13.415\n",
      "Loss after mini batch    73: 11.967\n",
      "Loss after mini batch    73: 12.360\n",
      "Loss after mini batch    73: 12.381\n",
      "Loss after mini batch    73: 12.162\n",
      "Loss after mini batch    73: 13.214\n",
      "Loss after mini batch    73: 12.557\n",
      "Loss after mini batch    73: 12.279\n",
      "Loss after mini batch    73: 12.169\n",
      "Loss after mini batch    73: 12.948\n",
      "Starting epoch 74\n",
      "Loss after mini batch    74: 12.408\n",
      "Loss after mini batch    74: 11.770\n",
      "Loss after mini batch    74: 12.871\n",
      "Loss after mini batch    74: 12.632\n",
      "Loss after mini batch    74: 12.539\n",
      "Loss after mini batch    74: 12.440\n",
      "Loss after mini batch    74: 12.224\n",
      "Loss after mini batch    74: 14.315\n",
      "Loss after mini batch    74: 13.917\n",
      "Loss after mini batch    74: 13.127\n",
      "Loss after mini batch    74: 13.253\n",
      "Starting epoch 75\n",
      "Loss after mini batch    75: 12.430\n",
      "Loss after mini batch    75: 13.246\n",
      "Loss after mini batch    75: 12.593\n",
      "Loss after mini batch    75: 13.481\n",
      "Loss after mini batch    75: 13.131\n",
      "Loss after mini batch    75: 11.610\n",
      "Loss after mini batch    75: 12.691\n",
      "Loss after mini batch    75: 12.803\n",
      "Loss after mini batch    75: 12.458\n",
      "Loss after mini batch    75: 12.466\n",
      "Loss after mini batch    75: 12.452\n",
      "Starting epoch 76\n",
      "Loss after mini batch    76: 12.104\n",
      "Loss after mini batch    76: 12.662\n",
      "Loss after mini batch    76: 12.576\n",
      "Loss after mini batch    76: 12.833\n",
      "Loss after mini batch    76: 12.010\n",
      "Loss after mini batch    76: 13.386\n",
      "Loss after mini batch    76: 12.979\n",
      "Loss after mini batch    76: 13.336\n",
      "Loss after mini batch    76: 12.177\n",
      "Loss after mini batch    76: 12.394\n",
      "Loss after mini batch    76: 12.212\n",
      "Starting epoch 77\n",
      "Loss after mini batch    77: 13.511\n",
      "Loss after mini batch    77: 12.916\n",
      "Loss after mini batch    77: 13.774\n",
      "Loss after mini batch    77: 12.320\n",
      "Loss after mini batch    77: 12.542\n",
      "Loss after mini batch    77: 11.771\n",
      "Loss after mini batch    77: 12.608\n",
      "Loss after mini batch    77: 12.424\n",
      "Loss after mini batch    77: 11.757\n",
      "Loss after mini batch    77: 12.453\n",
      "Loss after mini batch    77: 12.059\n",
      "Starting epoch 78\n",
      "Loss after mini batch    78: 12.332\n",
      "Loss after mini batch    78: 13.108\n",
      "Loss after mini batch    78: 13.193\n",
      "Loss after mini batch    78: 12.388\n",
      "Loss after mini batch    78: 11.984\n",
      "Loss after mini batch    78: 12.560\n",
      "Loss after mini batch    78: 12.771\n",
      "Loss after mini batch    78: 13.576\n",
      "Loss after mini batch    78: 13.002\n",
      "Loss after mini batch    78: 12.769\n",
      "Loss after mini batch    78: 11.805\n",
      "Starting epoch 79\n",
      "Loss after mini batch    79: 11.922\n",
      "Loss after mini batch    79: 12.009\n",
      "Loss after mini batch    79: 12.249\n",
      "Loss after mini batch    79: 12.592\n",
      "Loss after mini batch    79: 12.837\n",
      "Loss after mini batch    79: 13.180\n",
      "Loss after mini batch    79: 13.295\n",
      "Loss after mini batch    79: 12.558\n",
      "Loss after mini batch    79: 13.181\n",
      "Loss after mini batch    79: 13.379\n",
      "Loss after mini batch    79: 12.486\n",
      "Starting epoch 80\n",
      "Loss after mini batch    80: 12.152\n",
      "Loss after mini batch    80: 13.427\n",
      "Loss after mini batch    80: 11.866\n",
      "Loss after mini batch    80: 12.710\n",
      "Loss after mini batch    80: 12.203\n",
      "Loss after mini batch    80: 11.015\n",
      "Loss after mini batch    80: 11.463\n",
      "Loss after mini batch    80: 13.005\n",
      "Loss after mini batch    80: 11.857\n",
      "Loss after mini batch    80: 12.804\n",
      "Loss after mini batch    80: 14.553\n",
      "Starting epoch 81\n",
      "Loss after mini batch    81: 13.298\n",
      "Loss after mini batch    81: 12.896\n",
      "Loss after mini batch    81: 12.239\n",
      "Loss after mini batch    81: 12.860\n",
      "Loss after mini batch    81: 11.390\n",
      "Loss after mini batch    81: 13.058\n",
      "Loss after mini batch    81: 12.224\n",
      "Loss after mini batch    81: 12.727\n",
      "Loss after mini batch    81: 12.208\n",
      "Loss after mini batch    81: 12.289\n",
      "Loss after mini batch    81: 12.612\n",
      "Starting epoch 82\n",
      "Loss after mini batch    82: 13.662\n",
      "Loss after mini batch    82: 12.130\n",
      "Loss after mini batch    82: 12.982\n",
      "Loss after mini batch    82: 12.424\n",
      "Loss after mini batch    82: 12.914\n",
      "Loss after mini batch    82: 12.516\n",
      "Loss after mini batch    82: 12.459\n",
      "Loss after mini batch    82: 11.413\n",
      "Loss after mini batch    82: 13.497\n",
      "Loss after mini batch    82: 12.283\n",
      "Loss after mini batch    82: 13.659\n",
      "Starting epoch 83\n",
      "Loss after mini batch    83: 12.347\n",
      "Loss after mini batch    83: 13.318\n",
      "Loss after mini batch    83: 12.342\n",
      "Loss after mini batch    83: 12.480\n",
      "Loss after mini batch    83: 11.910\n",
      "Loss after mini batch    83: 13.114\n",
      "Loss after mini batch    83: 12.895\n",
      "Loss after mini batch    83: 12.277\n",
      "Loss after mini batch    83: 12.906\n",
      "Loss after mini batch    83: 11.971\n",
      "Loss after mini batch    83: 11.848\n",
      "Starting epoch 84\n",
      "Loss after mini batch    84: 12.788\n",
      "Loss after mini batch    84: 13.067\n",
      "Loss after mini batch    84: 11.601\n",
      "Loss after mini batch    84: 12.539\n",
      "Loss after mini batch    84: 11.802\n",
      "Loss after mini batch    84: 11.721\n",
      "Loss after mini batch    84: 12.772\n",
      "Loss after mini batch    84: 11.549\n",
      "Loss after mini batch    84: 12.808\n",
      "Loss after mini batch    84: 12.935\n",
      "Loss after mini batch    84: 13.823\n",
      "Starting epoch 85\n",
      "Loss after mini batch    85: 12.892\n",
      "Loss after mini batch    85: 12.419\n",
      "Loss after mini batch    85: 12.343\n",
      "Loss after mini batch    85: 13.709\n",
      "Loss after mini batch    85: 11.988\n",
      "Loss after mini batch    85: 13.127\n",
      "Loss after mini batch    85: 12.681\n",
      "Loss after mini batch    85: 11.434\n",
      "Loss after mini batch    85: 11.575\n",
      "Loss after mini batch    85: 12.765\n",
      "Loss after mini batch    85: 11.826\n",
      "Starting epoch 86\n",
      "Loss after mini batch    86: 11.910\n",
      "Loss after mini batch    86: 12.772\n",
      "Loss after mini batch    86: 12.357\n",
      "Loss after mini batch    86: 12.157\n",
      "Loss after mini batch    86: 13.477\n",
      "Loss after mini batch    86: 12.249\n",
      "Loss after mini batch    86: 11.743\n",
      "Loss after mini batch    86: 13.065\n",
      "Loss after mini batch    86: 12.729\n",
      "Loss after mini batch    86: 12.484\n",
      "Loss after mini batch    86: 12.237\n",
      "Starting epoch 87\n",
      "Loss after mini batch    87: 11.913\n",
      "Loss after mini batch    87: 12.854\n",
      "Loss after mini batch    87: 12.016\n",
      "Loss after mini batch    87: 12.282\n",
      "Loss after mini batch    87: 14.500\n",
      "Loss after mini batch    87: 13.677\n",
      "Loss after mini batch    87: 13.844\n",
      "Loss after mini batch    87: 10.884\n",
      "Loss after mini batch    87: 13.443\n",
      "Loss after mini batch    87: 12.803\n",
      "Loss after mini batch    87: 11.512\n",
      "Starting epoch 88\n",
      "Loss after mini batch    88: 12.987\n",
      "Loss after mini batch    88: 12.012\n",
      "Loss after mini batch    88: 12.424\n",
      "Loss after mini batch    88: 11.677\n",
      "Loss after mini batch    88: 11.768\n",
      "Loss after mini batch    88: 14.216\n",
      "Loss after mini batch    88: 12.912\n",
      "Loss after mini batch    88: 11.494\n",
      "Loss after mini batch    88: 12.612\n",
      "Loss after mini batch    88: 12.990\n",
      "Loss after mini batch    88: 12.474\n",
      "Starting epoch 89\n",
      "Loss after mini batch    89: 13.416\n",
      "Loss after mini batch    89: 11.907\n",
      "Loss after mini batch    89: 12.484\n",
      "Loss after mini batch    89: 13.032\n",
      "Loss after mini batch    89: 13.017\n",
      "Loss after mini batch    89: 12.954\n",
      "Loss after mini batch    89: 11.347\n",
      "Loss after mini batch    89: 10.639\n",
      "Loss after mini batch    89: 14.114\n",
      "Loss after mini batch    89: 12.740\n",
      "Loss after mini batch    89: 12.772\n",
      "Starting epoch 90\n",
      "Loss after mini batch    90: 13.423\n",
      "Loss after mini batch    90: 12.033\n",
      "Loss after mini batch    90: 12.278\n",
      "Loss after mini batch    90: 12.138\n",
      "Loss after mini batch    90: 11.673\n",
      "Loss after mini batch    90: 11.633\n",
      "Loss after mini batch    90: 12.705\n",
      "Loss after mini batch    90: 12.076\n",
      "Loss after mini batch    90: 11.996\n",
      "Loss after mini batch    90: 12.460\n",
      "Loss after mini batch    90: 12.500\n",
      "Starting epoch 91\n",
      "Loss after mini batch    91: 13.311\n",
      "Loss after mini batch    91: 12.960\n",
      "Loss after mini batch    91: 12.292\n",
      "Loss after mini batch    91: 12.511\n",
      "Loss after mini batch    91: 11.344\n",
      "Loss after mini batch    91: 12.503\n",
      "Loss after mini batch    91: 11.955\n",
      "Loss after mini batch    91: 13.368\n",
      "Loss after mini batch    91: 13.717\n",
      "Loss after mini batch    91: 13.826\n",
      "Loss after mini batch    91: 12.076\n",
      "Starting epoch 92\n",
      "Loss after mini batch    92: 13.596\n",
      "Loss after mini batch    92: 12.094\n",
      "Loss after mini batch    92: 11.820\n",
      "Loss after mini batch    92: 11.468\n",
      "Loss after mini batch    92: 12.870\n",
      "Loss after mini batch    92: 13.180\n",
      "Loss after mini batch    92: 11.332\n",
      "Loss after mini batch    92: 11.504\n",
      "Loss after mini batch    92: 13.327\n",
      "Loss after mini batch    92: 13.214\n",
      "Loss after mini batch    92: 13.350\n",
      "Starting epoch 93\n",
      "Loss after mini batch    93: 11.955\n",
      "Loss after mini batch    93: 11.091\n",
      "Loss after mini batch    93: 13.418\n",
      "Loss after mini batch    93: 13.760\n",
      "Loss after mini batch    93: 11.990\n",
      "Loss after mini batch    93: 12.262\n",
      "Loss after mini batch    93: 11.377\n",
      "Loss after mini batch    93: 12.571\n",
      "Loss after mini batch    93: 12.868\n",
      "Loss after mini batch    93: 12.870\n",
      "Loss after mini batch    93: 11.918\n",
      "Starting epoch 94\n",
      "Loss after mini batch    94: 11.926\n",
      "Loss after mini batch    94: 12.501\n",
      "Loss after mini batch    94: 11.454\n",
      "Loss after mini batch    94: 12.503\n",
      "Loss after mini batch    94: 13.796\n",
      "Loss after mini batch    94: 13.303\n",
      "Loss after mini batch    94: 11.913\n",
      "Loss after mini batch    94: 12.821\n",
      "Loss after mini batch    94: 12.528\n",
      "Loss after mini batch    94: 12.134\n",
      "Loss after mini batch    94: 12.403\n",
      "Starting epoch 95\n",
      "Loss after mini batch    95: 12.099\n",
      "Loss after mini batch    95: 12.050\n",
      "Loss after mini batch    95: 12.753\n",
      "Loss after mini batch    95: 13.212\n",
      "Loss after mini batch    95: 11.814\n",
      "Loss after mini batch    95: 12.408\n",
      "Loss after mini batch    95: 11.809\n",
      "Loss after mini batch    95: 12.497\n",
      "Loss after mini batch    95: 13.230\n",
      "Loss after mini batch    95: 12.812\n",
      "Loss after mini batch    95: 11.686\n",
      "Starting epoch 96\n",
      "Loss after mini batch    96: 12.492\n",
      "Loss after mini batch    96: 11.631\n",
      "Loss after mini batch    96: 12.660\n",
      "Loss after mini batch    96: 11.880\n",
      "Loss after mini batch    96: 13.077\n",
      "Loss after mini batch    96: 12.413\n",
      "Loss after mini batch    96: 13.607\n",
      "Loss after mini batch    96: 11.899\n",
      "Loss after mini batch    96: 11.873\n",
      "Loss after mini batch    96: 13.896\n",
      "Loss after mini batch    96: 12.182\n",
      "Starting epoch 97\n",
      "Loss after mini batch    97: 12.720\n",
      "Loss after mini batch    97: 13.214\n",
      "Loss after mini batch    97: 11.073\n",
      "Loss after mini batch    97: 12.857\n",
      "Loss after mini batch    97: 12.462\n",
      "Loss after mini batch    97: 10.314\n",
      "Loss after mini batch    97: 12.207\n",
      "Loss after mini batch    97: 13.340\n",
      "Loss after mini batch    97: 11.898\n",
      "Loss after mini batch    97: 12.722\n",
      "Loss after mini batch    97: 13.164\n",
      "Starting epoch 98\n",
      "Loss after mini batch    98: 12.880\n",
      "Loss after mini batch    98: 13.236\n",
      "Loss after mini batch    98: 11.971\n",
      "Loss after mini batch    98: 12.768\n",
      "Loss after mini batch    98: 13.189\n",
      "Loss after mini batch    98: 12.552\n",
      "Loss after mini batch    98: 13.267\n",
      "Loss after mini batch    98: 11.830\n",
      "Loss after mini batch    98: 12.580\n",
      "Loss after mini batch    98: 10.926\n",
      "Loss after mini batch    98: 12.403\n",
      "Starting epoch 99\n",
      "Loss after mini batch    99: 12.283\n",
      "Loss after mini batch    99: 13.360\n",
      "Loss after mini batch    99: 11.723\n",
      "Loss after mini batch    99: 12.804\n",
      "Loss after mini batch    99: 13.030\n",
      "Loss after mini batch    99: 12.174\n",
      "Loss after mini batch    99: 11.375\n",
      "Loss after mini batch    99: 11.493\n",
      "Loss after mini batch    99: 12.770\n",
      "Loss after mini batch    99: 12.569\n",
      "Loss after mini batch    99: 12.211\n",
      "Starting epoch 100\n",
      "Loss after mini batch   100: 11.445\n",
      "Loss after mini batch   100: 12.911\n",
      "Loss after mini batch   100: 11.442\n",
      "Loss after mini batch   100: 11.824\n",
      "Loss after mini batch   100: 11.561\n",
      "Loss after mini batch   100: 13.414\n",
      "Loss after mini batch   100: 12.378\n",
      "Loss after mini batch   100: 12.764\n",
      "Loss after mini batch   100: 12.970\n",
      "Loss after mini batch   100: 13.709\n",
      "Loss after mini batch   100: 11.871\n",
      "Training Complete\n",
      "Starting Testing\n",
      "RMSE for fold 4: 3.3627152271238296\n",
      "rRMSE for fold 4: 0.066440030742429\n",
      "r for fold 4: 0.9901330919332638\n",
      "Fast RMSE for fold 4: 3.151579167364981\n",
      "Fast rRMSE for fold 4: 0.06016615752796617\n",
      "Fast r for fold 4: 0.9840545613779015\n",
      "Slow RMSE for fold 4: 4.239978868683754\n",
      "Slow rRMSE for fold 4: 0.08085825156283818\n",
      "Slow r for fold 4: 0.997707455437186\n",
      "Regular RMSE for fold 4: 2.6204123599109588\n",
      "Regular rRMSE for fold 4: 0.055525677692088736\n",
      "Regular r for fold 4: 0.9968723718481343\n",
      "Fold 5\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini batch     1: 391.949\n",
      "Loss after mini batch     1: 15.132\n",
      "Loss after mini batch     1: 15.403\n",
      "Loss after mini batch     1: 18.502\n",
      "Loss after mini batch     1: 18.208\n",
      "Loss after mini batch     1: 17.812\n",
      "Loss after mini batch     1: 29.694\n",
      "Loss after mini batch     1: 19.105\n",
      "Loss after mini batch     1: 18.275\n",
      "Loss after mini batch     1: 23.451\n",
      "Loss after mini batch     1: 19.387\n",
      "Starting epoch 2\n",
      "Loss after mini batch     2: 22.044\n",
      "Loss after mini batch     2: 25.221\n",
      "Loss after mini batch     2: 22.859\n",
      "Loss after mini batch     2: 20.384\n",
      "Loss after mini batch     2: 20.555\n",
      "Loss after mini batch     2: 23.484\n",
      "Loss after mini batch     2: 21.799\n",
      "Loss after mini batch     2: 18.430\n",
      "Loss after mini batch     2: 22.650\n",
      "Loss after mini batch     2: 17.825\n",
      "Loss after mini batch     2: 24.861\n",
      "Starting epoch 3\n",
      "Loss after mini batch     3: 20.555\n",
      "Loss after mini batch     3: 16.699\n",
      "Loss after mini batch     3: 17.021\n",
      "Loss after mini batch     3: 18.776\n",
      "Loss after mini batch     3: 17.899\n",
      "Loss after mini batch     3: 19.967\n",
      "Loss after mini batch     3: 21.799\n",
      "Loss after mini batch     3: 21.825\n",
      "Loss after mini batch     3: 15.852\n",
      "Loss after mini batch     3: 18.123\n",
      "Loss after mini batch     3: 16.795\n",
      "Starting epoch 4\n",
      "Loss after mini batch     4: 19.754\n",
      "Loss after mini batch     4: 16.104\n",
      "Loss after mini batch     4: 22.275\n",
      "Loss after mini batch     4: 18.200\n",
      "Loss after mini batch     4: 17.049\n",
      "Loss after mini batch     4: 17.912\n",
      "Loss after mini batch     4: 14.894\n",
      "Loss after mini batch     4: 16.511\n",
      "Loss after mini batch     4: 21.085\n",
      "Loss after mini batch     4: 17.594\n",
      "Loss after mini batch     4: 14.842\n",
      "Starting epoch 5\n",
      "Loss after mini batch     5: 17.601\n",
      "Loss after mini batch     5: 14.567\n",
      "Loss after mini batch     5: 16.342\n",
      "Loss after mini batch     5: 17.963\n",
      "Loss after mini batch     5: 21.503\n",
      "Loss after mini batch     5: 17.738\n",
      "Loss after mini batch     5: 27.269\n",
      "Loss after mini batch     5: 18.997\n",
      "Loss after mini batch     5: 16.126\n",
      "Loss after mini batch     5: 14.009\n",
      "Loss after mini batch     5: 15.405\n",
      "Starting epoch 6\n",
      "Loss after mini batch     6: 15.882\n",
      "Loss after mini batch     6: 15.203\n",
      "Loss after mini batch     6: 13.040\n",
      "Loss after mini batch     6: 19.167\n",
      "Loss after mini batch     6: 16.165\n",
      "Loss after mini batch     6: 13.803\n",
      "Loss after mini batch     6: 13.912\n",
      "Loss after mini batch     6: 13.795\n",
      "Loss after mini batch     6: 13.248\n",
      "Loss after mini batch     6: 15.010\n",
      "Loss after mini batch     6: 16.089\n",
      "Starting epoch 7\n",
      "Loss after mini batch     7: 13.995\n",
      "Loss after mini batch     7: 15.599\n",
      "Loss after mini batch     7: 15.047\n",
      "Loss after mini batch     7: 14.971\n",
      "Loss after mini batch     7: 16.778\n",
      "Loss after mini batch     7: 16.797\n",
      "Loss after mini batch     7: 13.089\n",
      "Loss after mini batch     7: 18.210\n",
      "Loss after mini batch     7: 14.254\n",
      "Loss after mini batch     7: 13.363\n",
      "Loss after mini batch     7: 20.316\n",
      "Starting epoch 8\n",
      "Loss after mini batch     8: 13.220\n",
      "Loss after mini batch     8: 16.946\n",
      "Loss after mini batch     8: 15.891\n",
      "Loss after mini batch     8: 14.314\n",
      "Loss after mini batch     8: 15.281\n",
      "Loss after mini batch     8: 14.390\n",
      "Loss after mini batch     8: 15.661\n",
      "Loss after mini batch     8: 15.782\n",
      "Loss after mini batch     8: 13.732\n",
      "Loss after mini batch     8: 17.320\n",
      "Loss after mini batch     8: 15.343\n",
      "Starting epoch 9\n",
      "Loss after mini batch     9: 15.029\n",
      "Loss after mini batch     9: 15.236\n",
      "Loss after mini batch     9: 18.158\n",
      "Loss after mini batch     9: 18.507\n",
      "Loss after mini batch     9: 14.382\n",
      "Loss after mini batch     9: 14.609\n",
      "Loss after mini batch     9: 15.065\n",
      "Loss after mini batch     9: 12.969\n",
      "Loss after mini batch     9: 14.511\n",
      "Loss after mini batch     9: 16.012\n",
      "Loss after mini batch     9: 15.443\n",
      "Starting epoch 10\n",
      "Loss after mini batch    10: 14.802\n",
      "Loss after mini batch    10: 13.765\n",
      "Loss after mini batch    10: 14.225\n",
      "Loss after mini batch    10: 15.263\n",
      "Loss after mini batch    10: 14.717\n",
      "Loss after mini batch    10: 17.626\n",
      "Loss after mini batch    10: 18.035\n",
      "Loss after mini batch    10: 15.981\n",
      "Loss after mini batch    10: 13.635\n",
      "Loss after mini batch    10: 14.865\n",
      "Loss after mini batch    10: 14.797\n",
      "Starting epoch 11\n",
      "Loss after mini batch    11: 15.056\n",
      "Loss after mini batch    11: 15.539\n",
      "Loss after mini batch    11: 12.671\n",
      "Loss after mini batch    11: 13.308\n",
      "Loss after mini batch    11: 14.541\n",
      "Loss after mini batch    11: 18.165\n",
      "Loss after mini batch    11: 12.829\n",
      "Loss after mini batch    11: 17.570\n",
      "Loss after mini batch    11: 14.068\n",
      "Loss after mini batch    11: 12.285\n",
      "Loss after mini batch    11: 14.162\n",
      "Starting epoch 12\n",
      "Loss after mini batch    12: 12.996\n",
      "Loss after mini batch    12: 14.230\n",
      "Loss after mini batch    12: 13.248\n",
      "Loss after mini batch    12: 13.945\n",
      "Loss after mini batch    12: 13.116\n",
      "Loss after mini batch    12: 13.992\n",
      "Loss after mini batch    12: 16.455\n",
      "Loss after mini batch    12: 13.735\n",
      "Loss after mini batch    12: 15.815\n",
      "Loss after mini batch    12: 16.184\n",
      "Loss after mini batch    12: 14.342\n",
      "Starting epoch 13\n",
      "Loss after mini batch    13: 15.281\n",
      "Loss after mini batch    13: 13.210\n",
      "Loss after mini batch    13: 14.717\n",
      "Loss after mini batch    13: 15.961\n",
      "Loss after mini batch    13: 14.403\n",
      "Loss after mini batch    13: 14.401\n",
      "Loss after mini batch    13: 13.979\n",
      "Loss after mini batch    13: 13.301\n",
      "Loss after mini batch    13: 15.875\n",
      "Loss after mini batch    13: 16.286\n",
      "Loss after mini batch    13: 15.034\n",
      "Starting epoch 14\n",
      "Loss after mini batch    14: 15.214\n",
      "Loss after mini batch    14: 13.612\n",
      "Loss after mini batch    14: 14.618\n",
      "Loss after mini batch    14: 13.782\n",
      "Loss after mini batch    14: 13.983\n",
      "Loss after mini batch    14: 13.038\n",
      "Loss after mini batch    14: 14.563\n",
      "Loss after mini batch    14: 12.042\n",
      "Loss after mini batch    14: 15.406\n",
      "Loss after mini batch    14: 13.366\n",
      "Loss after mini batch    14: 14.664\n",
      "Starting epoch 15\n",
      "Loss after mini batch    15: 13.913\n",
      "Loss after mini batch    15: 13.880\n",
      "Loss after mini batch    15: 16.115\n",
      "Loss after mini batch    15: 13.147\n",
      "Loss after mini batch    15: 14.749\n",
      "Loss after mini batch    15: 14.706\n",
      "Loss after mini batch    15: 12.739\n",
      "Loss after mini batch    15: 12.438\n",
      "Loss after mini batch    15: 13.825\n",
      "Loss after mini batch    15: 13.417\n",
      "Loss after mini batch    15: 15.019\n",
      "Starting epoch 16\n",
      "Loss after mini batch    16: 11.786\n",
      "Loss after mini batch    16: 15.203\n",
      "Loss after mini batch    16: 15.118\n",
      "Loss after mini batch    16: 11.726\n",
      "Loss after mini batch    16: 12.959\n",
      "Loss after mini batch    16: 13.303\n",
      "Loss after mini batch    16: 13.972\n",
      "Loss after mini batch    16: 14.565\n",
      "Loss after mini batch    16: 14.637\n",
      "Loss after mini batch    16: 15.114\n",
      "Loss after mini batch    16: 15.046\n",
      "Starting epoch 17\n",
      "Loss after mini batch    17: 14.698\n",
      "Loss after mini batch    17: 12.764\n",
      "Loss after mini batch    17: 15.721\n",
      "Loss after mini batch    17: 13.398\n",
      "Loss after mini batch    17: 15.642\n",
      "Loss after mini batch    17: 14.901\n",
      "Loss after mini batch    17: 14.952\n",
      "Loss after mini batch    17: 15.125\n",
      "Loss after mini batch    17: 14.058\n",
      "Loss after mini batch    17: 12.243\n",
      "Loss after mini batch    17: 12.585\n",
      "Starting epoch 18\n",
      "Loss after mini batch    18: 15.099\n",
      "Loss after mini batch    18: 14.118\n",
      "Loss after mini batch    18: 12.936\n",
      "Loss after mini batch    18: 15.522\n",
      "Loss after mini batch    18: 13.819\n",
      "Loss after mini batch    18: 15.853\n",
      "Loss after mini batch    18: 13.803\n",
      "Loss after mini batch    18: 15.389\n",
      "Loss after mini batch    18: 15.440\n",
      "Loss after mini batch    18: 12.483\n",
      "Loss after mini batch    18: 12.772\n",
      "Starting epoch 19\n",
      "Loss after mini batch    19: 13.523\n",
      "Loss after mini batch    19: 16.859\n",
      "Loss after mini batch    19: 14.557\n",
      "Loss after mini batch    19: 12.803\n",
      "Loss after mini batch    19: 13.854\n",
      "Loss after mini batch    19: 13.474\n",
      "Loss after mini batch    19: 14.103\n",
      "Loss after mini batch    19: 13.309\n",
      "Loss after mini batch    19: 13.083\n",
      "Loss after mini batch    19: 13.782\n",
      "Loss after mini batch    19: 15.806\n",
      "Starting epoch 20\n",
      "Loss after mini batch    20: 13.493\n",
      "Loss after mini batch    20: 12.742\n",
      "Loss after mini batch    20: 13.315\n",
      "Loss after mini batch    20: 14.619\n",
      "Loss after mini batch    20: 14.097\n",
      "Loss after mini batch    20: 13.961\n",
      "Loss after mini batch    20: 15.567\n",
      "Loss after mini batch    20: 14.954\n",
      "Loss after mini batch    20: 16.777\n",
      "Loss after mini batch    20: 12.451\n",
      "Loss after mini batch    20: 13.615\n",
      "Starting epoch 21\n",
      "Loss after mini batch    21: 14.018\n",
      "Loss after mini batch    21: 13.688\n",
      "Loss after mini batch    21: 12.607\n",
      "Loss after mini batch    21: 13.283\n",
      "Loss after mini batch    21: 13.174\n",
      "Loss after mini batch    21: 13.674\n",
      "Loss after mini batch    21: 13.403\n",
      "Loss after mini batch    21: 13.980\n",
      "Loss after mini batch    21: 13.260\n",
      "Loss after mini batch    21: 13.750\n",
      "Loss after mini batch    21: 13.645\n",
      "Starting epoch 22\n",
      "Loss after mini batch    22: 15.197\n",
      "Loss after mini batch    22: 11.714\n",
      "Loss after mini batch    22: 13.132\n",
      "Loss after mini batch    22: 15.665\n",
      "Loss after mini batch    22: 14.178\n",
      "Loss after mini batch    22: 12.761\n",
      "Loss after mini batch    22: 13.616\n",
      "Loss after mini batch    22: 13.788\n",
      "Loss after mini batch    22: 12.117\n",
      "Loss after mini batch    22: 12.989\n",
      "Loss after mini batch    22: 14.731\n",
      "Starting epoch 23\n",
      "Loss after mini batch    23: 13.181\n",
      "Loss after mini batch    23: 14.491\n",
      "Loss after mini batch    23: 15.137\n",
      "Loss after mini batch    23: 12.234\n",
      "Loss after mini batch    23: 13.792\n",
      "Loss after mini batch    23: 13.934\n",
      "Loss after mini batch    23: 12.792\n",
      "Loss after mini batch    23: 14.388\n",
      "Loss after mini batch    23: 15.561\n",
      "Loss after mini batch    23: 14.505\n",
      "Loss after mini batch    23: 14.832\n",
      "Starting epoch 24\n",
      "Loss after mini batch    24: 15.170\n",
      "Loss after mini batch    24: 12.757\n",
      "Loss after mini batch    24: 12.150\n",
      "Loss after mini batch    24: 14.258\n",
      "Loss after mini batch    24: 13.578\n",
      "Loss after mini batch    24: 14.553\n",
      "Loss after mini batch    24: 12.776\n",
      "Loss after mini batch    24: 15.741\n",
      "Loss after mini batch    24: 14.990\n",
      "Loss after mini batch    24: 14.270\n",
      "Loss after mini batch    24: 12.734\n",
      "Starting epoch 25\n",
      "Loss after mini batch    25: 15.235\n",
      "Loss after mini batch    25: 14.127\n",
      "Loss after mini batch    25: 13.499\n",
      "Loss after mini batch    25: 13.843\n",
      "Loss after mini batch    25: 13.023\n",
      "Loss after mini batch    25: 13.396\n",
      "Loss after mini batch    25: 14.135\n",
      "Loss after mini batch    25: 14.322\n",
      "Loss after mini batch    25: 13.074\n",
      "Loss after mini batch    25: 11.921\n",
      "Loss after mini batch    25: 13.515\n",
      "Starting epoch 26\n",
      "Loss after mini batch    26: 11.910\n",
      "Loss after mini batch    26: 13.648\n",
      "Loss after mini batch    26: 12.183\n",
      "Loss after mini batch    26: 14.041\n",
      "Loss after mini batch    26: 16.416\n",
      "Loss after mini batch    26: 15.074\n",
      "Loss after mini batch    26: 14.302\n",
      "Loss after mini batch    26: 13.493\n",
      "Loss after mini batch    26: 15.248\n",
      "Loss after mini batch    26: 12.812\n",
      "Loss after mini batch    26: 15.360\n",
      "Starting epoch 27\n",
      "Loss after mini batch    27: 14.073\n",
      "Loss after mini batch    27: 13.366\n",
      "Loss after mini batch    27: 12.424\n",
      "Loss after mini batch    27: 16.440\n",
      "Loss after mini batch    27: 15.772\n",
      "Loss after mini batch    27: 13.433\n",
      "Loss after mini batch    27: 12.810\n",
      "Loss after mini batch    27: 12.823\n",
      "Loss after mini batch    27: 15.264\n",
      "Loss after mini batch    27: 13.027\n",
      "Loss after mini batch    27: 13.730\n",
      "Starting epoch 28\n",
      "Loss after mini batch    28: 14.054\n",
      "Loss after mini batch    28: 15.041\n",
      "Loss after mini batch    28: 12.236\n",
      "Loss after mini batch    28: 13.781\n",
      "Loss after mini batch    28: 13.056\n",
      "Loss after mini batch    28: 14.377\n",
      "Loss after mini batch    28: 12.754\n",
      "Loss after mini batch    28: 14.568\n",
      "Loss after mini batch    28: 13.375\n",
      "Loss after mini batch    28: 14.798\n",
      "Loss after mini batch    28: 13.227\n",
      "Starting epoch 29\n",
      "Loss after mini batch    29: 13.516\n",
      "Loss after mini batch    29: 14.371\n",
      "Loss after mini batch    29: 13.896\n",
      "Loss after mini batch    29: 13.879\n",
      "Loss after mini batch    29: 12.574\n",
      "Loss after mini batch    29: 14.905\n",
      "Loss after mini batch    29: 13.781\n",
      "Loss after mini batch    29: 13.477\n",
      "Loss after mini batch    29: 14.846\n",
      "Loss after mini batch    29: 13.319\n",
      "Loss after mini batch    29: 12.989\n",
      "Starting epoch 30\n",
      "Loss after mini batch    30: 12.429\n",
      "Loss after mini batch    30: 12.237\n",
      "Loss after mini batch    30: 13.885\n",
      "Loss after mini batch    30: 15.753\n",
      "Loss after mini batch    30: 14.329\n",
      "Loss after mini batch    30: 12.947\n",
      "Loss after mini batch    30: 13.368\n",
      "Loss after mini batch    30: 13.795\n",
      "Loss after mini batch    30: 12.561\n",
      "Loss after mini batch    30: 12.376\n",
      "Loss after mini batch    30: 15.337\n",
      "Starting epoch 31\n",
      "Loss after mini batch    31: 15.043\n",
      "Loss after mini batch    31: 13.041\n",
      "Loss after mini batch    31: 13.162\n",
      "Loss after mini batch    31: 12.129\n",
      "Loss after mini batch    31: 13.499\n",
      "Loss after mini batch    31: 13.581\n",
      "Loss after mini batch    31: 14.611\n",
      "Loss after mini batch    31: 12.974\n",
      "Loss after mini batch    31: 13.147\n",
      "Loss after mini batch    31: 12.538\n",
      "Loss after mini batch    31: 14.017\n",
      "Starting epoch 32\n",
      "Loss after mini batch    32: 13.526\n",
      "Loss after mini batch    32: 13.815\n",
      "Loss after mini batch    32: 14.003\n",
      "Loss after mini batch    32: 13.847\n",
      "Loss after mini batch    32: 12.114\n",
      "Loss after mini batch    32: 13.910\n",
      "Loss after mini batch    32: 13.171\n",
      "Loss after mini batch    32: 12.362\n",
      "Loss after mini batch    32: 12.487\n",
      "Loss after mini batch    32: 13.529\n",
      "Loss after mini batch    32: 14.811\n",
      "Starting epoch 33\n",
      "Loss after mini batch    33: 14.637\n",
      "Loss after mini batch    33: 11.727\n",
      "Loss after mini batch    33: 12.562\n",
      "Loss after mini batch    33: 13.132\n",
      "Loss after mini batch    33: 12.096\n",
      "Loss after mini batch    33: 14.349\n",
      "Loss after mini batch    33: 13.884\n",
      "Loss after mini batch    33: 13.376\n",
      "Loss after mini batch    33: 13.277\n",
      "Loss after mini batch    33: 11.696\n",
      "Loss after mini batch    33: 13.873\n",
      "Starting epoch 34\n",
      "Loss after mini batch    34: 12.494\n",
      "Loss after mini batch    34: 13.431\n",
      "Loss after mini batch    34: 14.390\n",
      "Loss after mini batch    34: 13.831\n",
      "Loss after mini batch    34: 13.464\n",
      "Loss after mini batch    34: 11.689\n",
      "Loss after mini batch    34: 13.136\n",
      "Loss after mini batch    34: 13.127\n",
      "Loss after mini batch    34: 14.852\n",
      "Loss after mini batch    34: 11.864\n",
      "Loss after mini batch    34: 11.656\n",
      "Starting epoch 35\n",
      "Loss after mini batch    35: 14.558\n",
      "Loss after mini batch    35: 14.015\n",
      "Loss after mini batch    35: 13.361\n",
      "Loss after mini batch    35: 13.140\n",
      "Loss after mini batch    35: 13.473\n",
      "Loss after mini batch    35: 13.383\n",
      "Loss after mini batch    35: 12.595\n",
      "Loss after mini batch    35: 13.393\n",
      "Loss after mini batch    35: 12.687\n",
      "Loss after mini batch    35: 11.446\n",
      "Loss after mini batch    35: 14.586\n",
      "Starting epoch 36\n",
      "Loss after mini batch    36: 13.712\n",
      "Loss after mini batch    36: 12.751\n",
      "Loss after mini batch    36: 12.583\n",
      "Loss after mini batch    36: 13.148\n",
      "Loss after mini batch    36: 12.397\n",
      "Loss after mini batch    36: 13.952\n",
      "Loss after mini batch    36: 14.504\n",
      "Loss after mini batch    36: 11.620\n",
      "Loss after mini batch    36: 13.935\n",
      "Loss after mini batch    36: 15.008\n",
      "Loss after mini batch    36: 12.571\n",
      "Starting epoch 37\n",
      "Loss after mini batch    37: 12.828\n",
      "Loss after mini batch    37: 14.079\n",
      "Loss after mini batch    37: 14.756\n",
      "Loss after mini batch    37: 13.612\n",
      "Loss after mini batch    37: 12.569\n",
      "Loss after mini batch    37: 12.414\n",
      "Loss after mini batch    37: 13.270\n",
      "Loss after mini batch    37: 14.355\n",
      "Loss after mini batch    37: 12.685\n",
      "Loss after mini batch    37: 14.111\n",
      "Loss after mini batch    37: 13.572\n",
      "Starting epoch 38\n",
      "Loss after mini batch    38: 12.873\n",
      "Loss after mini batch    38: 12.805\n",
      "Loss after mini batch    38: 12.288\n",
      "Loss after mini batch    38: 14.043\n",
      "Loss after mini batch    38: 13.521\n",
      "Loss after mini batch    38: 12.119\n",
      "Loss after mini batch    38: 13.969\n",
      "Loss after mini batch    38: 12.834\n",
      "Loss after mini batch    38: 14.073\n",
      "Loss after mini batch    38: 13.502\n",
      "Loss after mini batch    38: 12.458\n",
      "Starting epoch 39\n",
      "Loss after mini batch    39: 13.268\n",
      "Loss after mini batch    39: 13.730\n",
      "Loss after mini batch    39: 12.371\n",
      "Loss after mini batch    39: 14.672\n",
      "Loss after mini batch    39: 13.060\n",
      "Loss after mini batch    39: 13.748\n",
      "Loss after mini batch    39: 12.526\n",
      "Loss after mini batch    39: 12.738\n",
      "Loss after mini batch    39: 12.970\n",
      "Loss after mini batch    39: 13.305\n",
      "Loss after mini batch    39: 12.746\n",
      "Starting epoch 40\n",
      "Loss after mini batch    40: 13.751\n",
      "Loss after mini batch    40: 14.413\n",
      "Loss after mini batch    40: 12.785\n",
      "Loss after mini batch    40: 13.605\n",
      "Loss after mini batch    40: 13.258\n",
      "Loss after mini batch    40: 12.499\n",
      "Loss after mini batch    40: 11.375\n",
      "Loss after mini batch    40: 14.654\n",
      "Loss after mini batch    40: 13.163\n",
      "Loss after mini batch    40: 13.804\n",
      "Loss after mini batch    40: 13.842\n",
      "Starting epoch 41\n",
      "Loss after mini batch    41: 13.657\n",
      "Loss after mini batch    41: 13.341\n",
      "Loss after mini batch    41: 13.887\n",
      "Loss after mini batch    41: 12.494\n",
      "Loss after mini batch    41: 13.995\n",
      "Loss after mini batch    41: 13.466\n",
      "Loss after mini batch    41: 12.727\n",
      "Loss after mini batch    41: 13.574\n",
      "Loss after mini batch    41: 13.433\n",
      "Loss after mini batch    41: 14.947\n",
      "Loss after mini batch    41: 11.365\n",
      "Starting epoch 42\n",
      "Loss after mini batch    42: 13.920\n",
      "Loss after mini batch    42: 14.145\n",
      "Loss after mini batch    42: 11.943\n",
      "Loss after mini batch    42: 11.804\n",
      "Loss after mini batch    42: 13.634\n",
      "Loss after mini batch    42: 12.012\n",
      "Loss after mini batch    42: 12.607\n",
      "Loss after mini batch    42: 13.081\n",
      "Loss after mini batch    42: 14.415\n",
      "Loss after mini batch    42: 14.393\n",
      "Loss after mini batch    42: 13.099\n",
      "Starting epoch 43\n",
      "Loss after mini batch    43: 12.585\n",
      "Loss after mini batch    43: 15.698\n",
      "Loss after mini batch    43: 13.963\n",
      "Loss after mini batch    43: 13.240\n",
      "Loss after mini batch    43: 13.928\n",
      "Loss after mini batch    43: 12.186\n",
      "Loss after mini batch    43: 13.668\n",
      "Loss after mini batch    43: 12.928\n",
      "Loss after mini batch    43: 12.618\n",
      "Loss after mini batch    43: 12.856\n",
      "Loss after mini batch    43: 14.752\n",
      "Starting epoch 44\n",
      "Loss after mini batch    44: 13.835\n",
      "Loss after mini batch    44: 13.479\n",
      "Loss after mini batch    44: 13.646\n",
      "Loss after mini batch    44: 12.372\n",
      "Loss after mini batch    44: 15.196\n",
      "Loss after mini batch    44: 13.462\n",
      "Loss after mini batch    44: 12.913\n",
      "Loss after mini batch    44: 11.861\n",
      "Loss after mini batch    44: 14.420\n",
      "Loss after mini batch    44: 13.522\n",
      "Loss after mini batch    44: 11.898\n",
      "Starting epoch 45\n",
      "Loss after mini batch    45: 13.104\n",
      "Loss after mini batch    45: 13.081\n",
      "Loss after mini batch    45: 13.489\n",
      "Loss after mini batch    45: 11.787\n",
      "Loss after mini batch    45: 13.883\n",
      "Loss after mini batch    45: 13.358\n",
      "Loss after mini batch    45: 13.892\n",
      "Loss after mini batch    45: 13.813\n",
      "Loss after mini batch    45: 13.686\n",
      "Loss after mini batch    45: 13.134\n",
      "Loss after mini batch    45: 13.541\n",
      "Starting epoch 46\n",
      "Loss after mini batch    46: 12.263\n",
      "Loss after mini batch    46: 11.526\n",
      "Loss after mini batch    46: 12.902\n",
      "Loss after mini batch    46: 13.975\n",
      "Loss after mini batch    46: 14.431\n",
      "Loss after mini batch    46: 12.195\n",
      "Loss after mini batch    46: 12.809\n",
      "Loss after mini batch    46: 12.444\n",
      "Loss after mini batch    46: 13.380\n",
      "Loss after mini batch    46: 14.570\n",
      "Loss after mini batch    46: 13.932\n",
      "Starting epoch 47\n",
      "Loss after mini batch    47: 13.249\n",
      "Loss after mini batch    47: 12.866\n",
      "Loss after mini batch    47: 12.277\n",
      "Loss after mini batch    47: 12.123\n",
      "Loss after mini batch    47: 13.044\n",
      "Loss after mini batch    47: 14.137\n",
      "Loss after mini batch    47: 13.873\n",
      "Loss after mini batch    47: 13.215\n",
      "Loss after mini batch    47: 13.710\n",
      "Loss after mini batch    47: 12.079\n",
      "Loss after mini batch    47: 13.225\n",
      "Starting epoch 48\n",
      "Loss after mini batch    48: 12.164\n",
      "Loss after mini batch    48: 13.430\n",
      "Loss after mini batch    48: 12.080\n",
      "Loss after mini batch    48: 12.979\n",
      "Loss after mini batch    48: 14.830\n",
      "Loss after mini batch    48: 14.364\n",
      "Loss after mini batch    48: 11.701\n",
      "Loss after mini batch    48: 13.762\n",
      "Loss after mini batch    48: 13.266\n",
      "Loss after mini batch    48: 13.085\n",
      "Loss after mini batch    48: 12.327\n",
      "Starting epoch 49\n",
      "Loss after mini batch    49: 12.802\n",
      "Loss after mini batch    49: 13.337\n",
      "Loss after mini batch    49: 11.563\n",
      "Loss after mini batch    49: 12.091\n",
      "Loss after mini batch    49: 12.740\n",
      "Loss after mini batch    49: 13.419\n",
      "Loss after mini batch    49: 13.047\n",
      "Loss after mini batch    49: 12.877\n",
      "Loss after mini batch    49: 12.437\n",
      "Loss after mini batch    49: 13.218\n",
      "Loss after mini batch    49: 13.760\n",
      "Starting epoch 50\n",
      "Loss after mini batch    50: 13.073\n",
      "Loss after mini batch    50: 11.893\n",
      "Loss after mini batch    50: 12.886\n",
      "Loss after mini batch    50: 12.881\n",
      "Loss after mini batch    50: 15.777\n",
      "Loss after mini batch    50: 12.208\n",
      "Loss after mini batch    50: 13.043\n",
      "Loss after mini batch    50: 11.983\n",
      "Loss after mini batch    50: 13.892\n",
      "Loss after mini batch    50: 13.851\n",
      "Loss after mini batch    50: 13.524\n",
      "Starting epoch 51\n",
      "Loss after mini batch    51: 12.714\n",
      "Loss after mini batch    51: 13.379\n",
      "Loss after mini batch    51: 13.093\n",
      "Loss after mini batch    51: 14.113\n",
      "Loss after mini batch    51: 12.595\n",
      "Loss after mini batch    51: 12.240\n",
      "Loss after mini batch    51: 13.167\n",
      "Loss after mini batch    51: 12.795\n",
      "Loss after mini batch    51: 12.032\n",
      "Loss after mini batch    51: 12.949\n",
      "Loss after mini batch    51: 13.795\n",
      "Starting epoch 52\n",
      "Loss after mini batch    52: 12.744\n",
      "Loss after mini batch    52: 12.414\n",
      "Loss after mini batch    52: 13.589\n",
      "Loss after mini batch    52: 11.918\n",
      "Loss after mini batch    52: 13.788\n",
      "Loss after mini batch    52: 12.578\n",
      "Loss after mini batch    52: 13.013\n",
      "Loss after mini batch    52: 13.281\n",
      "Loss after mini batch    52: 13.099\n",
      "Loss after mini batch    52: 14.016\n",
      "Loss after mini batch    52: 12.054\n",
      "Starting epoch 53\n",
      "Loss after mini batch    53: 12.749\n",
      "Loss after mini batch    53: 13.093\n",
      "Loss after mini batch    53: 13.585\n",
      "Loss after mini batch    53: 11.989\n",
      "Loss after mini batch    53: 12.275\n",
      "Loss after mini batch    53: 14.008\n",
      "Loss after mini batch    53: 12.507\n",
      "Loss after mini batch    53: 12.166\n",
      "Loss after mini batch    53: 14.602\n",
      "Loss after mini batch    53: 12.334\n",
      "Loss after mini batch    53: 12.658\n",
      "Starting epoch 54\n",
      "Loss after mini batch    54: 12.606\n",
      "Loss after mini batch    54: 12.777\n",
      "Loss after mini batch    54: 12.733\n",
      "Loss after mini batch    54: 12.453\n",
      "Loss after mini batch    54: 12.467\n",
      "Loss after mini batch    54: 13.774\n",
      "Loss after mini batch    54: 11.704\n",
      "Loss after mini batch    54: 12.006\n",
      "Loss after mini batch    54: 13.750\n",
      "Loss after mini batch    54: 13.831\n",
      "Loss after mini batch    54: 13.153\n",
      "Starting epoch 55\n",
      "Loss after mini batch    55: 13.108\n",
      "Loss after mini batch    55: 15.093\n",
      "Loss after mini batch    55: 13.244\n",
      "Loss after mini batch    55: 12.674\n",
      "Loss after mini batch    55: 13.922\n",
      "Loss after mini batch    55: 12.861\n",
      "Loss after mini batch    55: 12.710\n",
      "Loss after mini batch    55: 12.333\n",
      "Loss after mini batch    55: 12.552\n",
      "Loss after mini batch    55: 13.878\n",
      "Loss after mini batch    55: 12.123\n",
      "Starting epoch 56\n",
      "Loss after mini batch    56: 12.421\n",
      "Loss after mini batch    56: 14.270\n",
      "Loss after mini batch    56: 13.103\n",
      "Loss after mini batch    56: 12.346\n",
      "Loss after mini batch    56: 13.611\n",
      "Loss after mini batch    56: 12.614\n",
      "Loss after mini batch    56: 14.153\n",
      "Loss after mini batch    56: 13.004\n",
      "Loss after mini batch    56: 14.530\n",
      "Loss after mini batch    56: 13.538\n",
      "Loss after mini batch    56: 12.798\n",
      "Starting epoch 57\n",
      "Loss after mini batch    57: 13.414\n",
      "Loss after mini batch    57: 13.613\n",
      "Loss after mini batch    57: 12.475\n",
      "Loss after mini batch    57: 12.360\n",
      "Loss after mini batch    57: 12.496\n",
      "Loss after mini batch    57: 12.518\n",
      "Loss after mini batch    57: 13.375\n",
      "Loss after mini batch    57: 13.164\n",
      "Loss after mini batch    57: 13.162\n",
      "Loss after mini batch    57: 12.608\n",
      "Loss after mini batch    57: 12.329\n",
      "Starting epoch 58\n",
      "Loss after mini batch    58: 12.914\n",
      "Loss after mini batch    58: 13.365\n",
      "Loss after mini batch    58: 14.239\n",
      "Loss after mini batch    58: 12.703\n",
      "Loss after mini batch    58: 13.221\n",
      "Loss after mini batch    58: 11.101\n",
      "Loss after mini batch    58: 12.313\n",
      "Loss after mini batch    58: 12.641\n",
      "Loss after mini batch    58: 13.363\n",
      "Loss after mini batch    58: 13.380\n",
      "Loss after mini batch    58: 12.196\n",
      "Starting epoch 59\n",
      "Loss after mini batch    59: 12.730\n",
      "Loss after mini batch    59: 12.868\n",
      "Loss after mini batch    59: 13.596\n",
      "Loss after mini batch    59: 11.614\n",
      "Loss after mini batch    59: 14.189\n",
      "Loss after mini batch    59: 11.281\n",
      "Loss after mini batch    59: 12.573\n",
      "Loss after mini batch    59: 13.372\n",
      "Loss after mini batch    59: 12.120\n",
      "Loss after mini batch    59: 12.893\n",
      "Loss after mini batch    59: 13.172\n",
      "Starting epoch 60\n",
      "Loss after mini batch    60: 12.784\n",
      "Loss after mini batch    60: 11.550\n",
      "Loss after mini batch    60: 12.394\n",
      "Loss after mini batch    60: 12.794\n",
      "Loss after mini batch    60: 12.522\n",
      "Loss after mini batch    60: 12.236\n",
      "Loss after mini batch    60: 12.962\n",
      "Loss after mini batch    60: 12.381\n",
      "Loss after mini batch    60: 12.217\n",
      "Loss after mini batch    60: 12.527\n",
      "Loss after mini batch    60: 13.187\n",
      "Starting epoch 61\n",
      "Loss after mini batch    61: 13.810\n",
      "Loss after mini batch    61: 13.249\n",
      "Loss after mini batch    61: 13.288\n",
      "Loss after mini batch    61: 11.542\n",
      "Loss after mini batch    61: 12.486\n",
      "Loss after mini batch    61: 13.690\n",
      "Loss after mini batch    61: 11.618\n",
      "Loss after mini batch    61: 13.499\n",
      "Loss after mini batch    61: 12.948\n",
      "Loss after mini batch    61: 13.388\n",
      "Loss after mini batch    61: 12.395\n",
      "Starting epoch 62\n",
      "Loss after mini batch    62: 13.669\n",
      "Loss after mini batch    62: 13.069\n",
      "Loss after mini batch    62: 11.653\n",
      "Loss after mini batch    62: 13.850\n",
      "Loss after mini batch    62: 13.541\n",
      "Loss after mini batch    62: 13.013\n",
      "Loss after mini batch    62: 12.653\n",
      "Loss after mini batch    62: 11.924\n",
      "Loss after mini batch    62: 12.528\n",
      "Loss after mini batch    62: 13.190\n",
      "Loss after mini batch    62: 13.001\n",
      "Starting epoch 63\n",
      "Loss after mini batch    63: 12.531\n",
      "Loss after mini batch    63: 13.437\n",
      "Loss after mini batch    63: 12.565\n",
      "Loss after mini batch    63: 11.286\n",
      "Loss after mini batch    63: 15.117\n",
      "Loss after mini batch    63: 14.026\n",
      "Loss after mini batch    63: 12.394\n",
      "Loss after mini batch    63: 11.530\n",
      "Loss after mini batch    63: 13.281\n",
      "Loss after mini batch    63: 13.481\n",
      "Loss after mini batch    63: 11.332\n",
      "Starting epoch 64\n",
      "Loss after mini batch    64: 14.403\n",
      "Loss after mini batch    64: 11.230\n",
      "Loss after mini batch    64: 13.603\n",
      "Loss after mini batch    64: 11.191\n",
      "Loss after mini batch    64: 12.080\n",
      "Loss after mini batch    64: 13.160\n",
      "Loss after mini batch    64: 13.512\n",
      "Loss after mini batch    64: 13.180\n",
      "Loss after mini batch    64: 13.636\n",
      "Loss after mini batch    64: 12.723\n",
      "Loss after mini batch    64: 12.284\n",
      "Starting epoch 65\n",
      "Loss after mini batch    65: 13.158\n",
      "Loss after mini batch    65: 13.348\n",
      "Loss after mini batch    65: 14.482\n",
      "Loss after mini batch    65: 12.897\n",
      "Loss after mini batch    65: 13.009\n",
      "Loss after mini batch    65: 13.681\n",
      "Loss after mini batch    65: 12.589\n",
      "Loss after mini batch    65: 14.142\n",
      "Loss after mini batch    65: 12.045\n",
      "Loss after mini batch    65: 12.097\n",
      "Loss after mini batch    65: 12.234\n",
      "Starting epoch 66\n",
      "Loss after mini batch    66: 12.326\n",
      "Loss after mini batch    66: 13.055\n",
      "Loss after mini batch    66: 13.685\n",
      "Loss after mini batch    66: 12.946\n",
      "Loss after mini batch    66: 12.564\n",
      "Loss after mini batch    66: 12.780\n",
      "Loss after mini batch    66: 13.010\n",
      "Loss after mini batch    66: 13.811\n",
      "Loss after mini batch    66: 12.092\n",
      "Loss after mini batch    66: 12.147\n",
      "Loss after mini batch    66: 12.593\n",
      "Starting epoch 67\n",
      "Loss after mini batch    67: 12.535\n",
      "Loss after mini batch    67: 13.478\n",
      "Loss after mini batch    67: 13.469\n",
      "Loss after mini batch    67: 12.483\n",
      "Loss after mini batch    67: 13.424\n",
      "Loss after mini batch    67: 12.407\n",
      "Loss after mini batch    67: 13.493\n",
      "Loss after mini batch    67: 12.582\n",
      "Loss after mini batch    67: 12.744\n",
      "Loss after mini batch    67: 11.768\n",
      "Loss after mini batch    67: 12.133\n",
      "Starting epoch 68\n",
      "Loss after mini batch    68: 12.980\n",
      "Loss after mini batch    68: 12.404\n",
      "Loss after mini batch    68: 12.548\n",
      "Loss after mini batch    68: 13.859\n",
      "Loss after mini batch    68: 12.458\n",
      "Loss after mini batch    68: 12.053\n",
      "Loss after mini batch    68: 13.043\n",
      "Loss after mini batch    68: 11.481\n",
      "Loss after mini batch    68: 13.836\n",
      "Loss after mini batch    68: 12.169\n",
      "Loss after mini batch    68: 12.696\n",
      "Starting epoch 69\n",
      "Loss after mini batch    69: 14.258\n",
      "Loss after mini batch    69: 12.172\n",
      "Loss after mini batch    69: 12.181\n",
      "Loss after mini batch    69: 13.939\n",
      "Loss after mini batch    69: 12.144\n",
      "Loss after mini batch    69: 12.568\n",
      "Loss after mini batch    69: 12.847\n",
      "Loss after mini batch    69: 12.133\n",
      "Loss after mini batch    69: 12.588\n",
      "Loss after mini batch    69: 11.363\n",
      "Loss after mini batch    69: 12.794\n",
      "Starting epoch 70\n",
      "Loss after mini batch    70: 12.643\n",
      "Loss after mini batch    70: 11.000\n",
      "Loss after mini batch    70: 12.756\n",
      "Loss after mini batch    70: 12.295\n",
      "Loss after mini batch    70: 13.652\n",
      "Loss after mini batch    70: 12.538\n",
      "Loss after mini batch    70: 12.585\n",
      "Loss after mini batch    70: 13.361\n",
      "Loss after mini batch    70: 13.175\n",
      "Loss after mini batch    70: 12.574\n",
      "Loss after mini batch    70: 11.389\n",
      "Starting epoch 71\n",
      "Loss after mini batch    71: 13.747\n",
      "Loss after mini batch    71: 12.830\n",
      "Loss after mini batch    71: 11.535\n",
      "Loss after mini batch    71: 11.947\n",
      "Loss after mini batch    71: 12.742\n",
      "Loss after mini batch    71: 12.728\n",
      "Loss after mini batch    71: 12.744\n",
      "Loss after mini batch    71: 13.004\n",
      "Loss after mini batch    71: 11.071\n",
      "Loss after mini batch    71: 12.852\n",
      "Loss after mini batch    71: 13.060\n",
      "Starting epoch 72\n",
      "Loss after mini batch    72: 12.281\n",
      "Loss after mini batch    72: 12.793\n",
      "Loss after mini batch    72: 12.245\n",
      "Loss after mini batch    72: 11.799\n",
      "Loss after mini batch    72: 12.707\n",
      "Loss after mini batch    72: 14.315\n",
      "Loss after mini batch    72: 13.213\n",
      "Loss after mini batch    72: 13.053\n",
      "Loss after mini batch    72: 11.386\n",
      "Loss after mini batch    72: 13.643\n",
      "Loss after mini batch    72: 12.747\n",
      "Starting epoch 73\n",
      "Loss after mini batch    73: 12.019\n",
      "Loss after mini batch    73: 13.189\n",
      "Loss after mini batch    73: 12.130\n",
      "Loss after mini batch    73: 13.085\n",
      "Loss after mini batch    73: 11.973\n",
      "Loss after mini batch    73: 13.005\n",
      "Loss after mini batch    73: 12.209\n",
      "Loss after mini batch    73: 11.963\n",
      "Loss after mini batch    73: 12.334\n",
      "Loss after mini batch    73: 11.741\n",
      "Loss after mini batch    73: 12.583\n",
      "Starting epoch 74\n",
      "Loss after mini batch    74: 12.644\n",
      "Loss after mini batch    74: 13.021\n",
      "Loss after mini batch    74: 12.724\n",
      "Loss after mini batch    74: 12.828\n",
      "Loss after mini batch    74: 12.963\n",
      "Loss after mini batch    74: 14.612\n",
      "Loss after mini batch    74: 12.509\n",
      "Loss after mini batch    74: 12.830\n",
      "Loss after mini batch    74: 13.646\n",
      "Loss after mini batch    74: 10.773\n",
      "Loss after mini batch    74: 12.605\n",
      "Starting epoch 75\n",
      "Loss after mini batch    75: 12.106\n",
      "Loss after mini batch    75: 12.161\n",
      "Loss after mini batch    75: 11.797\n",
      "Loss after mini batch    75: 12.895\n",
      "Loss after mini batch    75: 11.109\n",
      "Loss after mini batch    75: 12.652\n",
      "Loss after mini batch    75: 12.485\n",
      "Loss after mini batch    75: 12.172\n",
      "Loss after mini batch    75: 12.334\n",
      "Loss after mini batch    75: 12.430\n",
      "Loss after mini batch    75: 12.814\n",
      "Starting epoch 76\n",
      "Loss after mini batch    76: 12.246\n",
      "Loss after mini batch    76: 12.182\n",
      "Loss after mini batch    76: 12.648\n",
      "Loss after mini batch    76: 12.157\n",
      "Loss after mini batch    76: 12.245\n",
      "Loss after mini batch    76: 12.528\n",
      "Loss after mini batch    76: 11.467\n",
      "Loss after mini batch    76: 12.274\n",
      "Loss after mini batch    76: 13.472\n",
      "Loss after mini batch    76: 12.530\n",
      "Loss after mini batch    76: 12.814\n",
      "Starting epoch 77\n",
      "Loss after mini batch    77: 12.571\n",
      "Loss after mini batch    77: 11.493\n",
      "Loss after mini batch    77: 12.565\n",
      "Loss after mini batch    77: 12.258\n",
      "Loss after mini batch    77: 13.257\n",
      "Loss after mini batch    77: 12.493\n",
      "Loss after mini batch    77: 13.630\n",
      "Loss after mini batch    77: 12.154\n",
      "Loss after mini batch    77: 12.779\n",
      "Loss after mini batch    77: 11.666\n",
      "Loss after mini batch    77: 12.716\n",
      "Starting epoch 78\n",
      "Loss after mini batch    78: 12.903\n",
      "Loss after mini batch    78: 12.217\n",
      "Loss after mini batch    78: 12.457\n",
      "Loss after mini batch    78: 13.048\n",
      "Loss after mini batch    78: 11.748\n",
      "Loss after mini batch    78: 12.179\n",
      "Loss after mini batch    78: 11.849\n",
      "Loss after mini batch    78: 12.792\n",
      "Loss after mini batch    78: 12.201\n",
      "Loss after mini batch    78: 12.338\n",
      "Loss after mini batch    78: 13.996\n",
      "Starting epoch 79\n",
      "Loss after mini batch    79: 13.900\n",
      "Loss after mini batch    79: 12.710\n",
      "Loss after mini batch    79: 12.057\n",
      "Loss after mini batch    79: 12.701\n",
      "Loss after mini batch    79: 12.634\n",
      "Loss after mini batch    79: 12.026\n",
      "Loss after mini batch    79: 13.886\n",
      "Loss after mini batch    79: 11.464\n",
      "Loss after mini batch    79: 12.407\n",
      "Loss after mini batch    79: 11.817\n",
      "Loss after mini batch    79: 12.094\n",
      "Starting epoch 80\n",
      "Loss after mini batch    80: 12.380\n",
      "Loss after mini batch    80: 11.000\n",
      "Loss after mini batch    80: 11.834\n",
      "Loss after mini batch    80: 13.659\n",
      "Loss after mini batch    80: 12.399\n",
      "Loss after mini batch    80: 12.484\n",
      "Loss after mini batch    80: 13.509\n",
      "Loss after mini batch    80: 13.931\n",
      "Loss after mini batch    80: 11.983\n",
      "Loss after mini batch    80: 11.816\n",
      "Loss after mini batch    80: 13.054\n",
      "Starting epoch 81\n",
      "Loss after mini batch    81: 14.370\n",
      "Loss after mini batch    81: 12.935\n",
      "Loss after mini batch    81: 13.361\n",
      "Loss after mini batch    81: 12.480\n",
      "Loss after mini batch    81: 12.606\n",
      "Loss after mini batch    81: 12.304\n",
      "Loss after mini batch    81: 11.939\n",
      "Loss after mini batch    81: 11.580\n",
      "Loss after mini batch    81: 11.567\n",
      "Loss after mini batch    81: 14.472\n",
      "Loss after mini batch    81: 13.327\n",
      "Starting epoch 82\n",
      "Loss after mini batch    82: 12.291\n",
      "Loss after mini batch    82: 12.699\n",
      "Loss after mini batch    82: 11.629\n",
      "Loss after mini batch    82: 12.643\n",
      "Loss after mini batch    82: 13.698\n",
      "Loss after mini batch    82: 12.505\n",
      "Loss after mini batch    82: 12.977\n",
      "Loss after mini batch    82: 12.091\n",
      "Loss after mini batch    82: 11.883\n",
      "Loss after mini batch    82: 11.452\n",
      "Loss after mini batch    82: 12.423\n",
      "Starting epoch 83\n",
      "Loss after mini batch    83: 12.766\n",
      "Loss after mini batch    83: 12.583\n",
      "Loss after mini batch    83: 12.748\n",
      "Loss after mini batch    83: 11.789\n",
      "Loss after mini batch    83: 11.776\n",
      "Loss after mini batch    83: 12.922\n",
      "Loss after mini batch    83: 11.689\n",
      "Loss after mini batch    83: 11.505\n",
      "Loss after mini batch    83: 12.721\n",
      "Loss after mini batch    83: 13.092\n",
      "Loss after mini batch    83: 12.289\n",
      "Starting epoch 84\n",
      "Loss after mini batch    84: 12.463\n",
      "Loss after mini batch    84: 12.630\n",
      "Loss after mini batch    84: 13.372\n",
      "Loss after mini batch    84: 12.252\n",
      "Loss after mini batch    84: 12.406\n",
      "Loss after mini batch    84: 11.947\n",
      "Loss after mini batch    84: 12.012\n",
      "Loss after mini batch    84: 12.333\n",
      "Loss after mini batch    84: 14.559\n",
      "Loss after mini batch    84: 11.910\n",
      "Loss after mini batch    84: 12.294\n",
      "Starting epoch 85\n",
      "Loss after mini batch    85: 12.994\n",
      "Loss after mini batch    85: 12.071\n",
      "Loss after mini batch    85: 12.952\n",
      "Loss after mini batch    85: 14.253\n",
      "Loss after mini batch    85: 12.427\n",
      "Loss after mini batch    85: 13.348\n",
      "Loss after mini batch    85: 12.418\n",
      "Loss after mini batch    85: 12.523\n",
      "Loss after mini batch    85: 12.599\n",
      "Loss after mini batch    85: 12.821\n",
      "Loss after mini batch    85: 11.795\n",
      "Starting epoch 86\n",
      "Loss after mini batch    86: 13.203\n",
      "Loss after mini batch    86: 11.382\n",
      "Loss after mini batch    86: 13.083\n",
      "Loss after mini batch    86: 12.911\n",
      "Loss after mini batch    86: 13.281\n",
      "Loss after mini batch    86: 12.736\n",
      "Loss after mini batch    86: 13.090\n",
      "Loss after mini batch    86: 12.257\n",
      "Loss after mini batch    86: 11.739\n",
      "Loss after mini batch    86: 12.981\n",
      "Loss after mini batch    86: 11.746\n",
      "Starting epoch 87\n",
      "Loss after mini batch    87: 11.897\n",
      "Loss after mini batch    87: 12.724\n",
      "Loss after mini batch    87: 12.362\n",
      "Loss after mini batch    87: 12.585\n",
      "Loss after mini batch    87: 10.215\n",
      "Loss after mini batch    87: 14.359\n",
      "Loss after mini batch    87: 12.178\n",
      "Loss after mini batch    87: 12.639\n",
      "Loss after mini batch    87: 12.063\n",
      "Loss after mini batch    87: 13.119\n",
      "Loss after mini batch    87: 12.712\n",
      "Starting epoch 88\n",
      "Loss after mini batch    88: 10.123\n",
      "Loss after mini batch    88: 13.100\n",
      "Loss after mini batch    88: 11.929\n",
      "Loss after mini batch    88: 11.383\n",
      "Loss after mini batch    88: 13.542\n",
      "Loss after mini batch    88: 11.258\n",
      "Loss after mini batch    88: 12.297\n",
      "Loss after mini batch    88: 12.725\n",
      "Loss after mini batch    88: 13.259\n",
      "Loss after mini batch    88: 12.783\n",
      "Loss after mini batch    88: 12.731\n",
      "Starting epoch 89\n",
      "Loss after mini batch    89: 12.346\n",
      "Loss after mini batch    89: 12.482\n",
      "Loss after mini batch    89: 12.579\n",
      "Loss after mini batch    89: 11.375\n",
      "Loss after mini batch    89: 12.075\n",
      "Loss after mini batch    89: 11.830\n",
      "Loss after mini batch    89: 12.937\n",
      "Loss after mini batch    89: 11.758\n",
      "Loss after mini batch    89: 12.659\n",
      "Loss after mini batch    89: 13.543\n",
      "Loss after mini batch    89: 13.499\n",
      "Starting epoch 90\n",
      "Loss after mini batch    90: 11.162\n",
      "Loss after mini batch    90: 13.086\n",
      "Loss after mini batch    90: 11.888\n",
      "Loss after mini batch    90: 12.441\n",
      "Loss after mini batch    90: 11.432\n",
      "Loss after mini batch    90: 12.784\n",
      "Loss after mini batch    90: 12.725\n",
      "Loss after mini batch    90: 13.194\n",
      "Loss after mini batch    90: 11.504\n",
      "Loss after mini batch    90: 12.706\n",
      "Loss after mini batch    90: 13.379\n",
      "Starting epoch 91\n",
      "Loss after mini batch    91: 11.552\n",
      "Loss after mini batch    91: 11.867\n",
      "Loss after mini batch    91: 11.514\n",
      "Loss after mini batch    91: 11.837\n",
      "Loss after mini batch    91: 13.873\n",
      "Loss after mini batch    91: 12.296\n",
      "Loss after mini batch    91: 12.932\n",
      "Loss after mini batch    91: 13.617\n",
      "Loss after mini batch    91: 12.045\n",
      "Loss after mini batch    91: 11.726\n",
      "Loss after mini batch    91: 11.861\n",
      "Starting epoch 92\n",
      "Loss after mini batch    92: 12.479\n",
      "Loss after mini batch    92: 12.283\n",
      "Loss after mini batch    92: 12.520\n",
      "Loss after mini batch    92: 13.662\n",
      "Loss after mini batch    92: 12.974\n",
      "Loss after mini batch    92: 12.826\n",
      "Loss after mini batch    92: 12.343\n",
      "Loss after mini batch    92: 13.078\n",
      "Loss after mini batch    92: 11.961\n",
      "Loss after mini batch    92: 12.281\n",
      "Loss after mini batch    92: 12.455\n",
      "Starting epoch 93\n",
      "Loss after mini batch    93: 11.956\n",
      "Loss after mini batch    93: 12.153\n",
      "Loss after mini batch    93: 13.064\n",
      "Loss after mini batch    93: 11.696\n",
      "Loss after mini batch    93: 10.626\n",
      "Loss after mini batch    93: 13.031\n",
      "Loss after mini batch    93: 12.015\n",
      "Loss after mini batch    93: 11.695\n",
      "Loss after mini batch    93: 13.240\n",
      "Loss after mini batch    93: 12.944\n",
      "Loss after mini batch    93: 12.562\n",
      "Starting epoch 94\n",
      "Loss after mini batch    94: 11.069\n",
      "Loss after mini batch    94: 11.714\n",
      "Loss after mini batch    94: 12.070\n",
      "Loss after mini batch    94: 12.792\n",
      "Loss after mini batch    94: 12.317\n",
      "Loss after mini batch    94: 12.141\n",
      "Loss after mini batch    94: 11.990\n",
      "Loss after mini batch    94: 12.639\n",
      "Loss after mini batch    94: 11.966\n",
      "Loss after mini batch    94: 13.268\n",
      "Loss after mini batch    94: 12.839\n",
      "Starting epoch 95\n",
      "Loss after mini batch    95: 13.313\n",
      "Loss after mini batch    95: 11.340\n",
      "Loss after mini batch    95: 13.788\n",
      "Loss after mini batch    95: 11.913\n",
      "Loss after mini batch    95: 13.152\n",
      "Loss after mini batch    95: 10.989\n",
      "Loss after mini batch    95: 11.745\n",
      "Loss after mini batch    95: 12.287\n",
      "Loss after mini batch    95: 11.648\n",
      "Loss after mini batch    95: 12.759\n",
      "Loss after mini batch    95: 12.734\n",
      "Starting epoch 96\n",
      "Loss after mini batch    96: 12.664\n",
      "Loss after mini batch    96: 12.335\n",
      "Loss after mini batch    96: 11.615\n",
      "Loss after mini batch    96: 12.952\n",
      "Loss after mini batch    96: 12.019\n",
      "Loss after mini batch    96: 11.180\n",
      "Loss after mini batch    96: 11.728\n",
      "Loss after mini batch    96: 13.018\n",
      "Loss after mini batch    96: 12.143\n",
      "Loss after mini batch    96: 11.935\n",
      "Loss after mini batch    96: 12.439\n",
      "Starting epoch 97\n",
      "Loss after mini batch    97: 13.213\n",
      "Loss after mini batch    97: 11.505\n",
      "Loss after mini batch    97: 12.269\n",
      "Loss after mini batch    97: 12.125\n",
      "Loss after mini batch    97: 11.594\n",
      "Loss after mini batch    97: 11.105\n",
      "Loss after mini batch    97: 12.115\n",
      "Loss after mini batch    97: 12.218\n",
      "Loss after mini batch    97: 11.329\n",
      "Loss after mini batch    97: 12.687\n",
      "Loss after mini batch    97: 14.401\n",
      "Starting epoch 98\n",
      "Loss after mini batch    98: 11.153\n",
      "Loss after mini batch    98: 11.993\n",
      "Loss after mini batch    98: 12.658\n",
      "Loss after mini batch    98: 13.403\n",
      "Loss after mini batch    98: 12.807\n",
      "Loss after mini batch    98: 11.554\n",
      "Loss after mini batch    98: 11.643\n",
      "Loss after mini batch    98: 12.250\n",
      "Loss after mini batch    98: 11.387\n",
      "Loss after mini batch    98: 12.307\n",
      "Loss after mini batch    98: 13.034\n",
      "Starting epoch 99\n",
      "Loss after mini batch    99: 11.894\n",
      "Loss after mini batch    99: 11.356\n",
      "Loss after mini batch    99: 13.174\n",
      "Loss after mini batch    99: 12.279\n",
      "Loss after mini batch    99: 13.179\n",
      "Loss after mini batch    99: 12.740\n",
      "Loss after mini batch    99: 11.551\n",
      "Loss after mini batch    99: 11.413\n",
      "Loss after mini batch    99: 11.045\n",
      "Loss after mini batch    99: 14.105\n",
      "Loss after mini batch    99: 11.696\n",
      "Starting epoch 100\n",
      "Loss after mini batch   100: 12.113\n",
      "Loss after mini batch   100: 13.444\n",
      "Loss after mini batch   100: 12.531\n",
      "Loss after mini batch   100: 11.720\n",
      "Loss after mini batch   100: 12.544\n",
      "Loss after mini batch   100: 12.200\n",
      "Loss after mini batch   100: 11.529\n",
      "Loss after mini batch   100: 12.691\n",
      "Loss after mini batch   100: 11.712\n",
      "Loss after mini batch   100: 12.421\n",
      "Loss after mini batch   100: 12.429\n",
      "Training Complete\n",
      "Starting Testing\n",
      "RMSE for fold 5: 3.3468918344909544\n",
      "rRMSE for fold 5: 0.06494981848514836\n",
      "r for fold 5: 0.9905152911059146\n",
      "Fast RMSE for fold 5: 3.2255801721394888\n",
      "Fast rRMSE for fold 5: 0.062159524030710865\n",
      "Fast r for fold 5: 0.9834692204002321\n",
      "Slow RMSE for fold 5: 4.058962176492325\n",
      "Slow rRMSE for fold 5: 0.07451157653878054\n",
      "Slow r for fold 5: 0.9976772091279198\n",
      "Regular RMSE for fold 5: 2.7553312470121845\n",
      "Regular rRMSE for fold 5: 0.05648892536904532\n",
      "Regular r for fold 5: 0.9964950022079134\n",
      "Fold 6\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini batch     1: 357.143\n",
      "Loss after mini batch     1: 21.327\n",
      "Loss after mini batch     1: 29.615\n",
      "Loss after mini batch     1: 32.837\n",
      "Loss after mini batch     1: 27.784\n",
      "Loss after mini batch     1: 20.335\n",
      "Loss after mini batch     1: 22.080\n",
      "Loss after mini batch     1: 16.250\n",
      "Loss after mini batch     1: 23.130\n",
      "Loss after mini batch     1: 20.386\n",
      "Loss after mini batch     1: 49.007\n",
      "Starting epoch 2\n",
      "Loss after mini batch     2: 29.654\n",
      "Loss after mini batch     2: 18.894\n",
      "Loss after mini batch     2: 19.388\n",
      "Loss after mini batch     2: 26.701\n",
      "Loss after mini batch     2: 14.790\n",
      "Loss after mini batch     2: 16.196\n",
      "Loss after mini batch     2: 18.870\n",
      "Loss after mini batch     2: 22.677\n",
      "Loss after mini batch     2: 22.652\n",
      "Loss after mini batch     2: 16.269\n",
      "Loss after mini batch     2: 16.755\n",
      "Starting epoch 3\n",
      "Loss after mini batch     3: 15.484\n",
      "Loss after mini batch     3: 18.356\n",
      "Loss after mini batch     3: 15.595\n",
      "Loss after mini batch     3: 19.085\n",
      "Loss after mini batch     3: 16.130\n",
      "Loss after mini batch     3: 20.253\n",
      "Loss after mini batch     3: 19.021\n",
      "Loss after mini batch     3: 19.541\n",
      "Loss after mini batch     3: 16.047\n",
      "Loss after mini batch     3: 22.823\n",
      "Loss after mini batch     3: 14.852\n",
      "Starting epoch 4\n",
      "Loss after mini batch     4: 16.234\n",
      "Loss after mini batch     4: 16.688\n",
      "Loss after mini batch     4: 13.226\n",
      "Loss after mini batch     4: 16.334\n",
      "Loss after mini batch     4: 16.864\n",
      "Loss after mini batch     4: 18.377\n",
      "Loss after mini batch     4: 14.149\n",
      "Loss after mini batch     4: 14.265\n",
      "Loss after mini batch     4: 18.294\n",
      "Loss after mini batch     4: 16.701\n",
      "Loss after mini batch     4: 16.646\n",
      "Starting epoch 5\n",
      "Loss after mini batch     5: 16.735\n",
      "Loss after mini batch     5: 18.104\n",
      "Loss after mini batch     5: 14.325\n",
      "Loss after mini batch     5: 15.055\n",
      "Loss after mini batch     5: 15.049\n",
      "Loss after mini batch     5: 20.349\n",
      "Loss after mini batch     5: 17.964\n",
      "Loss after mini batch     5: 14.710\n",
      "Loss after mini batch     5: 16.363\n",
      "Loss after mini batch     5: 16.313\n",
      "Loss after mini batch     5: 13.993\n",
      "Starting epoch 6\n",
      "Loss after mini batch     6: 15.742\n",
      "Loss after mini batch     6: 13.886\n",
      "Loss after mini batch     6: 16.088\n",
      "Loss after mini batch     6: 14.763\n",
      "Loss after mini batch     6: 14.603\n",
      "Loss after mini batch     6: 15.316\n",
      "Loss after mini batch     6: 15.237\n",
      "Loss after mini batch     6: 13.788\n",
      "Loss after mini batch     6: 16.496\n",
      "Loss after mini batch     6: 14.621\n",
      "Loss after mini batch     6: 18.565\n",
      "Starting epoch 7\n",
      "Loss after mini batch     7: 16.229\n",
      "Loss after mini batch     7: 15.360\n",
      "Loss after mini batch     7: 15.475\n",
      "Loss after mini batch     7: 15.174\n",
      "Loss after mini batch     7: 13.502\n",
      "Loss after mini batch     7: 14.392\n",
      "Loss after mini batch     7: 14.330\n",
      "Loss after mini batch     7: 14.735\n",
      "Loss after mini batch     7: 13.626\n",
      "Loss after mini batch     7: 14.786\n",
      "Loss after mini batch     7: 14.959\n",
      "Starting epoch 8\n",
      "Loss after mini batch     8: 13.605\n",
      "Loss after mini batch     8: 16.520\n",
      "Loss after mini batch     8: 17.097\n",
      "Loss after mini batch     8: 13.723\n",
      "Loss after mini batch     8: 14.475\n",
      "Loss after mini batch     8: 14.684\n",
      "Loss after mini batch     8: 14.616\n",
      "Loss after mini batch     8: 16.505\n",
      "Loss after mini batch     8: 14.542\n",
      "Loss after mini batch     8: 13.509\n",
      "Loss after mini batch     8: 14.358\n",
      "Starting epoch 9\n",
      "Loss after mini batch     9: 15.085\n",
      "Loss after mini batch     9: 15.504\n",
      "Loss after mini batch     9: 13.843\n",
      "Loss after mini batch     9: 15.103\n",
      "Loss after mini batch     9: 14.350\n",
      "Loss after mini batch     9: 12.881\n",
      "Loss after mini batch     9: 12.990\n",
      "Loss after mini batch     9: 16.893\n",
      "Loss after mini batch     9: 15.422\n",
      "Loss after mini batch     9: 14.625\n",
      "Loss after mini batch     9: 12.815\n",
      "Starting epoch 10\n",
      "Loss after mini batch    10: 14.275\n",
      "Loss after mini batch    10: 15.635\n",
      "Loss after mini batch    10: 14.695\n",
      "Loss after mini batch    10: 13.523\n",
      "Loss after mini batch    10: 16.352\n",
      "Loss after mini batch    10: 14.548\n",
      "Loss after mini batch    10: 13.005\n",
      "Loss after mini batch    10: 13.622\n",
      "Loss after mini batch    10: 15.092\n",
      "Loss after mini batch    10: 13.413\n",
      "Loss after mini batch    10: 13.552\n",
      "Starting epoch 11\n",
      "Loss after mini batch    11: 15.819\n",
      "Loss after mini batch    11: 13.889\n",
      "Loss after mini batch    11: 13.420\n",
      "Loss after mini batch    11: 15.046\n",
      "Loss after mini batch    11: 14.575\n",
      "Loss after mini batch    11: 15.678\n",
      "Loss after mini batch    11: 14.190\n",
      "Loss after mini batch    11: 14.858\n",
      "Loss after mini batch    11: 14.893\n",
      "Loss after mini batch    11: 12.431\n",
      "Loss after mini batch    11: 14.469\n",
      "Starting epoch 12\n",
      "Loss after mini batch    12: 13.155\n",
      "Loss after mini batch    12: 13.403\n",
      "Loss after mini batch    12: 13.511\n",
      "Loss after mini batch    12: 15.891\n",
      "Loss after mini batch    12: 14.900\n",
      "Loss after mini batch    12: 14.632\n",
      "Loss after mini batch    12: 13.708\n",
      "Loss after mini batch    12: 13.498\n",
      "Loss after mini batch    12: 15.898\n",
      "Loss after mini batch    12: 14.533\n",
      "Loss after mini batch    12: 12.502\n",
      "Starting epoch 13\n",
      "Loss after mini batch    13: 13.285\n",
      "Loss after mini batch    13: 14.781\n",
      "Loss after mini batch    13: 13.252\n",
      "Loss after mini batch    13: 14.691\n",
      "Loss after mini batch    13: 14.953\n",
      "Loss after mini batch    13: 11.457\n",
      "Loss after mini batch    13: 16.409\n",
      "Loss after mini batch    13: 14.415\n",
      "Loss after mini batch    13: 13.430\n",
      "Loss after mini batch    13: 12.817\n",
      "Loss after mini batch    13: 12.314\n",
      "Starting epoch 14\n",
      "Loss after mini batch    14: 13.506\n",
      "Loss after mini batch    14: 12.675\n",
      "Loss after mini batch    14: 13.860\n",
      "Loss after mini batch    14: 18.056\n",
      "Loss after mini batch    14: 14.574\n",
      "Loss after mini batch    14: 13.796\n",
      "Loss after mini batch    14: 12.621\n",
      "Loss after mini batch    14: 13.258\n",
      "Loss after mini batch    14: 12.805\n",
      "Loss after mini batch    14: 13.809\n",
      "Loss after mini batch    14: 14.627\n",
      "Starting epoch 15\n",
      "Loss after mini batch    15: 13.893\n",
      "Loss after mini batch    15: 14.407\n",
      "Loss after mini batch    15: 13.166\n",
      "Loss after mini batch    15: 12.898\n",
      "Loss after mini batch    15: 12.858\n",
      "Loss after mini batch    15: 13.860\n",
      "Loss after mini batch    15: 12.915\n",
      "Loss after mini batch    15: 13.129\n",
      "Loss after mini batch    15: 13.609\n",
      "Loss after mini batch    15: 11.594\n",
      "Loss after mini batch    15: 14.521\n",
      "Starting epoch 16\n",
      "Loss after mini batch    16: 13.378\n",
      "Loss after mini batch    16: 12.761\n",
      "Loss after mini batch    16: 12.563\n",
      "Loss after mini batch    16: 13.825\n",
      "Loss after mini batch    16: 12.491\n",
      "Loss after mini batch    16: 13.660\n",
      "Loss after mini batch    16: 13.484\n",
      "Loss after mini batch    16: 12.951\n",
      "Loss after mini batch    16: 15.268\n",
      "Loss after mini batch    16: 12.402\n",
      "Loss after mini batch    16: 13.125\n",
      "Starting epoch 17\n",
      "Loss after mini batch    17: 14.093\n",
      "Loss after mini batch    17: 13.011\n",
      "Loss after mini batch    17: 12.245\n",
      "Loss after mini batch    17: 13.049\n",
      "Loss after mini batch    17: 14.805\n",
      "Loss after mini batch    17: 13.334\n",
      "Loss after mini batch    17: 15.675\n",
      "Loss after mini batch    17: 14.229\n",
      "Loss after mini batch    17: 13.936\n",
      "Loss after mini batch    17: 12.271\n",
      "Loss after mini batch    17: 15.423\n",
      "Starting epoch 18\n",
      "Loss after mini batch    18: 12.466\n",
      "Loss after mini batch    18: 12.971\n",
      "Loss after mini batch    18: 12.822\n",
      "Loss after mini batch    18: 12.739\n",
      "Loss after mini batch    18: 12.978\n",
      "Loss after mini batch    18: 12.999\n",
      "Loss after mini batch    18: 14.638\n",
      "Loss after mini batch    18: 14.030\n",
      "Loss after mini batch    18: 14.382\n",
      "Loss after mini batch    18: 13.174\n",
      "Loss after mini batch    18: 13.261\n",
      "Starting epoch 19\n",
      "Loss after mini batch    19: 13.213\n",
      "Loss after mini batch    19: 13.086\n",
      "Loss after mini batch    19: 12.876\n",
      "Loss after mini batch    19: 14.472\n",
      "Loss after mini batch    19: 13.468\n",
      "Loss after mini batch    19: 15.008\n",
      "Loss after mini batch    19: 13.868\n",
      "Loss after mini batch    19: 15.103\n",
      "Loss after mini batch    19: 13.841\n",
      "Loss after mini batch    19: 14.590\n",
      "Loss after mini batch    19: 12.316\n",
      "Starting epoch 20\n",
      "Loss after mini batch    20: 13.779\n",
      "Loss after mini batch    20: 13.083\n",
      "Loss after mini batch    20: 12.800\n",
      "Loss after mini batch    20: 13.925\n",
      "Loss after mini batch    20: 11.773\n",
      "Loss after mini batch    20: 12.968\n",
      "Loss after mini batch    20: 12.333\n",
      "Loss after mini batch    20: 13.396\n",
      "Loss after mini batch    20: 13.303\n",
      "Loss after mini batch    20: 13.039\n",
      "Loss after mini batch    20: 13.593\n",
      "Starting epoch 21\n",
      "Loss after mini batch    21: 12.029\n",
      "Loss after mini batch    21: 13.566\n",
      "Loss after mini batch    21: 13.400\n",
      "Loss after mini batch    21: 12.438\n",
      "Loss after mini batch    21: 12.637\n",
      "Loss after mini batch    21: 13.171\n",
      "Loss after mini batch    21: 14.182\n",
      "Loss after mini batch    21: 14.575\n",
      "Loss after mini batch    21: 12.866\n",
      "Loss after mini batch    21: 13.390\n",
      "Loss after mini batch    21: 13.997\n",
      "Starting epoch 22\n",
      "Loss after mini batch    22: 13.334\n",
      "Loss after mini batch    22: 12.023\n",
      "Loss after mini batch    22: 15.550\n",
      "Loss after mini batch    22: 15.167\n",
      "Loss after mini batch    22: 12.021\n",
      "Loss after mini batch    22: 11.963\n",
      "Loss after mini batch    22: 14.198\n",
      "Loss after mini batch    22: 13.804\n",
      "Loss after mini batch    22: 12.641\n",
      "Loss after mini batch    22: 12.431\n",
      "Loss after mini batch    22: 13.217\n",
      "Starting epoch 23\n",
      "Loss after mini batch    23: 11.298\n",
      "Loss after mini batch    23: 16.933\n",
      "Loss after mini batch    23: 12.604\n",
      "Loss after mini batch    23: 15.105\n",
      "Loss after mini batch    23: 13.489\n",
      "Loss after mini batch    23: 15.620\n",
      "Loss after mini batch    23: 14.339\n",
      "Loss after mini batch    23: 12.903\n",
      "Loss after mini batch    23: 13.573\n",
      "Loss after mini batch    23: 12.651\n",
      "Loss after mini batch    23: 13.378\n",
      "Starting epoch 24\n",
      "Loss after mini batch    24: 13.009\n",
      "Loss after mini batch    24: 13.442\n",
      "Loss after mini batch    24: 14.166\n",
      "Loss after mini batch    24: 13.292\n",
      "Loss after mini batch    24: 11.894\n",
      "Loss after mini batch    24: 13.164\n",
      "Loss after mini batch    24: 13.922\n",
      "Loss after mini batch    24: 12.541\n",
      "Loss after mini batch    24: 13.369\n",
      "Loss after mini batch    24: 14.209\n",
      "Loss after mini batch    24: 12.799\n",
      "Starting epoch 25\n",
      "Loss after mini batch    25: 12.303\n",
      "Loss after mini batch    25: 13.052\n",
      "Loss after mini batch    25: 12.089\n",
      "Loss after mini batch    25: 13.608\n",
      "Loss after mini batch    25: 13.347\n",
      "Loss after mini batch    25: 13.713\n",
      "Loss after mini batch    25: 12.847\n",
      "Loss after mini batch    25: 12.604\n",
      "Loss after mini batch    25: 14.057\n",
      "Loss after mini batch    25: 14.422\n",
      "Loss after mini batch    25: 13.087\n",
      "Starting epoch 26\n",
      "Loss after mini batch    26: 13.994\n",
      "Loss after mini batch    26: 11.580\n",
      "Loss after mini batch    26: 13.142\n",
      "Loss after mini batch    26: 13.916\n",
      "Loss after mini batch    26: 13.043\n",
      "Loss after mini batch    26: 12.364\n",
      "Loss after mini batch    26: 14.228\n",
      "Loss after mini batch    26: 13.411\n",
      "Loss after mini batch    26: 11.988\n",
      "Loss after mini batch    26: 12.411\n",
      "Loss after mini batch    26: 14.468\n",
      "Starting epoch 27\n",
      "Loss after mini batch    27: 13.463\n",
      "Loss after mini batch    27: 13.753\n",
      "Loss after mini batch    27: 14.184\n",
      "Loss after mini batch    27: 12.124\n",
      "Loss after mini batch    27: 13.599\n",
      "Loss after mini batch    27: 13.846\n",
      "Loss after mini batch    27: 11.932\n",
      "Loss after mini batch    27: 11.409\n",
      "Loss after mini batch    27: 12.883\n",
      "Loss after mini batch    27: 13.701\n",
      "Loss after mini batch    27: 13.585\n",
      "Starting epoch 28\n",
      "Loss after mini batch    28: 12.555\n",
      "Loss after mini batch    28: 14.470\n",
      "Loss after mini batch    28: 14.178\n",
      "Loss after mini batch    28: 12.663\n",
      "Loss after mini batch    28: 11.566\n",
      "Loss after mini batch    28: 13.602\n",
      "Loss after mini batch    28: 13.582\n",
      "Loss after mini batch    28: 13.236\n",
      "Loss after mini batch    28: 12.920\n",
      "Loss after mini batch    28: 13.507\n",
      "Loss after mini batch    28: 13.107\n",
      "Starting epoch 29\n",
      "Loss after mini batch    29: 15.247\n",
      "Loss after mini batch    29: 12.655\n",
      "Loss after mini batch    29: 14.515\n",
      "Loss after mini batch    29: 13.516\n",
      "Loss after mini batch    29: 13.038\n",
      "Loss after mini batch    29: 11.427\n",
      "Loss after mini batch    29: 12.546\n",
      "Loss after mini batch    29: 14.017\n",
      "Loss after mini batch    29: 13.511\n",
      "Loss after mini batch    29: 11.997\n",
      "Loss after mini batch    29: 15.764\n",
      "Starting epoch 30\n",
      "Loss after mini batch    30: 12.777\n",
      "Loss after mini batch    30: 14.613\n",
      "Loss after mini batch    30: 14.198\n",
      "Loss after mini batch    30: 12.423\n",
      "Loss after mini batch    30: 12.830\n",
      "Loss after mini batch    30: 12.045\n",
      "Loss after mini batch    30: 11.952\n",
      "Loss after mini batch    30: 15.678\n",
      "Loss after mini batch    30: 12.075\n",
      "Loss after mini batch    30: 11.848\n",
      "Loss after mini batch    30: 13.602\n",
      "Starting epoch 31\n",
      "Loss after mini batch    31: 13.262\n",
      "Loss after mini batch    31: 14.255\n",
      "Loss after mini batch    31: 13.814\n",
      "Loss after mini batch    31: 11.614\n",
      "Loss after mini batch    31: 12.474\n",
      "Loss after mini batch    31: 13.002\n",
      "Loss after mini batch    31: 13.316\n",
      "Loss after mini batch    31: 12.432\n",
      "Loss after mini batch    31: 14.194\n",
      "Loss after mini batch    31: 12.172\n",
      "Loss after mini batch    31: 14.387\n",
      "Starting epoch 32\n",
      "Loss after mini batch    32: 12.959\n",
      "Loss after mini batch    32: 12.546\n",
      "Loss after mini batch    32: 13.543\n",
      "Loss after mini batch    32: 14.511\n",
      "Loss after mini batch    32: 12.225\n",
      "Loss after mini batch    32: 12.902\n",
      "Loss after mini batch    32: 12.777\n",
      "Loss after mini batch    32: 11.502\n",
      "Loss after mini batch    32: 12.520\n",
      "Loss after mini batch    32: 13.272\n",
      "Loss after mini batch    32: 14.845\n",
      "Starting epoch 33\n",
      "Loss after mini batch    33: 12.659\n",
      "Loss after mini batch    33: 13.489\n",
      "Loss after mini batch    33: 13.570\n",
      "Loss after mini batch    33: 12.205\n",
      "Loss after mini batch    33: 13.074\n",
      "Loss after mini batch    33: 11.899\n",
      "Loss after mini batch    33: 13.013\n",
      "Loss after mini batch    33: 12.899\n",
      "Loss after mini batch    33: 13.888\n",
      "Loss after mini batch    33: 13.516\n",
      "Loss after mini batch    33: 13.719\n",
      "Starting epoch 34\n",
      "Loss after mini batch    34: 11.723\n",
      "Loss after mini batch    34: 13.777\n",
      "Loss after mini batch    34: 13.283\n",
      "Loss after mini batch    34: 12.462\n",
      "Loss after mini batch    34: 12.703\n",
      "Loss after mini batch    34: 12.936\n",
      "Loss after mini batch    34: 12.209\n",
      "Loss after mini batch    34: 13.435\n",
      "Loss after mini batch    34: 13.702\n",
      "Loss after mini batch    34: 14.181\n",
      "Loss after mini batch    34: 11.605\n",
      "Starting epoch 35\n",
      "Loss after mini batch    35: 12.017\n",
      "Loss after mini batch    35: 13.622\n",
      "Loss after mini batch    35: 13.601\n",
      "Loss after mini batch    35: 13.314\n",
      "Loss after mini batch    35: 12.981\n",
      "Loss after mini batch    35: 13.818\n",
      "Loss after mini batch    35: 13.480\n",
      "Loss after mini batch    35: 12.528\n",
      "Loss after mini batch    35: 13.222\n",
      "Loss after mini batch    35: 13.191\n",
      "Loss after mini batch    35: 12.815\n",
      "Starting epoch 36\n",
      "Loss after mini batch    36: 12.541\n",
      "Loss after mini batch    36: 11.907\n",
      "Loss after mini batch    36: 13.574\n",
      "Loss after mini batch    36: 12.784\n",
      "Loss after mini batch    36: 13.841\n",
      "Loss after mini batch    36: 12.622\n",
      "Loss after mini batch    36: 13.618\n",
      "Loss after mini batch    36: 14.178\n",
      "Loss after mini batch    36: 13.662\n",
      "Loss after mini batch    36: 12.054\n",
      "Loss after mini batch    36: 13.115\n",
      "Starting epoch 37\n",
      "Loss after mini batch    37: 11.304\n",
      "Loss after mini batch    37: 13.507\n",
      "Loss after mini batch    37: 13.021\n",
      "Loss after mini batch    37: 12.671\n",
      "Loss after mini batch    37: 13.590\n",
      "Loss after mini batch    37: 13.083\n",
      "Loss after mini batch    37: 13.395\n",
      "Loss after mini batch    37: 12.211\n",
      "Loss after mini batch    37: 13.908\n",
      "Loss after mini batch    37: 13.321\n",
      "Loss after mini batch    37: 13.058\n",
      "Starting epoch 38\n",
      "Loss after mini batch    38: 12.766\n",
      "Loss after mini batch    38: 13.652\n",
      "Loss after mini batch    38: 13.086\n",
      "Loss after mini batch    38: 12.561\n",
      "Loss after mini batch    38: 12.825\n",
      "Loss after mini batch    38: 12.993\n",
      "Loss after mini batch    38: 13.835\n",
      "Loss after mini batch    38: 11.607\n",
      "Loss after mini batch    38: 13.656\n",
      "Loss after mini batch    38: 13.599\n",
      "Loss after mini batch    38: 12.936\n",
      "Starting epoch 39\n",
      "Loss after mini batch    39: 13.036\n",
      "Loss after mini batch    39: 13.209\n",
      "Loss after mini batch    39: 12.844\n",
      "Loss after mini batch    39: 12.205\n",
      "Loss after mini batch    39: 12.719\n",
      "Loss after mini batch    39: 13.409\n",
      "Loss after mini batch    39: 13.635\n",
      "Loss after mini batch    39: 12.967\n",
      "Loss after mini batch    39: 12.570\n",
      "Loss after mini batch    39: 13.614\n",
      "Loss after mini batch    39: 13.435\n",
      "Starting epoch 40\n",
      "Loss after mini batch    40: 15.043\n",
      "Loss after mini batch    40: 13.210\n",
      "Loss after mini batch    40: 12.342\n",
      "Loss after mini batch    40: 13.278\n",
      "Loss after mini batch    40: 12.399\n",
      "Loss after mini batch    40: 12.611\n",
      "Loss after mini batch    40: 12.617\n",
      "Loss after mini batch    40: 13.854\n",
      "Loss after mini batch    40: 12.775\n",
      "Loss after mini batch    40: 12.521\n",
      "Loss after mini batch    40: 13.860\n",
      "Starting epoch 41\n",
      "Loss after mini batch    41: 12.881\n",
      "Loss after mini batch    41: 11.553\n",
      "Loss after mini batch    41: 11.955\n",
      "Loss after mini batch    41: 12.860\n",
      "Loss after mini batch    41: 13.431\n",
      "Loss after mini batch    41: 13.711\n",
      "Loss after mini batch    41: 12.072\n",
      "Loss after mini batch    41: 12.137\n",
      "Loss after mini batch    41: 12.579\n",
      "Loss after mini batch    41: 14.305\n",
      "Loss after mini batch    41: 14.937\n",
      "Starting epoch 42\n",
      "Loss after mini batch    42: 13.277\n",
      "Loss after mini batch    42: 12.612\n",
      "Loss after mini batch    42: 11.923\n",
      "Loss after mini batch    42: 12.563\n",
      "Loss after mini batch    42: 14.265\n",
      "Loss after mini batch    42: 14.096\n",
      "Loss after mini batch    42: 12.899\n",
      "Loss after mini batch    42: 13.035\n",
      "Loss after mini batch    42: 12.124\n",
      "Loss after mini batch    42: 12.728\n",
      "Loss after mini batch    42: 12.378\n",
      "Starting epoch 43\n",
      "Loss after mini batch    43: 12.696\n",
      "Loss after mini batch    43: 15.474\n",
      "Loss after mini batch    43: 13.125\n",
      "Loss after mini batch    43: 12.856\n",
      "Loss after mini batch    43: 12.910\n",
      "Loss after mini batch    43: 12.285\n",
      "Loss after mini batch    43: 12.608\n",
      "Loss after mini batch    43: 13.284\n",
      "Loss after mini batch    43: 13.213\n",
      "Loss after mini batch    43: 12.829\n",
      "Loss after mini batch    43: 12.972\n",
      "Starting epoch 44\n",
      "Loss after mini batch    44: 13.179\n",
      "Loss after mini batch    44: 11.784\n",
      "Loss after mini batch    44: 13.788\n",
      "Loss after mini batch    44: 13.084\n",
      "Loss after mini batch    44: 13.914\n",
      "Loss after mini batch    44: 13.787\n",
      "Loss after mini batch    44: 13.396\n",
      "Loss after mini batch    44: 11.808\n",
      "Loss after mini batch    44: 12.348\n",
      "Loss after mini batch    44: 12.117\n",
      "Loss after mini batch    44: 12.729\n",
      "Starting epoch 45\n",
      "Loss after mini batch    45: 13.327\n",
      "Loss after mini batch    45: 12.648\n",
      "Loss after mini batch    45: 12.410\n",
      "Loss after mini batch    45: 13.519\n",
      "Loss after mini batch    45: 13.097\n",
      "Loss after mini batch    45: 12.499\n",
      "Loss after mini batch    45: 12.458\n",
      "Loss after mini batch    45: 13.243\n",
      "Loss after mini batch    45: 11.875\n",
      "Loss after mini batch    45: 12.897\n",
      "Loss after mini batch    45: 13.267\n",
      "Starting epoch 46\n",
      "Loss after mini batch    46: 12.934\n",
      "Loss after mini batch    46: 13.026\n",
      "Loss after mini batch    46: 11.438\n",
      "Loss after mini batch    46: 12.121\n",
      "Loss after mini batch    46: 13.350\n",
      "Loss after mini batch    46: 13.464\n",
      "Loss after mini batch    46: 12.617\n",
      "Loss after mini batch    46: 12.714\n",
      "Loss after mini batch    46: 13.149\n",
      "Loss after mini batch    46: 13.017\n",
      "Loss after mini batch    46: 12.217\n",
      "Starting epoch 47\n",
      "Loss after mini batch    47: 13.666\n",
      "Loss after mini batch    47: 13.564\n",
      "Loss after mini batch    47: 13.001\n",
      "Loss after mini batch    47: 12.234\n",
      "Loss after mini batch    47: 12.305\n",
      "Loss after mini batch    47: 13.388\n",
      "Loss after mini batch    47: 12.478\n",
      "Loss after mini batch    47: 12.778\n",
      "Loss after mini batch    47: 12.063\n",
      "Loss after mini batch    47: 13.233\n",
      "Loss after mini batch    47: 13.022\n",
      "Starting epoch 48\n",
      "Loss after mini batch    48: 12.377\n",
      "Loss after mini batch    48: 11.840\n",
      "Loss after mini batch    48: 11.455\n",
      "Loss after mini batch    48: 12.633\n",
      "Loss after mini batch    48: 13.837\n",
      "Loss after mini batch    48: 13.003\n",
      "Loss after mini batch    48: 12.690\n",
      "Loss after mini batch    48: 13.133\n",
      "Loss after mini batch    48: 13.600\n",
      "Loss after mini batch    48: 12.330\n",
      "Loss after mini batch    48: 12.827\n",
      "Starting epoch 49\n",
      "Loss after mini batch    49: 12.828\n",
      "Loss after mini batch    49: 12.084\n",
      "Loss after mini batch    49: 12.190\n",
      "Loss after mini batch    49: 12.648\n",
      "Loss after mini batch    49: 12.624\n",
      "Loss after mini batch    49: 13.148\n",
      "Loss after mini batch    49: 12.918\n",
      "Loss after mini batch    49: 12.969\n",
      "Loss after mini batch    49: 11.572\n",
      "Loss after mini batch    49: 14.110\n",
      "Loss after mini batch    49: 12.678\n",
      "Starting epoch 50\n",
      "Loss after mini batch    50: 13.877\n",
      "Loss after mini batch    50: 13.972\n",
      "Loss after mini batch    50: 11.720\n",
      "Loss after mini batch    50: 14.627\n",
      "Loss after mini batch    50: 13.828\n",
      "Loss after mini batch    50: 12.528\n",
      "Loss after mini batch    50: 13.428\n",
      "Loss after mini batch    50: 13.619\n",
      "Loss after mini batch    50: 12.949\n",
      "Loss after mini batch    50: 11.733\n",
      "Loss after mini batch    50: 11.887\n",
      "Starting epoch 51\n",
      "Loss after mini batch    51: 11.552\n",
      "Loss after mini batch    51: 14.088\n",
      "Loss after mini batch    51: 13.850\n",
      "Loss after mini batch    51: 12.435\n",
      "Loss after mini batch    51: 12.131\n",
      "Loss after mini batch    51: 10.975\n",
      "Loss after mini batch    51: 13.181\n",
      "Loss after mini batch    51: 13.634\n",
      "Loss after mini batch    51: 12.749\n",
      "Loss after mini batch    51: 13.578\n",
      "Loss after mini batch    51: 13.693\n",
      "Starting epoch 52\n",
      "Loss after mini batch    52: 12.757\n",
      "Loss after mini batch    52: 12.245\n",
      "Loss after mini batch    52: 13.672\n",
      "Loss after mini batch    52: 12.726\n",
      "Loss after mini batch    52: 13.402\n",
      "Loss after mini batch    52: 13.312\n",
      "Loss after mini batch    52: 11.948\n",
      "Loss after mini batch    52: 12.702\n",
      "Loss after mini batch    52: 13.042\n",
      "Loss after mini batch    52: 13.161\n",
      "Loss after mini batch    52: 11.507\n",
      "Starting epoch 53\n",
      "Loss after mini batch    53: 14.243\n",
      "Loss after mini batch    53: 12.880\n",
      "Loss after mini batch    53: 13.993\n",
      "Loss after mini batch    53: 13.716\n",
      "Loss after mini batch    53: 12.037\n",
      "Loss after mini batch    53: 12.042\n",
      "Loss after mini batch    53: 11.204\n",
      "Loss after mini batch    53: 12.454\n",
      "Loss after mini batch    53: 14.300\n",
      "Loss after mini batch    53: 11.979\n",
      "Loss after mini batch    53: 12.919\n",
      "Starting epoch 54\n",
      "Loss after mini batch    54: 13.287\n",
      "Loss after mini batch    54: 12.941\n",
      "Loss after mini batch    54: 11.731\n",
      "Loss after mini batch    54: 12.491\n",
      "Loss after mini batch    54: 12.121\n",
      "Loss after mini batch    54: 12.726\n",
      "Loss after mini batch    54: 12.772\n",
      "Loss after mini batch    54: 13.216\n",
      "Loss after mini batch    54: 13.890\n",
      "Loss after mini batch    54: 12.957\n",
      "Loss after mini batch    54: 13.787\n",
      "Starting epoch 55\n",
      "Loss after mini batch    55: 12.807\n",
      "Loss after mini batch    55: 12.827\n",
      "Loss after mini batch    55: 12.745\n",
      "Loss after mini batch    55: 13.611\n",
      "Loss after mini batch    55: 11.513\n",
      "Loss after mini batch    55: 11.859\n",
      "Loss after mini batch    55: 12.973\n",
      "Loss after mini batch    55: 11.944\n",
      "Loss after mini batch    55: 12.953\n",
      "Loss after mini batch    55: 14.217\n",
      "Loss after mini batch    55: 13.046\n",
      "Starting epoch 56\n",
      "Loss after mini batch    56: 11.957\n",
      "Loss after mini batch    56: 13.857\n",
      "Loss after mini batch    56: 11.661\n",
      "Loss after mini batch    56: 13.383\n",
      "Loss after mini batch    56: 12.116\n",
      "Loss after mini batch    56: 12.658\n",
      "Loss after mini batch    56: 12.756\n",
      "Loss after mini batch    56: 13.665\n",
      "Loss after mini batch    56: 13.036\n",
      "Loss after mini batch    56: 12.230\n",
      "Loss after mini batch    56: 15.501\n",
      "Starting epoch 57\n",
      "Loss after mini batch    57: 12.268\n",
      "Loss after mini batch    57: 14.523\n",
      "Loss after mini batch    57: 12.508\n",
      "Loss after mini batch    57: 12.683\n",
      "Loss after mini batch    57: 13.622\n",
      "Loss after mini batch    57: 11.428\n",
      "Loss after mini batch    57: 12.332\n",
      "Loss after mini batch    57: 12.357\n",
      "Loss after mini batch    57: 12.090\n",
      "Loss after mini batch    57: 14.213\n",
      "Loss after mini batch    57: 13.066\n",
      "Starting epoch 58\n",
      "Loss after mini batch    58: 13.744\n",
      "Loss after mini batch    58: 13.399\n",
      "Loss after mini batch    58: 12.653\n",
      "Loss after mini batch    58: 12.503\n",
      "Loss after mini batch    58: 13.249\n",
      "Loss after mini batch    58: 11.164\n",
      "Loss after mini batch    58: 11.808\n",
      "Loss after mini batch    58: 12.297\n",
      "Loss after mini batch    58: 12.446\n",
      "Loss after mini batch    58: 12.825\n",
      "Loss after mini batch    58: 13.638\n",
      "Starting epoch 59\n",
      "Loss after mini batch    59: 14.011\n",
      "Loss after mini batch    59: 13.741\n",
      "Loss after mini batch    59: 11.152\n",
      "Loss after mini batch    59: 12.849\n",
      "Loss after mini batch    59: 11.634\n",
      "Loss after mini batch    59: 13.299\n",
      "Loss after mini batch    59: 11.381\n",
      "Loss after mini batch    59: 13.816\n",
      "Loss after mini batch    59: 12.786\n",
      "Loss after mini batch    59: 11.710\n",
      "Loss after mini batch    59: 12.212\n",
      "Starting epoch 60\n",
      "Loss after mini batch    60: 12.155\n",
      "Loss after mini batch    60: 11.940\n",
      "Loss after mini batch    60: 12.743\n",
      "Loss after mini batch    60: 11.964\n",
      "Loss after mini batch    60: 12.888\n",
      "Loss after mini batch    60: 13.089\n",
      "Loss after mini batch    60: 13.116\n",
      "Loss after mini batch    60: 13.767\n",
      "Loss after mini batch    60: 12.830\n",
      "Loss after mini batch    60: 12.901\n",
      "Loss after mini batch    60: 12.261\n",
      "Starting epoch 61\n",
      "Loss after mini batch    61: 13.702\n",
      "Loss after mini batch    61: 14.041\n",
      "Loss after mini batch    61: 11.777\n",
      "Loss after mini batch    61: 12.100\n",
      "Loss after mini batch    61: 14.030\n",
      "Loss after mini batch    61: 12.977\n",
      "Loss after mini batch    61: 13.593\n",
      "Loss after mini batch    61: 11.714\n",
      "Loss after mini batch    61: 12.669\n",
      "Loss after mini batch    61: 12.946\n",
      "Loss after mini batch    61: 12.461\n",
      "Starting epoch 62\n",
      "Loss after mini batch    62: 12.191\n",
      "Loss after mini batch    62: 13.691\n",
      "Loss after mini batch    62: 12.780\n",
      "Loss after mini batch    62: 12.410\n",
      "Loss after mini batch    62: 13.310\n",
      "Loss after mini batch    62: 11.734\n",
      "Loss after mini batch    62: 11.707\n",
      "Loss after mini batch    62: 13.824\n",
      "Loss after mini batch    62: 13.028\n",
      "Loss after mini batch    62: 11.782\n",
      "Loss after mini batch    62: 12.809\n",
      "Starting epoch 63\n",
      "Loss after mini batch    63: 12.700\n",
      "Loss after mini batch    63: 11.595\n",
      "Loss after mini batch    63: 12.135\n",
      "Loss after mini batch    63: 12.496\n",
      "Loss after mini batch    63: 12.025\n",
      "Loss after mini batch    63: 12.499\n",
      "Loss after mini batch    63: 14.143\n",
      "Loss after mini batch    63: 12.480\n",
      "Loss after mini batch    63: 13.941\n",
      "Loss after mini batch    63: 12.936\n",
      "Loss after mini batch    63: 10.589\n",
      "Starting epoch 64\n",
      "Loss after mini batch    64: 12.712\n",
      "Loss after mini batch    64: 13.083\n",
      "Loss after mini batch    64: 13.459\n",
      "Loss after mini batch    64: 11.913\n",
      "Loss after mini batch    64: 10.661\n",
      "Loss after mini batch    64: 13.833\n",
      "Loss after mini batch    64: 13.311\n",
      "Loss after mini batch    64: 13.044\n",
      "Loss after mini batch    64: 13.045\n",
      "Loss after mini batch    64: 10.801\n",
      "Loss after mini batch    64: 13.151\n",
      "Starting epoch 65\n",
      "Loss after mini batch    65: 13.465\n",
      "Loss after mini batch    65: 13.090\n",
      "Loss after mini batch    65: 12.246\n",
      "Loss after mini batch    65: 11.407\n",
      "Loss after mini batch    65: 12.821\n",
      "Loss after mini batch    65: 12.224\n",
      "Loss after mini batch    65: 12.581\n",
      "Loss after mini batch    65: 12.524\n",
      "Loss after mini batch    65: 12.065\n",
      "Loss after mini batch    65: 12.618\n",
      "Loss after mini batch    65: 12.101\n",
      "Starting epoch 66\n",
      "Loss after mini batch    66: 12.371\n",
      "Loss after mini batch    66: 12.211\n",
      "Loss after mini batch    66: 12.274\n",
      "Loss after mini batch    66: 12.321\n",
      "Loss after mini batch    66: 11.770\n",
      "Loss after mini batch    66: 13.001\n",
      "Loss after mini batch    66: 12.458\n",
      "Loss after mini batch    66: 13.130\n",
      "Loss after mini batch    66: 12.156\n",
      "Loss after mini batch    66: 13.161\n",
      "Loss after mini batch    66: 12.798\n",
      "Starting epoch 67\n",
      "Loss after mini batch    67: 11.716\n",
      "Loss after mini batch    67: 12.766\n",
      "Loss after mini batch    67: 12.547\n",
      "Loss after mini batch    67: 13.241\n",
      "Loss after mini batch    67: 11.793\n",
      "Loss after mini batch    67: 12.100\n",
      "Loss after mini batch    67: 12.604\n",
      "Loss after mini batch    67: 12.448\n",
      "Loss after mini batch    67: 12.008\n",
      "Loss after mini batch    67: 13.700\n",
      "Loss after mini batch    67: 13.577\n",
      "Starting epoch 68\n",
      "Loss after mini batch    68: 13.263\n",
      "Loss after mini batch    68: 13.027\n",
      "Loss after mini batch    68: 11.952\n",
      "Loss after mini batch    68: 13.329\n",
      "Loss after mini batch    68: 12.498\n",
      "Loss after mini batch    68: 12.237\n",
      "Loss after mini batch    68: 12.838\n",
      "Loss after mini batch    68: 11.911\n",
      "Loss after mini batch    68: 12.627\n",
      "Loss after mini batch    68: 11.908\n",
      "Loss after mini batch    68: 13.186\n",
      "Starting epoch 69\n",
      "Loss after mini batch    69: 12.875\n",
      "Loss after mini batch    69: 12.083\n",
      "Loss after mini batch    69: 13.167\n",
      "Loss after mini batch    69: 11.988\n",
      "Loss after mini batch    69: 13.354\n",
      "Loss after mini batch    69: 13.355\n",
      "Loss after mini batch    69: 11.621\n",
      "Loss after mini batch    69: 12.036\n",
      "Loss after mini batch    69: 13.208\n",
      "Loss after mini batch    69: 12.108\n",
      "Loss after mini batch    69: 12.287\n",
      "Starting epoch 70\n",
      "Loss after mini batch    70: 11.153\n",
      "Loss after mini batch    70: 12.133\n",
      "Loss after mini batch    70: 10.921\n",
      "Loss after mini batch    70: 12.763\n",
      "Loss after mini batch    70: 13.862\n",
      "Loss after mini batch    70: 11.976\n",
      "Loss after mini batch    70: 13.082\n",
      "Loss after mini batch    70: 12.283\n",
      "Loss after mini batch    70: 12.630\n",
      "Loss after mini batch    70: 13.835\n",
      "Loss after mini batch    70: 12.704\n",
      "Starting epoch 71\n",
      "Loss after mini batch    71: 12.294\n",
      "Loss after mini batch    71: 11.689\n",
      "Loss after mini batch    71: 12.806\n",
      "Loss after mini batch    71: 11.988\n",
      "Loss after mini batch    71: 12.686\n",
      "Loss after mini batch    71: 11.722\n",
      "Loss after mini batch    71: 12.898\n",
      "Loss after mini batch    71: 13.120\n",
      "Loss after mini batch    71: 12.712\n",
      "Loss after mini batch    71: 11.674\n",
      "Loss after mini batch    71: 11.854\n",
      "Starting epoch 72\n",
      "Loss after mini batch    72: 13.126\n",
      "Loss after mini batch    72: 12.850\n",
      "Loss after mini batch    72: 12.554\n",
      "Loss after mini batch    72: 12.077\n",
      "Loss after mini batch    72: 12.681\n",
      "Loss after mini batch    72: 12.038\n",
      "Loss after mini batch    72: 12.511\n",
      "Loss after mini batch    72: 13.754\n",
      "Loss after mini batch    72: 12.528\n",
      "Loss after mini batch    72: 11.336\n",
      "Loss after mini batch    72: 12.309\n",
      "Starting epoch 73\n",
      "Loss after mini batch    73: 12.863\n",
      "Loss after mini batch    73: 13.028\n",
      "Loss after mini batch    73: 11.487\n",
      "Loss after mini batch    73: 12.787\n",
      "Loss after mini batch    73: 12.387\n",
      "Loss after mini batch    73: 12.484\n",
      "Loss after mini batch    73: 13.515\n",
      "Loss after mini batch    73: 11.883\n",
      "Loss after mini batch    73: 14.138\n",
      "Loss after mini batch    73: 12.165\n",
      "Loss after mini batch    73: 12.857\n",
      "Starting epoch 74\n",
      "Loss after mini batch    74: 11.250\n",
      "Loss after mini batch    74: 12.777\n",
      "Loss after mini batch    74: 14.087\n",
      "Loss after mini batch    74: 11.507\n",
      "Loss after mini batch    74: 12.300\n",
      "Loss after mini batch    74: 13.112\n",
      "Loss after mini batch    74: 12.751\n",
      "Loss after mini batch    74: 12.438\n",
      "Loss after mini batch    74: 12.196\n",
      "Loss after mini batch    74: 12.377\n",
      "Loss after mini batch    74: 13.310\n",
      "Starting epoch 75\n",
      "Loss after mini batch    75: 11.346\n",
      "Loss after mini batch    75: 12.147\n",
      "Loss after mini batch    75: 11.809\n",
      "Loss after mini batch    75: 13.649\n",
      "Loss after mini batch    75: 12.565\n",
      "Loss after mini batch    75: 13.393\n",
      "Loss after mini batch    75: 11.869\n",
      "Loss after mini batch    75: 12.548\n",
      "Loss after mini batch    75: 13.248\n",
      "Loss after mini batch    75: 11.730\n",
      "Loss after mini batch    75: 12.446\n",
      "Starting epoch 76\n",
      "Loss after mini batch    76: 10.777\n",
      "Loss after mini batch    76: 13.007\n",
      "Loss after mini batch    76: 11.968\n",
      "Loss after mini batch    76: 11.863\n",
      "Loss after mini batch    76: 12.636\n",
      "Loss after mini batch    76: 12.547\n",
      "Loss after mini batch    76: 11.684\n",
      "Loss after mini batch    76: 12.804\n",
      "Loss after mini batch    76: 12.082\n",
      "Loss after mini batch    76: 13.559\n",
      "Loss after mini batch    76: 13.024\n",
      "Starting epoch 77\n",
      "Loss after mini batch    77: 12.773\n",
      "Loss after mini batch    77: 12.933\n",
      "Loss after mini batch    77: 12.025\n",
      "Loss after mini batch    77: 12.549\n",
      "Loss after mini batch    77: 12.013\n",
      "Loss after mini batch    77: 11.317\n",
      "Loss after mini batch    77: 11.071\n",
      "Loss after mini batch    77: 12.523\n",
      "Loss after mini batch    77: 13.817\n",
      "Loss after mini batch    77: 12.784\n",
      "Loss after mini batch    77: 12.329\n",
      "Starting epoch 78\n",
      "Loss after mini batch    78: 14.386\n",
      "Loss after mini batch    78: 11.851\n",
      "Loss after mini batch    78: 12.505\n",
      "Loss after mini batch    78: 11.951\n",
      "Loss after mini batch    78: 12.171\n",
      "Loss after mini batch    78: 12.023\n",
      "Loss after mini batch    78: 13.007\n",
      "Loss after mini batch    78: 12.606\n",
      "Loss after mini batch    78: 12.692\n",
      "Loss after mini batch    78: 12.071\n",
      "Loss after mini batch    78: 12.099\n",
      "Starting epoch 79\n",
      "Loss after mini batch    79: 12.058\n",
      "Loss after mini batch    79: 13.196\n",
      "Loss after mini batch    79: 11.751\n",
      "Loss after mini batch    79: 12.293\n",
      "Loss after mini batch    79: 12.275\n",
      "Loss after mini batch    79: 10.944\n",
      "Loss after mini batch    79: 12.909\n",
      "Loss after mini batch    79: 12.292\n",
      "Loss after mini batch    79: 11.311\n",
      "Loss after mini batch    79: 11.116\n",
      "Loss after mini batch    79: 12.418\n",
      "Starting epoch 80\n",
      "Loss after mini batch    80: 12.749\n",
      "Loss after mini batch    80: 12.468\n",
      "Loss after mini batch    80: 12.142\n",
      "Loss after mini batch    80: 12.463\n",
      "Loss after mini batch    80: 13.437\n",
      "Loss after mini batch    80: 11.643\n",
      "Loss after mini batch    80: 13.250\n",
      "Loss after mini batch    80: 12.424\n",
      "Loss after mini batch    80: 12.329\n",
      "Loss after mini batch    80: 12.126\n",
      "Loss after mini batch    80: 12.699\n",
      "Starting epoch 81\n",
      "Loss after mini batch    81: 12.420\n",
      "Loss after mini batch    81: 11.890\n",
      "Loss after mini batch    81: 13.242\n",
      "Loss after mini batch    81: 11.800\n",
      "Loss after mini batch    81: 11.870\n",
      "Loss after mini batch    81: 14.111\n",
      "Loss after mini batch    81: 12.185\n",
      "Loss after mini batch    81: 12.525\n",
      "Loss after mini batch    81: 11.937\n",
      "Loss after mini batch    81: 11.856\n",
      "Loss after mini batch    81: 12.287\n",
      "Starting epoch 82\n",
      "Loss after mini batch    82: 11.897\n",
      "Loss after mini batch    82: 11.977\n",
      "Loss after mini batch    82: 13.165\n",
      "Loss after mini batch    82: 12.497\n",
      "Loss after mini batch    82: 11.802\n",
      "Loss after mini batch    82: 11.453\n",
      "Loss after mini batch    82: 12.026\n",
      "Loss after mini batch    82: 12.822\n",
      "Loss after mini batch    82: 12.535\n",
      "Loss after mini batch    82: 12.677\n",
      "Loss after mini batch    82: 12.476\n",
      "Starting epoch 83\n",
      "Loss after mini batch    83: 12.835\n",
      "Loss after mini batch    83: 11.705\n",
      "Loss after mini batch    83: 12.060\n",
      "Loss after mini batch    83: 12.736\n",
      "Loss after mini batch    83: 13.604\n",
      "Loss after mini batch    83: 12.664\n",
      "Loss after mini batch    83: 11.765\n",
      "Loss after mini batch    83: 11.074\n",
      "Loss after mini batch    83: 13.644\n",
      "Loss after mini batch    83: 11.154\n",
      "Loss after mini batch    83: 12.625\n",
      "Starting epoch 84\n",
      "Loss after mini batch    84: 11.933\n",
      "Loss after mini batch    84: 13.175\n",
      "Loss after mini batch    84: 12.833\n",
      "Loss after mini batch    84: 12.208\n",
      "Loss after mini batch    84: 12.734\n",
      "Loss after mini batch    84: 11.502\n",
      "Loss after mini batch    84: 11.041\n",
      "Loss after mini batch    84: 12.080\n",
      "Loss after mini batch    84: 12.980\n",
      "Loss after mini batch    84: 11.874\n",
      "Loss after mini batch    84: 11.368\n",
      "Starting epoch 85\n",
      "Loss after mini batch    85: 11.998\n",
      "Loss after mini batch    85: 11.958\n",
      "Loss after mini batch    85: 12.511\n",
      "Loss after mini batch    85: 14.778\n",
      "Loss after mini batch    85: 12.263\n",
      "Loss after mini batch    85: 11.684\n",
      "Loss after mini batch    85: 13.383\n",
      "Loss after mini batch    85: 11.938\n",
      "Loss after mini batch    85: 11.252\n",
      "Loss after mini batch    85: 12.411\n",
      "Loss after mini batch    85: 13.413\n",
      "Starting epoch 86\n",
      "Loss after mini batch    86: 11.278\n",
      "Loss after mini batch    86: 12.760\n",
      "Loss after mini batch    86: 13.143\n",
      "Loss after mini batch    86: 11.969\n",
      "Loss after mini batch    86: 12.503\n",
      "Loss after mini batch    86: 11.872\n",
      "Loss after mini batch    86: 11.431\n",
      "Loss after mini batch    86: 13.217\n",
      "Loss after mini batch    86: 13.369\n",
      "Loss after mini batch    86: 11.531\n",
      "Loss after mini batch    86: 12.172\n",
      "Starting epoch 87\n",
      "Loss after mini batch    87: 12.223\n",
      "Loss after mini batch    87: 13.587\n",
      "Loss after mini batch    87: 12.166\n",
      "Loss after mini batch    87: 12.914\n",
      "Loss after mini batch    87: 13.314\n",
      "Loss after mini batch    87: 11.806\n",
      "Loss after mini batch    87: 11.495\n",
      "Loss after mini batch    87: 12.962\n",
      "Loss after mini batch    87: 12.613\n",
      "Loss after mini batch    87: 12.663\n",
      "Loss after mini batch    87: 11.301\n",
      "Starting epoch 88\n",
      "Loss after mini batch    88: 12.060\n",
      "Loss after mini batch    88: 12.194\n",
      "Loss after mini batch    88: 12.093\n",
      "Loss after mini batch    88: 13.299\n",
      "Loss after mini batch    88: 13.641\n",
      "Loss after mini batch    88: 11.988\n",
      "Loss after mini batch    88: 12.916\n",
      "Loss after mini batch    88: 10.855\n",
      "Loss after mini batch    88: 12.455\n",
      "Loss after mini batch    88: 13.529\n",
      "Loss after mini batch    88: 11.085\n",
      "Starting epoch 89\n",
      "Loss after mini batch    89: 12.604\n",
      "Loss after mini batch    89: 13.655\n",
      "Loss after mini batch    89: 12.862\n",
      "Loss after mini batch    89: 11.381\n",
      "Loss after mini batch    89: 11.397\n",
      "Loss after mini batch    89: 10.801\n",
      "Loss after mini batch    89: 12.405\n",
      "Loss after mini batch    89: 12.792\n",
      "Loss after mini batch    89: 12.934\n",
      "Loss after mini batch    89: 12.925\n",
      "Loss after mini batch    89: 12.546\n",
      "Starting epoch 90\n",
      "Loss after mini batch    90: 11.902\n",
      "Loss after mini batch    90: 13.374\n",
      "Loss after mini batch    90: 13.989\n",
      "Loss after mini batch    90: 11.589\n",
      "Loss after mini batch    90: 11.272\n",
      "Loss after mini batch    90: 12.821\n",
      "Loss after mini batch    90: 12.887\n",
      "Loss after mini batch    90: 12.563\n",
      "Loss after mini batch    90: 12.477\n",
      "Loss after mini batch    90: 12.401\n",
      "Loss after mini batch    90: 12.052\n",
      "Starting epoch 91\n",
      "Loss after mini batch    91: 14.057\n",
      "Loss after mini batch    91: 12.264\n",
      "Loss after mini batch    91: 11.496\n",
      "Loss after mini batch    91: 10.899\n",
      "Loss after mini batch    91: 12.718\n",
      "Loss after mini batch    91: 12.256\n",
      "Loss after mini batch    91: 12.892\n",
      "Loss after mini batch    91: 12.293\n",
      "Loss after mini batch    91: 12.928\n",
      "Loss after mini batch    91: 12.417\n",
      "Loss after mini batch    91: 12.495\n",
      "Starting epoch 92\n",
      "Loss after mini batch    92: 12.498\n",
      "Loss after mini batch    92: 11.465\n",
      "Loss after mini batch    92: 12.786\n",
      "Loss after mini batch    92: 12.491\n",
      "Loss after mini batch    92: 11.785\n",
      "Loss after mini batch    92: 11.635\n",
      "Loss after mini batch    92: 12.271\n",
      "Loss after mini batch    92: 12.732\n",
      "Loss after mini batch    92: 13.545\n",
      "Loss after mini batch    92: 12.946\n",
      "Loss after mini batch    92: 12.189\n",
      "Starting epoch 93\n",
      "Loss after mini batch    93: 11.204\n",
      "Loss after mini batch    93: 11.518\n",
      "Loss after mini batch    93: 11.311\n",
      "Loss after mini batch    93: 14.004\n",
      "Loss after mini batch    93: 11.861\n",
      "Loss after mini batch    93: 11.941\n",
      "Loss after mini batch    93: 12.167\n",
      "Loss after mini batch    93: 11.814\n",
      "Loss after mini batch    93: 13.744\n",
      "Loss after mini batch    93: 13.107\n",
      "Loss after mini batch    93: 13.079\n",
      "Starting epoch 94\n",
      "Loss after mini batch    94: 11.728\n",
      "Loss after mini batch    94: 11.258\n",
      "Loss after mini batch    94: 10.870\n",
      "Loss after mini batch    94: 13.145\n",
      "Loss after mini batch    94: 12.130\n",
      "Loss after mini batch    94: 12.575\n",
      "Loss after mini batch    94: 11.387\n",
      "Loss after mini batch    94: 11.506\n",
      "Loss after mini batch    94: 12.522\n",
      "Loss after mini batch    94: 12.199\n",
      "Loss after mini batch    94: 13.234\n",
      "Starting epoch 95\n",
      "Loss after mini batch    95: 11.800\n",
      "Loss after mini batch    95: 12.766\n",
      "Loss after mini batch    95: 11.946\n",
      "Loss after mini batch    95: 11.197\n",
      "Loss after mini batch    95: 12.832\n",
      "Loss after mini batch    95: 12.916\n",
      "Loss after mini batch    95: 13.420\n",
      "Loss after mini batch    95: 11.948\n",
      "Loss after mini batch    95: 11.697\n",
      "Loss after mini batch    95: 12.144\n",
      "Loss after mini batch    95: 12.402\n",
      "Starting epoch 96\n",
      "Loss after mini batch    96: 11.902\n",
      "Loss after mini batch    96: 12.102\n",
      "Loss after mini batch    96: 10.283\n",
      "Loss after mini batch    96: 13.036\n",
      "Loss after mini batch    96: 11.909\n",
      "Loss after mini batch    96: 12.880\n",
      "Loss after mini batch    96: 11.911\n",
      "Loss after mini batch    96: 11.554\n",
      "Loss after mini batch    96: 13.913\n",
      "Loss after mini batch    96: 12.442\n",
      "Loss after mini batch    96: 11.466\n",
      "Starting epoch 97\n",
      "Loss after mini batch    97: 12.788\n",
      "Loss after mini batch    97: 11.738\n",
      "Loss after mini batch    97: 12.070\n",
      "Loss after mini batch    97: 12.171\n",
      "Loss after mini batch    97: 12.089\n",
      "Loss after mini batch    97: 11.511\n",
      "Loss after mini batch    97: 12.593\n",
      "Loss after mini batch    97: 12.621\n",
      "Loss after mini batch    97: 11.763\n",
      "Loss after mini batch    97: 12.045\n",
      "Loss after mini batch    97: 11.470\n",
      "Starting epoch 98\n",
      "Loss after mini batch    98: 12.442\n",
      "Loss after mini batch    98: 12.302\n",
      "Loss after mini batch    98: 12.182\n",
      "Loss after mini batch    98: 10.904\n",
      "Loss after mini batch    98: 12.727\n",
      "Loss after mini batch    98: 11.390\n",
      "Loss after mini batch    98: 11.879\n",
      "Loss after mini batch    98: 13.156\n",
      "Loss after mini batch    98: 13.140\n",
      "Loss after mini batch    98: 12.173\n",
      "Loss after mini batch    98: 13.158\n",
      "Starting epoch 99\n",
      "Loss after mini batch    99: 11.941\n",
      "Loss after mini batch    99: 12.520\n",
      "Loss after mini batch    99: 12.586\n",
      "Loss after mini batch    99: 12.814\n",
      "Loss after mini batch    99: 14.252\n",
      "Loss after mini batch    99: 10.698\n",
      "Loss after mini batch    99: 12.103\n",
      "Loss after mini batch    99: 11.928\n",
      "Loss after mini batch    99: 11.793\n",
      "Loss after mini batch    99: 11.956\n",
      "Loss after mini batch    99: 11.835\n",
      "Starting epoch 100\n",
      "Loss after mini batch   100: 11.694\n",
      "Loss after mini batch   100: 12.407\n",
      "Loss after mini batch   100: 12.341\n",
      "Loss after mini batch   100: 13.118\n",
      "Loss after mini batch   100: 12.414\n",
      "Loss after mini batch   100: 11.444\n",
      "Loss after mini batch   100: 11.485\n",
      "Loss after mini batch   100: 12.699\n",
      "Loss after mini batch   100: 12.302\n",
      "Loss after mini batch   100: 12.046\n",
      "Loss after mini batch   100: 11.825\n",
      "Training Complete\n",
      "Starting Testing\n",
      "RMSE for fold 6: 3.5172663269293074\n",
      "rRMSE for fold 6: 0.06814397912443612\n",
      "r for fold 6: 0.9896902384633325\n",
      "Fast RMSE for fold 6: 3.2361816667788457\n",
      "Fast rRMSE for fold 6: 0.061597688577340486\n",
      "Fast r for fold 6: 0.9781187063071688\n",
      "Slow RMSE for fold 6: 4.427979047331132\n",
      "Slow rRMSE for fold 6: 0.08724826217010401\n",
      "Slow r for fold 6: 0.9981730965828121\n",
      "Regular RMSE for fold 6: 2.9598879602208843\n",
      "Regular rRMSE for fold 6: 0.057626963365798256\n",
      "Regular r for fold 6: 0.9963359612688285\n",
      "Fold 7\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini batch     1: 362.124\n",
      "Loss after mini batch     1: 18.843\n",
      "Loss after mini batch     1: 19.195\n",
      "Loss after mini batch     1: 24.759\n",
      "Loss after mini batch     1: 23.717\n",
      "Loss after mini batch     1: 16.816\n",
      "Loss after mini batch     1: 21.962\n",
      "Loss after mini batch     1: 20.829\n",
      "Loss after mini batch     1: 22.592\n",
      "Loss after mini batch     1: 20.212\n",
      "Loss after mini batch     1: 25.951\n",
      "Starting epoch 2\n",
      "Loss after mini batch     2: 17.098\n",
      "Loss after mini batch     2: 24.725\n",
      "Loss after mini batch     2: 17.653\n",
      "Loss after mini batch     2: 16.489\n",
      "Loss after mini batch     2: 19.193\n",
      "Loss after mini batch     2: 40.193\n",
      "Loss after mini batch     2: 19.182\n",
      "Loss after mini batch     2: 16.241\n",
      "Loss after mini batch     2: 17.049\n",
      "Loss after mini batch     2: 19.797\n",
      "Loss after mini batch     2: 24.054\n",
      "Starting epoch 3\n",
      "Loss after mini batch     3: 17.119\n",
      "Loss after mini batch     3: 14.984\n",
      "Loss after mini batch     3: 22.540\n",
      "Loss after mini batch     3: 16.828\n",
      "Loss after mini batch     3: 19.269\n",
      "Loss after mini batch     3: 16.559\n",
      "Loss after mini batch     3: 15.495\n",
      "Loss after mini batch     3: 15.468\n",
      "Loss after mini batch     3: 16.533\n",
      "Loss after mini batch     3: 24.781\n",
      "Loss after mini batch     3: 23.410\n",
      "Starting epoch 4\n",
      "Loss after mini batch     4: 16.948\n",
      "Loss after mini batch     4: 19.418\n",
      "Loss after mini batch     4: 15.577\n",
      "Loss after mini batch     4: 18.625\n",
      "Loss after mini batch     4: 20.785\n",
      "Loss after mini batch     4: 14.465\n",
      "Loss after mini batch     4: 16.878\n",
      "Loss after mini batch     4: 16.328\n",
      "Loss after mini batch     4: 14.998\n",
      "Loss after mini batch     4: 16.999\n",
      "Loss after mini batch     4: 15.471\n",
      "Starting epoch 5\n",
      "Loss after mini batch     5: 17.584\n",
      "Loss after mini batch     5: 26.413\n",
      "Loss after mini batch     5: 16.415\n",
      "Loss after mini batch     5: 15.117\n",
      "Loss after mini batch     5: 18.367\n",
      "Loss after mini batch     5: 20.299\n",
      "Loss after mini batch     5: 13.977\n",
      "Loss after mini batch     5: 17.197\n",
      "Loss after mini batch     5: 14.216\n",
      "Loss after mini batch     5: 14.863\n",
      "Loss after mini batch     5: 13.621\n",
      "Starting epoch 6\n",
      "Loss after mini batch     6: 12.892\n",
      "Loss after mini batch     6: 15.218\n",
      "Loss after mini batch     6: 14.383\n",
      "Loss after mini batch     6: 14.722\n",
      "Loss after mini batch     6: 14.171\n",
      "Loss after mini batch     6: 17.143\n",
      "Loss after mini batch     6: 14.990\n",
      "Loss after mini batch     6: 17.802\n",
      "Loss after mini batch     6: 14.872\n",
      "Loss after mini batch     6: 16.361\n",
      "Loss after mini batch     6: 17.018\n",
      "Starting epoch 7\n",
      "Loss after mini batch     7: 14.953\n",
      "Loss after mini batch     7: 17.328\n",
      "Loss after mini batch     7: 13.580\n",
      "Loss after mini batch     7: 15.835\n",
      "Loss after mini batch     7: 13.878\n",
      "Loss after mini batch     7: 14.840\n",
      "Loss after mini batch     7: 14.106\n",
      "Loss after mini batch     7: 16.657\n",
      "Loss after mini batch     7: 13.431\n",
      "Loss after mini batch     7: 12.798\n",
      "Loss after mini batch     7: 14.357\n",
      "Starting epoch 8\n",
      "Loss after mini batch     8: 14.248\n",
      "Loss after mini batch     8: 16.361\n",
      "Loss after mini batch     8: 12.549\n",
      "Loss after mini batch     8: 18.801\n",
      "Loss after mini batch     8: 16.788\n",
      "Loss after mini batch     8: 14.191\n",
      "Loss after mini batch     8: 14.883\n",
      "Loss after mini batch     8: 15.653\n",
      "Loss after mini batch     8: 12.718\n",
      "Loss after mini batch     8: 17.030\n",
      "Loss after mini batch     8: 13.833\n",
      "Starting epoch 9\n",
      "Loss after mini batch     9: 14.842\n",
      "Loss after mini batch     9: 15.720\n",
      "Loss after mini batch     9: 12.534\n",
      "Loss after mini batch     9: 15.622\n",
      "Loss after mini batch     9: 14.985\n",
      "Loss after mini batch     9: 14.504\n",
      "Loss after mini batch     9: 13.538\n",
      "Loss after mini batch     9: 13.278\n",
      "Loss after mini batch     9: 12.620\n",
      "Loss after mini batch     9: 14.567\n",
      "Loss after mini batch     9: 13.698\n",
      "Starting epoch 10\n",
      "Loss after mini batch    10: 15.221\n",
      "Loss after mini batch    10: 12.957\n",
      "Loss after mini batch    10: 15.042\n",
      "Loss after mini batch    10: 14.594\n",
      "Loss after mini batch    10: 13.455\n",
      "Loss after mini batch    10: 16.507\n",
      "Loss after mini batch    10: 15.790\n",
      "Loss after mini batch    10: 13.816\n",
      "Loss after mini batch    10: 13.850\n",
      "Loss after mini batch    10: 15.349\n",
      "Loss after mini batch    10: 14.133\n",
      "Starting epoch 11\n",
      "Loss after mini batch    11: 13.936\n",
      "Loss after mini batch    11: 14.360\n",
      "Loss after mini batch    11: 14.033\n",
      "Loss after mini batch    11: 17.304\n",
      "Loss after mini batch    11: 13.607\n",
      "Loss after mini batch    11: 13.796\n",
      "Loss after mini batch    11: 13.697\n",
      "Loss after mini batch    11: 14.668\n",
      "Loss after mini batch    11: 12.785\n",
      "Loss after mini batch    11: 15.859\n",
      "Loss after mini batch    11: 14.336\n",
      "Starting epoch 12\n",
      "Loss after mini batch    12: 12.640\n",
      "Loss after mini batch    12: 13.625\n",
      "Loss after mini batch    12: 15.059\n",
      "Loss after mini batch    12: 15.029\n",
      "Loss after mini batch    12: 12.675\n",
      "Loss after mini batch    12: 14.274\n",
      "Loss after mini batch    12: 15.176\n",
      "Loss after mini batch    12: 14.451\n",
      "Loss after mini batch    12: 14.488\n",
      "Loss after mini batch    12: 14.678\n",
      "Loss after mini batch    12: 13.614\n",
      "Starting epoch 13\n",
      "Loss after mini batch    13: 14.052\n",
      "Loss after mini batch    13: 12.506\n",
      "Loss after mini batch    13: 13.921\n",
      "Loss after mini batch    13: 13.663\n",
      "Loss after mini batch    13: 14.065\n",
      "Loss after mini batch    13: 12.780\n",
      "Loss after mini batch    13: 13.976\n",
      "Loss after mini batch    13: 17.922\n",
      "Loss after mini batch    13: 13.973\n",
      "Loss after mini batch    13: 13.803\n",
      "Loss after mini batch    13: 13.077\n",
      "Starting epoch 14\n",
      "Loss after mini batch    14: 14.473\n",
      "Loss after mini batch    14: 12.379\n",
      "Loss after mini batch    14: 13.547\n",
      "Loss after mini batch    14: 14.025\n",
      "Loss after mini batch    14: 12.533\n",
      "Loss after mini batch    14: 13.816\n",
      "Loss after mini batch    14: 12.392\n",
      "Loss after mini batch    14: 13.411\n",
      "Loss after mini batch    14: 15.074\n",
      "Loss after mini batch    14: 15.374\n",
      "Loss after mini batch    14: 14.450\n",
      "Starting epoch 15\n",
      "Loss after mini batch    15: 13.254\n",
      "Loss after mini batch    15: 14.587\n",
      "Loss after mini batch    15: 15.015\n",
      "Loss after mini batch    15: 13.875\n",
      "Loss after mini batch    15: 13.228\n",
      "Loss after mini batch    15: 12.760\n",
      "Loss after mini batch    15: 13.592\n",
      "Loss after mini batch    15: 14.748\n",
      "Loss after mini batch    15: 16.141\n",
      "Loss after mini batch    15: 12.707\n",
      "Loss after mini batch    15: 14.885\n",
      "Starting epoch 16\n",
      "Loss after mini batch    16: 13.233\n",
      "Loss after mini batch    16: 13.735\n",
      "Loss after mini batch    16: 14.278\n",
      "Loss after mini batch    16: 13.482\n",
      "Loss after mini batch    16: 13.385\n",
      "Loss after mini batch    16: 12.382\n",
      "Loss after mini batch    16: 13.410\n",
      "Loss after mini batch    16: 15.472\n",
      "Loss after mini batch    16: 12.873\n",
      "Loss after mini batch    16: 13.349\n",
      "Loss after mini batch    16: 12.805\n",
      "Starting epoch 17\n",
      "Loss after mini batch    17: 13.730\n",
      "Loss after mini batch    17: 12.837\n",
      "Loss after mini batch    17: 12.926\n",
      "Loss after mini batch    17: 12.605\n",
      "Loss after mini batch    17: 13.715\n",
      "Loss after mini batch    17: 13.398\n",
      "Loss after mini batch    17: 14.644\n",
      "Loss after mini batch    17: 15.311\n",
      "Loss after mini batch    17: 14.128\n",
      "Loss after mini batch    17: 15.895\n",
      "Loss after mini batch    17: 13.247\n",
      "Starting epoch 18\n",
      "Loss after mini batch    18: 13.068\n",
      "Loss after mini batch    18: 12.884\n",
      "Loss after mini batch    18: 13.836\n",
      "Loss after mini batch    18: 13.423\n",
      "Loss after mini batch    18: 14.327\n",
      "Loss after mini batch    18: 12.343\n",
      "Loss after mini batch    18: 14.060\n",
      "Loss after mini batch    18: 12.902\n",
      "Loss after mini batch    18: 12.951\n",
      "Loss after mini batch    18: 13.803\n",
      "Loss after mini batch    18: 12.966\n",
      "Starting epoch 19\n",
      "Loss after mini batch    19: 14.751\n",
      "Loss after mini batch    19: 13.850\n",
      "Loss after mini batch    19: 13.168\n",
      "Loss after mini batch    19: 13.641\n",
      "Loss after mini batch    19: 12.616\n",
      "Loss after mini batch    19: 13.316\n",
      "Loss after mini batch    19: 13.200\n",
      "Loss after mini batch    19: 12.952\n",
      "Loss after mini batch    19: 13.363\n",
      "Loss after mini batch    19: 14.151\n",
      "Loss after mini batch    19: 12.599\n",
      "Starting epoch 20\n",
      "Loss after mini batch    20: 13.908\n",
      "Loss after mini batch    20: 13.054\n",
      "Loss after mini batch    20: 12.675\n",
      "Loss after mini batch    20: 14.778\n",
      "Loss after mini batch    20: 15.081\n",
      "Loss after mini batch    20: 13.553\n",
      "Loss after mini batch    20: 12.651\n",
      "Loss after mini batch    20: 15.198\n",
      "Loss after mini batch    20: 12.804\n",
      "Loss after mini batch    20: 12.810\n",
      "Loss after mini batch    20: 13.108\n",
      "Starting epoch 21\n",
      "Loss after mini batch    21: 14.523\n",
      "Loss after mini batch    21: 13.115\n",
      "Loss after mini batch    21: 13.285\n",
      "Loss after mini batch    21: 13.950\n",
      "Loss after mini batch    21: 12.150\n",
      "Loss after mini batch    21: 12.863\n",
      "Loss after mini batch    21: 13.699\n",
      "Loss after mini batch    21: 14.722\n",
      "Loss after mini batch    21: 13.699\n",
      "Loss after mini batch    21: 12.994\n",
      "Loss after mini batch    21: 12.886\n",
      "Starting epoch 22\n",
      "Loss after mini batch    22: 12.840\n",
      "Loss after mini batch    22: 12.366\n",
      "Loss after mini batch    22: 12.009\n",
      "Loss after mini batch    22: 14.236\n",
      "Loss after mini batch    22: 12.188\n",
      "Loss after mini batch    22: 11.768\n",
      "Loss after mini batch    22: 15.080\n",
      "Loss after mini batch    22: 13.926\n",
      "Loss after mini batch    22: 11.942\n",
      "Loss after mini batch    22: 13.621\n",
      "Loss after mini batch    22: 13.293\n",
      "Starting epoch 23\n",
      "Loss after mini batch    23: 12.309\n",
      "Loss after mini batch    23: 13.842\n",
      "Loss after mini batch    23: 13.698\n",
      "Loss after mini batch    23: 14.129\n",
      "Loss after mini batch    23: 12.581\n",
      "Loss after mini batch    23: 13.659\n",
      "Loss after mini batch    23: 12.750\n",
      "Loss after mini batch    23: 14.625\n",
      "Loss after mini batch    23: 14.239\n",
      "Loss after mini batch    23: 11.755\n",
      "Loss after mini batch    23: 12.520\n",
      "Starting epoch 24\n",
      "Loss after mini batch    24: 14.557\n",
      "Loss after mini batch    24: 12.946\n",
      "Loss after mini batch    24: 13.134\n",
      "Loss after mini batch    24: 12.623\n",
      "Loss after mini batch    24: 14.380\n",
      "Loss after mini batch    24: 13.061\n",
      "Loss after mini batch    24: 13.875\n",
      "Loss after mini batch    24: 13.367\n",
      "Loss after mini batch    24: 13.274\n",
      "Loss after mini batch    24: 13.050\n",
      "Loss after mini batch    24: 11.950\n",
      "Starting epoch 25\n",
      "Loss after mini batch    25: 13.463\n",
      "Loss after mini batch    25: 13.201\n",
      "Loss after mini batch    25: 13.224\n",
      "Loss after mini batch    25: 12.065\n",
      "Loss after mini batch    25: 12.292\n",
      "Loss after mini batch    25: 14.140\n",
      "Loss after mini batch    25: 11.672\n",
      "Loss after mini batch    25: 12.483\n",
      "Loss after mini batch    25: 13.892\n",
      "Loss after mini batch    25: 13.023\n",
      "Loss after mini batch    25: 12.770\n",
      "Starting epoch 26\n",
      "Loss after mini batch    26: 12.764\n",
      "Loss after mini batch    26: 13.822\n",
      "Loss after mini batch    26: 13.739\n",
      "Loss after mini batch    26: 12.655\n",
      "Loss after mini batch    26: 13.973\n",
      "Loss after mini batch    26: 12.348\n",
      "Loss after mini batch    26: 12.564\n",
      "Loss after mini batch    26: 14.011\n",
      "Loss after mini batch    26: 13.667\n",
      "Loss after mini batch    26: 13.454\n",
      "Loss after mini batch    26: 12.601\n",
      "Starting epoch 27\n",
      "Loss after mini batch    27: 13.035\n",
      "Loss after mini batch    27: 13.134\n",
      "Loss after mini batch    27: 12.228\n",
      "Loss after mini batch    27: 13.030\n",
      "Loss after mini batch    27: 13.256\n",
      "Loss after mini batch    27: 12.686\n",
      "Loss after mini batch    27: 13.915\n",
      "Loss after mini batch    27: 12.352\n",
      "Loss after mini batch    27: 12.777\n",
      "Loss after mini batch    27: 13.238\n",
      "Loss after mini batch    27: 13.516\n",
      "Starting epoch 28\n",
      "Loss after mini batch    28: 13.423\n",
      "Loss after mini batch    28: 14.159\n",
      "Loss after mini batch    28: 12.343\n",
      "Loss after mini batch    28: 13.598\n",
      "Loss after mini batch    28: 12.960\n",
      "Loss after mini batch    28: 12.970\n",
      "Loss after mini batch    28: 12.012\n",
      "Loss after mini batch    28: 13.149\n",
      "Loss after mini batch    28: 14.948\n",
      "Loss after mini batch    28: 11.819\n",
      "Loss after mini batch    28: 12.567\n",
      "Starting epoch 29\n",
      "Loss after mini batch    29: 12.697\n",
      "Loss after mini batch    29: 13.470\n",
      "Loss after mini batch    29: 12.656\n",
      "Loss after mini batch    29: 11.932\n",
      "Loss after mini batch    29: 12.689\n",
      "Loss after mini batch    29: 15.003\n",
      "Loss after mini batch    29: 14.979\n",
      "Loss after mini batch    29: 12.712\n",
      "Loss after mini batch    29: 13.354\n",
      "Loss after mini batch    29: 12.625\n",
      "Loss after mini batch    29: 13.347\n",
      "Starting epoch 30\n",
      "Loss after mini batch    30: 12.041\n",
      "Loss after mini batch    30: 13.871\n",
      "Loss after mini batch    30: 14.280\n",
      "Loss after mini batch    30: 14.009\n",
      "Loss after mini batch    30: 11.827\n",
      "Loss after mini batch    30: 11.963\n",
      "Loss after mini batch    30: 13.512\n",
      "Loss after mini batch    30: 13.356\n",
      "Loss after mini batch    30: 11.954\n",
      "Loss after mini batch    30: 12.719\n",
      "Loss after mini batch    30: 13.249\n",
      "Starting epoch 31\n",
      "Loss after mini batch    31: 12.568\n",
      "Loss after mini batch    31: 13.907\n",
      "Loss after mini batch    31: 14.169\n",
      "Loss after mini batch    31: 12.138\n",
      "Loss after mini batch    31: 12.474\n",
      "Loss after mini batch    31: 13.933\n",
      "Loss after mini batch    31: 11.929\n",
      "Loss after mini batch    31: 13.579\n",
      "Loss after mini batch    31: 12.948\n",
      "Loss after mini batch    31: 12.865\n",
      "Loss after mini batch    31: 12.312\n",
      "Starting epoch 32\n",
      "Loss after mini batch    32: 11.567\n",
      "Loss after mini batch    32: 12.021\n",
      "Loss after mini batch    32: 15.376\n",
      "Loss after mini batch    32: 12.781\n",
      "Loss after mini batch    32: 11.634\n",
      "Loss after mini batch    32: 11.637\n",
      "Loss after mini batch    32: 13.607\n",
      "Loss after mini batch    32: 13.429\n",
      "Loss after mini batch    32: 12.053\n",
      "Loss after mini batch    32: 12.875\n",
      "Loss after mini batch    32: 15.116\n",
      "Starting epoch 33\n",
      "Loss after mini batch    33: 11.638\n",
      "Loss after mini batch    33: 13.224\n",
      "Loss after mini batch    33: 13.444\n",
      "Loss after mini batch    33: 13.151\n",
      "Loss after mini batch    33: 12.105\n",
      "Loss after mini batch    33: 12.297\n",
      "Loss after mini batch    33: 15.988\n",
      "Loss after mini batch    33: 13.194\n",
      "Loss after mini batch    33: 12.564\n",
      "Loss after mini batch    33: 12.627\n",
      "Loss after mini batch    33: 13.174\n",
      "Starting epoch 34\n",
      "Loss after mini batch    34: 13.362\n",
      "Loss after mini batch    34: 12.817\n",
      "Loss after mini batch    34: 12.676\n",
      "Loss after mini batch    34: 12.827\n",
      "Loss after mini batch    34: 13.498\n",
      "Loss after mini batch    34: 12.789\n",
      "Loss after mini batch    34: 12.073\n",
      "Loss after mini batch    34: 12.317\n",
      "Loss after mini batch    34: 11.044\n",
      "Loss after mini batch    34: 15.908\n",
      "Loss after mini batch    34: 15.881\n",
      "Starting epoch 35\n",
      "Loss after mini batch    35: 13.818\n",
      "Loss after mini batch    35: 13.579\n",
      "Loss after mini batch    35: 13.130\n",
      "Loss after mini batch    35: 12.699\n",
      "Loss after mini batch    35: 11.678\n",
      "Loss after mini batch    35: 12.067\n",
      "Loss after mini batch    35: 13.592\n",
      "Loss after mini batch    35: 12.581\n",
      "Loss after mini batch    35: 12.685\n",
      "Loss after mini batch    35: 13.463\n",
      "Loss after mini batch    35: 13.206\n",
      "Starting epoch 36\n",
      "Loss after mini batch    36: 12.789\n",
      "Loss after mini batch    36: 14.591\n",
      "Loss after mini batch    36: 12.875\n",
      "Loss after mini batch    36: 15.055\n",
      "Loss after mini batch    36: 13.356\n",
      "Loss after mini batch    36: 12.688\n",
      "Loss after mini batch    36: 11.885\n",
      "Loss after mini batch    36: 11.975\n",
      "Loss after mini batch    36: 12.989\n",
      "Loss after mini batch    36: 13.172\n",
      "Loss after mini batch    36: 12.720\n",
      "Starting epoch 37\n",
      "Loss after mini batch    37: 12.880\n",
      "Loss after mini batch    37: 13.854\n",
      "Loss after mini batch    37: 12.032\n",
      "Loss after mini batch    37: 13.024\n",
      "Loss after mini batch    37: 11.473\n",
      "Loss after mini batch    37: 12.792\n",
      "Loss after mini batch    37: 13.640\n",
      "Loss after mini batch    37: 13.738\n",
      "Loss after mini batch    37: 13.692\n",
      "Loss after mini batch    37: 12.406\n",
      "Loss after mini batch    37: 13.542\n",
      "Starting epoch 38\n",
      "Loss after mini batch    38: 12.254\n",
      "Loss after mini batch    38: 13.228\n",
      "Loss after mini batch    38: 13.898\n",
      "Loss after mini batch    38: 13.199\n",
      "Loss after mini batch    38: 12.383\n",
      "Loss after mini batch    38: 13.708\n",
      "Loss after mini batch    38: 12.806\n",
      "Loss after mini batch    38: 13.987\n",
      "Loss after mini batch    38: 13.727\n",
      "Loss after mini batch    38: 11.035\n",
      "Loss after mini batch    38: 12.923\n",
      "Starting epoch 39\n",
      "Loss after mini batch    39: 12.693\n",
      "Loss after mini batch    39: 14.426\n",
      "Loss after mini batch    39: 12.301\n",
      "Loss after mini batch    39: 12.503\n",
      "Loss after mini batch    39: 13.337\n",
      "Loss after mini batch    39: 13.116\n",
      "Loss after mini batch    39: 11.844\n",
      "Loss after mini batch    39: 13.804\n",
      "Loss after mini batch    39: 13.583\n",
      "Loss after mini batch    39: 14.010\n",
      "Loss after mini batch    39: 11.682\n",
      "Starting epoch 40\n",
      "Loss after mini batch    40: 11.779\n",
      "Loss after mini batch    40: 12.018\n",
      "Loss after mini batch    40: 12.124\n",
      "Loss after mini batch    40: 14.527\n",
      "Loss after mini batch    40: 13.842\n",
      "Loss after mini batch    40: 12.463\n",
      "Loss after mini batch    40: 12.182\n",
      "Loss after mini batch    40: 12.922\n",
      "Loss after mini batch    40: 13.645\n",
      "Loss after mini batch    40: 12.898\n",
      "Loss after mini batch    40: 11.936\n",
      "Starting epoch 41\n",
      "Loss after mini batch    41: 13.019\n",
      "Loss after mini batch    41: 13.769\n",
      "Loss after mini batch    41: 13.818\n",
      "Loss after mini batch    41: 11.881\n",
      "Loss after mini batch    41: 12.152\n",
      "Loss after mini batch    41: 12.955\n",
      "Loss after mini batch    41: 12.824\n",
      "Loss after mini batch    41: 13.510\n",
      "Loss after mini batch    41: 13.474\n",
      "Loss after mini batch    41: 12.545\n",
      "Loss after mini batch    41: 11.471\n",
      "Starting epoch 42\n",
      "Loss after mini batch    42: 12.599\n",
      "Loss after mini batch    42: 13.269\n",
      "Loss after mini batch    42: 12.989\n",
      "Loss after mini batch    42: 13.284\n",
      "Loss after mini batch    42: 12.076\n",
      "Loss after mini batch    42: 12.800\n",
      "Loss after mini batch    42: 13.599\n",
      "Loss after mini batch    42: 12.793\n",
      "Loss after mini batch    42: 12.168\n",
      "Loss after mini batch    42: 12.738\n",
      "Loss after mini batch    42: 14.377\n",
      "Starting epoch 43\n",
      "Loss after mini batch    43: 13.471\n",
      "Loss after mini batch    43: 13.289\n",
      "Loss after mini batch    43: 12.713\n",
      "Loss after mini batch    43: 13.211\n",
      "Loss after mini batch    43: 13.019\n",
      "Loss after mini batch    43: 12.619\n",
      "Loss after mini batch    43: 12.691\n",
      "Loss after mini batch    43: 12.933\n",
      "Loss after mini batch    43: 13.314\n",
      "Loss after mini batch    43: 13.456\n",
      "Loss after mini batch    43: 11.852\n",
      "Starting epoch 44\n",
      "Loss after mini batch    44: 13.594\n",
      "Loss after mini batch    44: 13.039\n",
      "Loss after mini batch    44: 13.519\n",
      "Loss after mini batch    44: 13.044\n",
      "Loss after mini batch    44: 12.772\n",
      "Loss after mini batch    44: 13.532\n",
      "Loss after mini batch    44: 12.764\n",
      "Loss after mini batch    44: 13.149\n",
      "Loss after mini batch    44: 12.073\n",
      "Loss after mini batch    44: 13.218\n",
      "Loss after mini batch    44: 10.975\n",
      "Starting epoch 45\n",
      "Loss after mini batch    45: 12.466\n",
      "Loss after mini batch    45: 13.760\n",
      "Loss after mini batch    45: 11.975\n",
      "Loss after mini batch    45: 13.195\n",
      "Loss after mini batch    45: 13.040\n",
      "Loss after mini batch    45: 11.950\n",
      "Loss after mini batch    45: 13.657\n",
      "Loss after mini batch    45: 12.186\n",
      "Loss after mini batch    45: 13.351\n",
      "Loss after mini batch    45: 12.641\n",
      "Loss after mini batch    45: 13.223\n",
      "Starting epoch 46\n",
      "Loss after mini batch    46: 12.999\n",
      "Loss after mini batch    46: 14.994\n",
      "Loss after mini batch    46: 13.769\n",
      "Loss after mini batch    46: 13.977\n",
      "Loss after mini batch    46: 13.217\n",
      "Loss after mini batch    46: 13.316\n",
      "Loss after mini batch    46: 13.056\n",
      "Loss after mini batch    46: 13.572\n",
      "Loss after mini batch    46: 12.891\n",
      "Loss after mini batch    46: 12.449\n",
      "Loss after mini batch    46: 13.231\n",
      "Starting epoch 47\n",
      "Loss after mini batch    47: 13.896\n",
      "Loss after mini batch    47: 15.110\n",
      "Loss after mini batch    47: 14.328\n",
      "Loss after mini batch    47: 14.159\n",
      "Loss after mini batch    47: 12.326\n",
      "Loss after mini batch    47: 12.624\n",
      "Loss after mini batch    47: 12.560\n",
      "Loss after mini batch    47: 13.296\n",
      "Loss after mini batch    47: 12.232\n",
      "Loss after mini batch    47: 11.648\n",
      "Loss after mini batch    47: 12.577\n",
      "Starting epoch 48\n",
      "Loss after mini batch    48: 13.199\n",
      "Loss after mini batch    48: 12.447\n",
      "Loss after mini batch    48: 12.260\n",
      "Loss after mini batch    48: 13.250\n",
      "Loss after mini batch    48: 13.013\n",
      "Loss after mini batch    48: 13.957\n",
      "Loss after mini batch    48: 12.737\n",
      "Loss after mini batch    48: 12.357\n",
      "Loss after mini batch    48: 13.099\n",
      "Loss after mini batch    48: 12.704\n",
      "Loss after mini batch    48: 11.741\n",
      "Starting epoch 49\n",
      "Loss after mini batch    49: 13.765\n",
      "Loss after mini batch    49: 13.380\n",
      "Loss after mini batch    49: 11.764\n",
      "Loss after mini batch    49: 12.373\n",
      "Loss after mini batch    49: 13.381\n",
      "Loss after mini batch    49: 11.944\n",
      "Loss after mini batch    49: 14.545\n",
      "Loss after mini batch    49: 12.755\n",
      "Loss after mini batch    49: 13.212\n",
      "Loss after mini batch    49: 12.171\n",
      "Loss after mini batch    49: 13.108\n",
      "Starting epoch 50\n",
      "Loss after mini batch    50: 12.316\n",
      "Loss after mini batch    50: 13.534\n",
      "Loss after mini batch    50: 14.034\n",
      "Loss after mini batch    50: 11.632\n",
      "Loss after mini batch    50: 12.978\n",
      "Loss after mini batch    50: 12.605\n",
      "Loss after mini batch    50: 12.592\n",
      "Loss after mini batch    50: 12.493\n",
      "Loss after mini batch    50: 14.289\n",
      "Loss after mini batch    50: 14.040\n",
      "Loss after mini batch    50: 12.104\n",
      "Starting epoch 51\n",
      "Loss after mini batch    51: 12.866\n",
      "Loss after mini batch    51: 12.547\n",
      "Loss after mini batch    51: 12.396\n",
      "Loss after mini batch    51: 13.854\n",
      "Loss after mini batch    51: 13.738\n",
      "Loss after mini batch    51: 12.128\n",
      "Loss after mini batch    51: 11.864\n",
      "Loss after mini batch    51: 12.456\n",
      "Loss after mini batch    51: 12.908\n",
      "Loss after mini batch    51: 12.865\n",
      "Loss after mini batch    51: 13.164\n",
      "Starting epoch 52\n",
      "Loss after mini batch    52: 14.536\n",
      "Loss after mini batch    52: 12.027\n",
      "Loss after mini batch    52: 13.329\n",
      "Loss after mini batch    52: 12.770\n",
      "Loss after mini batch    52: 12.172\n",
      "Loss after mini batch    52: 13.637\n",
      "Loss after mini batch    52: 12.687\n",
      "Loss after mini batch    52: 11.492\n",
      "Loss after mini batch    52: 12.191\n",
      "Loss after mini batch    52: 12.059\n",
      "Loss after mini batch    52: 12.110\n",
      "Starting epoch 53\n",
      "Loss after mini batch    53: 12.358\n",
      "Loss after mini batch    53: 12.729\n",
      "Loss after mini batch    53: 11.881\n",
      "Loss after mini batch    53: 11.470\n",
      "Loss after mini batch    53: 12.154\n",
      "Loss after mini batch    53: 12.641\n",
      "Loss after mini batch    53: 12.220\n",
      "Loss after mini batch    53: 12.763\n",
      "Loss after mini batch    53: 13.588\n",
      "Loss after mini batch    53: 14.771\n",
      "Loss after mini batch    53: 13.278\n",
      "Starting epoch 54\n",
      "Loss after mini batch    54: 13.232\n",
      "Loss after mini batch    54: 12.033\n",
      "Loss after mini batch    54: 12.973\n",
      "Loss after mini batch    54: 12.603\n",
      "Loss after mini batch    54: 12.248\n",
      "Loss after mini batch    54: 13.727\n",
      "Loss after mini batch    54: 12.227\n",
      "Loss after mini batch    54: 13.418\n",
      "Loss after mini batch    54: 11.832\n",
      "Loss after mini batch    54: 12.077\n",
      "Loss after mini batch    54: 12.831\n",
      "Starting epoch 55\n",
      "Loss after mini batch    55: 13.913\n",
      "Loss after mini batch    55: 13.154\n",
      "Loss after mini batch    55: 12.032\n",
      "Loss after mini batch    55: 13.242\n",
      "Loss after mini batch    55: 12.653\n",
      "Loss after mini batch    55: 11.821\n",
      "Loss after mini batch    55: 12.141\n",
      "Loss after mini batch    55: 14.500\n",
      "Loss after mini batch    55: 13.442\n",
      "Loss after mini batch    55: 12.100\n",
      "Loss after mini batch    55: 12.562\n",
      "Starting epoch 56\n",
      "Loss after mini batch    56: 13.323\n",
      "Loss after mini batch    56: 13.856\n",
      "Loss after mini batch    56: 13.508\n",
      "Loss after mini batch    56: 11.959\n",
      "Loss after mini batch    56: 12.328\n",
      "Loss after mini batch    56: 11.917\n",
      "Loss after mini batch    56: 13.207\n",
      "Loss after mini batch    56: 12.579\n",
      "Loss after mini batch    56: 14.213\n",
      "Loss after mini batch    56: 11.124\n",
      "Loss after mini batch    56: 14.459\n",
      "Starting epoch 57\n",
      "Loss after mini batch    57: 12.884\n",
      "Loss after mini batch    57: 12.172\n",
      "Loss after mini batch    57: 13.112\n",
      "Loss after mini batch    57: 12.400\n",
      "Loss after mini batch    57: 12.460\n",
      "Loss after mini batch    57: 12.927\n",
      "Loss after mini batch    57: 13.106\n",
      "Loss after mini batch    57: 12.784\n",
      "Loss after mini batch    57: 12.827\n",
      "Loss after mini batch    57: 12.727\n",
      "Loss after mini batch    57: 13.167\n",
      "Starting epoch 58\n",
      "Loss after mini batch    58: 12.613\n",
      "Loss after mini batch    58: 12.926\n",
      "Loss after mini batch    58: 12.530\n",
      "Loss after mini batch    58: 11.860\n",
      "Loss after mini batch    58: 13.789\n",
      "Loss after mini batch    58: 13.146\n",
      "Loss after mini batch    58: 12.197\n",
      "Loss after mini batch    58: 13.353\n",
      "Loss after mini batch    58: 12.967\n",
      "Loss after mini batch    58: 12.600\n",
      "Loss after mini batch    58: 11.723\n",
      "Starting epoch 59\n",
      "Loss after mini batch    59: 13.409\n",
      "Loss after mini batch    59: 12.827\n",
      "Loss after mini batch    59: 12.254\n",
      "Loss after mini batch    59: 12.118\n",
      "Loss after mini batch    59: 13.370\n",
      "Loss after mini batch    59: 13.742\n",
      "Loss after mini batch    59: 13.425\n",
      "Loss after mini batch    59: 11.940\n",
      "Loss after mini batch    59: 11.873\n",
      "Loss after mini batch    59: 13.651\n",
      "Loss after mini batch    59: 12.245\n",
      "Starting epoch 60\n",
      "Loss after mini batch    60: 12.400\n",
      "Loss after mini batch    60: 13.286\n",
      "Loss after mini batch    60: 11.827\n",
      "Loss after mini batch    60: 13.167\n",
      "Loss after mini batch    60: 14.382\n",
      "Loss after mini batch    60: 14.152\n",
      "Loss after mini batch    60: 12.977\n",
      "Loss after mini batch    60: 12.468\n",
      "Loss after mini batch    60: 11.405\n",
      "Loss after mini batch    60: 12.124\n",
      "Loss after mini batch    60: 12.920\n",
      "Starting epoch 61\n",
      "Loss after mini batch    61: 13.144\n",
      "Loss after mini batch    61: 12.442\n",
      "Loss after mini batch    61: 12.431\n",
      "Loss after mini batch    61: 12.505\n",
      "Loss after mini batch    61: 12.426\n",
      "Loss after mini batch    61: 13.373\n",
      "Loss after mini batch    61: 13.301\n",
      "Loss after mini batch    61: 11.737\n",
      "Loss after mini batch    61: 11.975\n",
      "Loss after mini batch    61: 13.951\n",
      "Loss after mini batch    61: 11.883\n",
      "Starting epoch 62\n",
      "Loss after mini batch    62: 11.881\n",
      "Loss after mini batch    62: 13.231\n",
      "Loss after mini batch    62: 12.128\n",
      "Loss after mini batch    62: 12.776\n",
      "Loss after mini batch    62: 12.316\n",
      "Loss after mini batch    62: 12.483\n",
      "Loss after mini batch    62: 12.894\n",
      "Loss after mini batch    62: 11.759\n",
      "Loss after mini batch    62: 13.355\n",
      "Loss after mini batch    62: 13.023\n",
      "Loss after mini batch    62: 11.884\n",
      "Starting epoch 63\n",
      "Loss after mini batch    63: 13.066\n",
      "Loss after mini batch    63: 11.972\n",
      "Loss after mini batch    63: 13.105\n",
      "Loss after mini batch    63: 13.025\n",
      "Loss after mini batch    63: 12.530\n",
      "Loss after mini batch    63: 11.795\n",
      "Loss after mini batch    63: 13.287\n",
      "Loss after mini batch    63: 11.889\n",
      "Loss after mini batch    63: 12.358\n",
      "Loss after mini batch    63: 12.250\n",
      "Loss after mini batch    63: 13.081\n",
      "Starting epoch 64\n",
      "Loss after mini batch    64: 12.610\n",
      "Loss after mini batch    64: 12.790\n",
      "Loss after mini batch    64: 13.017\n",
      "Loss after mini batch    64: 12.465\n",
      "Loss after mini batch    64: 11.632\n",
      "Loss after mini batch    64: 12.067\n",
      "Loss after mini batch    64: 12.169\n",
      "Loss after mini batch    64: 12.693\n",
      "Loss after mini batch    64: 12.527\n",
      "Loss after mini batch    64: 12.812\n",
      "Loss after mini batch    64: 13.096\n",
      "Starting epoch 65\n",
      "Loss after mini batch    65: 13.804\n",
      "Loss after mini batch    65: 13.381\n",
      "Loss after mini batch    65: 13.058\n",
      "Loss after mini batch    65: 11.675\n",
      "Loss after mini batch    65: 12.886\n",
      "Loss after mini batch    65: 12.766\n",
      "Loss after mini batch    65: 11.810\n",
      "Loss after mini batch    65: 12.041\n",
      "Loss after mini batch    65: 13.016\n",
      "Loss after mini batch    65: 12.719\n",
      "Loss after mini batch    65: 12.679\n",
      "Starting epoch 66\n",
      "Loss after mini batch    66: 12.229\n",
      "Loss after mini batch    66: 11.234\n",
      "Loss after mini batch    66: 12.274\n",
      "Loss after mini batch    66: 13.307\n",
      "Loss after mini batch    66: 14.573\n",
      "Loss after mini batch    66: 12.502\n",
      "Loss after mini batch    66: 12.718\n",
      "Loss after mini batch    66: 13.150\n",
      "Loss after mini batch    66: 12.645\n",
      "Loss after mini batch    66: 11.502\n",
      "Loss after mini batch    66: 12.872\n",
      "Starting epoch 67\n",
      "Loss after mini batch    67: 12.613\n",
      "Loss after mini batch    67: 12.487\n",
      "Loss after mini batch    67: 13.662\n",
      "Loss after mini batch    67: 13.629\n",
      "Loss after mini batch    67: 13.021\n",
      "Loss after mini batch    67: 10.860\n",
      "Loss after mini batch    67: 12.863\n",
      "Loss after mini batch    67: 12.691\n",
      "Loss after mini batch    67: 12.310\n",
      "Loss after mini batch    67: 12.048\n",
      "Loss after mini batch    67: 12.527\n",
      "Starting epoch 68\n",
      "Loss after mini batch    68: 12.794\n",
      "Loss after mini batch    68: 12.820\n",
      "Loss after mini batch    68: 11.668\n",
      "Loss after mini batch    68: 13.587\n",
      "Loss after mini batch    68: 14.098\n",
      "Loss after mini batch    68: 12.201\n",
      "Loss after mini batch    68: 12.024\n",
      "Loss after mini batch    68: 13.110\n",
      "Loss after mini batch    68: 12.912\n",
      "Loss after mini batch    68: 12.236\n",
      "Loss after mini batch    68: 12.314\n",
      "Starting epoch 69\n",
      "Loss after mini batch    69: 12.735\n",
      "Loss after mini batch    69: 12.482\n",
      "Loss after mini batch    69: 12.464\n",
      "Loss after mini batch    69: 11.516\n",
      "Loss after mini batch    69: 13.983\n",
      "Loss after mini batch    69: 13.679\n",
      "Loss after mini batch    69: 11.646\n",
      "Loss after mini batch    69: 12.519\n",
      "Loss after mini batch    69: 11.424\n",
      "Loss after mini batch    69: 14.268\n",
      "Loss after mini batch    69: 11.792\n",
      "Starting epoch 70\n",
      "Loss after mini batch    70: 11.819\n",
      "Loss after mini batch    70: 12.029\n",
      "Loss after mini batch    70: 11.723\n",
      "Loss after mini batch    70: 11.401\n",
      "Loss after mini batch    70: 13.285\n",
      "Loss after mini batch    70: 12.628\n",
      "Loss after mini batch    70: 12.159\n",
      "Loss after mini batch    70: 13.549\n",
      "Loss after mini batch    70: 12.465\n",
      "Loss after mini batch    70: 13.643\n",
      "Loss after mini batch    70: 11.963\n",
      "Starting epoch 71\n",
      "Loss after mini batch    71: 13.566\n",
      "Loss after mini batch    71: 12.282\n",
      "Loss after mini batch    71: 12.664\n",
      "Loss after mini batch    71: 12.484\n",
      "Loss after mini batch    71: 12.518\n",
      "Loss after mini batch    71: 12.828\n",
      "Loss after mini batch    71: 13.194\n",
      "Loss after mini batch    71: 12.029\n",
      "Loss after mini batch    71: 11.632\n",
      "Loss after mini batch    71: 12.778\n",
      "Loss after mini batch    71: 12.194\n",
      "Starting epoch 72\n",
      "Loss after mini batch    72: 13.205\n",
      "Loss after mini batch    72: 13.601\n",
      "Loss after mini batch    72: 12.961\n",
      "Loss after mini batch    72: 12.494\n",
      "Loss after mini batch    72: 12.735\n",
      "Loss after mini batch    72: 13.305\n",
      "Loss after mini batch    72: 13.063\n",
      "Loss after mini batch    72: 12.153\n",
      "Loss after mini batch    72: 12.288\n",
      "Loss after mini batch    72: 11.695\n",
      "Loss after mini batch    72: 12.436\n",
      "Starting epoch 73\n",
      "Loss after mini batch    73: 13.221\n",
      "Loss after mini batch    73: 11.057\n",
      "Loss after mini batch    73: 12.174\n",
      "Loss after mini batch    73: 13.228\n",
      "Loss after mini batch    73: 12.685\n",
      "Loss after mini batch    73: 13.039\n",
      "Loss after mini batch    73: 11.869\n",
      "Loss after mini batch    73: 12.494\n",
      "Loss after mini batch    73: 13.307\n",
      "Loss after mini batch    73: 11.851\n",
      "Loss after mini batch    73: 12.607\n",
      "Starting epoch 74\n",
      "Loss after mini batch    74: 12.581\n",
      "Loss after mini batch    74: 12.278\n",
      "Loss after mini batch    74: 12.283\n",
      "Loss after mini batch    74: 12.195\n",
      "Loss after mini batch    74: 12.634\n",
      "Loss after mini batch    74: 11.911\n",
      "Loss after mini batch    74: 12.098\n",
      "Loss after mini batch    74: 12.570\n",
      "Loss after mini batch    74: 12.114\n",
      "Loss after mini batch    74: 13.493\n",
      "Loss after mini batch    74: 13.245\n",
      "Starting epoch 75\n",
      "Loss after mini batch    75: 14.018\n",
      "Loss after mini batch    75: 13.002\n",
      "Loss after mini batch    75: 12.969\n",
      "Loss after mini batch    75: 11.702\n",
      "Loss after mini batch    75: 11.501\n",
      "Loss after mini batch    75: 14.003\n",
      "Loss after mini batch    75: 12.178\n",
      "Loss after mini batch    75: 12.161\n",
      "Loss after mini batch    75: 12.347\n",
      "Loss after mini batch    75: 12.246\n",
      "Loss after mini batch    75: 13.179\n",
      "Starting epoch 76\n",
      "Loss after mini batch    76: 13.330\n",
      "Loss after mini batch    76: 11.747\n",
      "Loss after mini batch    76: 10.942\n",
      "Loss after mini batch    76: 12.233\n",
      "Loss after mini batch    76: 12.113\n",
      "Loss after mini batch    76: 11.781\n",
      "Loss after mini batch    76: 12.376\n",
      "Loss after mini batch    76: 12.963\n",
      "Loss after mini batch    76: 12.071\n",
      "Loss after mini batch    76: 12.480\n",
      "Loss after mini batch    76: 12.303\n",
      "Starting epoch 77\n",
      "Loss after mini batch    77: 11.577\n",
      "Loss after mini batch    77: 11.341\n",
      "Loss after mini batch    77: 12.591\n",
      "Loss after mini batch    77: 11.830\n",
      "Loss after mini batch    77: 13.061\n",
      "Loss after mini batch    77: 11.862\n",
      "Loss after mini batch    77: 13.296\n",
      "Loss after mini batch    77: 14.303\n",
      "Loss after mini batch    77: 12.709\n",
      "Loss after mini batch    77: 12.953\n",
      "Loss after mini batch    77: 12.350\n",
      "Starting epoch 78\n",
      "Loss after mini batch    78: 13.030\n",
      "Loss after mini batch    78: 13.460\n",
      "Loss after mini batch    78: 13.826\n",
      "Loss after mini batch    78: 12.038\n",
      "Loss after mini batch    78: 14.675\n",
      "Loss after mini batch    78: 10.182\n",
      "Loss after mini batch    78: 11.790\n",
      "Loss after mini batch    78: 12.920\n",
      "Loss after mini batch    78: 12.495\n",
      "Loss after mini batch    78: 13.691\n",
      "Loss after mini batch    78: 12.887\n",
      "Starting epoch 79\n",
      "Loss after mini batch    79: 13.147\n",
      "Loss after mini batch    79: 13.234\n",
      "Loss after mini batch    79: 12.071\n",
      "Loss after mini batch    79: 11.675\n",
      "Loss after mini batch    79: 11.901\n",
      "Loss after mini batch    79: 12.336\n",
      "Loss after mini batch    79: 13.377\n",
      "Loss after mini batch    79: 13.039\n",
      "Loss after mini batch    79: 12.155\n",
      "Loss after mini batch    79: 11.010\n",
      "Loss after mini batch    79: 12.822\n",
      "Starting epoch 80\n",
      "Loss after mini batch    80: 12.187\n",
      "Loss after mini batch    80: 12.406\n",
      "Loss after mini batch    80: 12.719\n",
      "Loss after mini batch    80: 12.912\n",
      "Loss after mini batch    80: 11.723\n",
      "Loss after mini batch    80: 11.424\n",
      "Loss after mini batch    80: 12.599\n",
      "Loss after mini batch    80: 12.288\n",
      "Loss after mini batch    80: 11.995\n",
      "Loss after mini batch    80: 13.035\n",
      "Loss after mini batch    80: 12.661\n",
      "Starting epoch 81\n",
      "Loss after mini batch    81: 13.421\n",
      "Loss after mini batch    81: 14.034\n",
      "Loss after mini batch    81: 11.736\n",
      "Loss after mini batch    81: 13.029\n",
      "Loss after mini batch    81: 12.269\n",
      "Loss after mini batch    81: 13.358\n",
      "Loss after mini batch    81: 11.505\n",
      "Loss after mini batch    81: 10.653\n",
      "Loss after mini batch    81: 11.402\n",
      "Loss after mini batch    81: 11.988\n",
      "Loss after mini batch    81: 11.580\n",
      "Starting epoch 82\n",
      "Loss after mini batch    82: 12.868\n",
      "Loss after mini batch    82: 12.316\n",
      "Loss after mini batch    82: 12.710\n",
      "Loss after mini batch    82: 11.581\n",
      "Loss after mini batch    82: 13.586\n",
      "Loss after mini batch    82: 13.296\n",
      "Loss after mini batch    82: 12.781\n",
      "Loss after mini batch    82: 12.210\n",
      "Loss after mini batch    82: 12.896\n",
      "Loss after mini batch    82: 12.380\n",
      "Loss after mini batch    82: 12.333\n",
      "Starting epoch 83\n",
      "Loss after mini batch    83: 12.252\n",
      "Loss after mini batch    83: 12.595\n",
      "Loss after mini batch    83: 11.883\n",
      "Loss after mini batch    83: 11.386\n",
      "Loss after mini batch    83: 11.860\n",
      "Loss after mini batch    83: 13.214\n",
      "Loss after mini batch    83: 13.186\n",
      "Loss after mini batch    83: 12.097\n",
      "Loss after mini batch    83: 13.096\n",
      "Loss after mini batch    83: 11.964\n",
      "Loss after mini batch    83: 12.247\n",
      "Starting epoch 84\n",
      "Loss after mini batch    84: 11.101\n",
      "Loss after mini batch    84: 12.567\n",
      "Loss after mini batch    84: 13.069\n",
      "Loss after mini batch    84: 12.237\n",
      "Loss after mini batch    84: 12.065\n",
      "Loss after mini batch    84: 12.663\n",
      "Loss after mini batch    84: 11.407\n",
      "Loss after mini batch    84: 13.880\n",
      "Loss after mini batch    84: 13.149\n",
      "Loss after mini batch    84: 11.754\n",
      "Loss after mini batch    84: 12.453\n",
      "Starting epoch 85\n",
      "Loss after mini batch    85: 11.182\n",
      "Loss after mini batch    85: 12.819\n",
      "Loss after mini batch    85: 12.654\n",
      "Loss after mini batch    85: 12.640\n",
      "Loss after mini batch    85: 12.200\n",
      "Loss after mini batch    85: 11.577\n",
      "Loss after mini batch    85: 11.683\n",
      "Loss after mini batch    85: 11.565\n",
      "Loss after mini batch    85: 11.114\n",
      "Loss after mini batch    85: 13.311\n",
      "Loss after mini batch    85: 12.747\n",
      "Starting epoch 86\n",
      "Loss after mini batch    86: 12.077\n",
      "Loss after mini batch    86: 13.217\n",
      "Loss after mini batch    86: 11.431\n",
      "Loss after mini batch    86: 11.786\n",
      "Loss after mini batch    86: 13.469\n",
      "Loss after mini batch    86: 11.451\n",
      "Loss after mini batch    86: 12.578\n",
      "Loss after mini batch    86: 12.613\n",
      "Loss after mini batch    86: 13.501\n",
      "Loss after mini batch    86: 12.955\n",
      "Loss after mini batch    86: 12.008\n",
      "Starting epoch 87\n",
      "Loss after mini batch    87: 12.767\n",
      "Loss after mini batch    87: 12.556\n",
      "Loss after mini batch    87: 12.925\n",
      "Loss after mini batch    87: 11.879\n",
      "Loss after mini batch    87: 12.554\n",
      "Loss after mini batch    87: 11.191\n",
      "Loss after mini batch    87: 12.026\n",
      "Loss after mini batch    87: 12.920\n",
      "Loss after mini batch    87: 13.251\n",
      "Loss after mini batch    87: 11.968\n",
      "Loss after mini batch    87: 12.544\n",
      "Starting epoch 88\n",
      "Loss after mini batch    88: 11.921\n",
      "Loss after mini batch    88: 12.286\n",
      "Loss after mini batch    88: 12.550\n",
      "Loss after mini batch    88: 12.795\n",
      "Loss after mini batch    88: 13.758\n",
      "Loss after mini batch    88: 11.152\n",
      "Loss after mini batch    88: 11.570\n",
      "Loss after mini batch    88: 12.260\n",
      "Loss after mini batch    88: 13.350\n",
      "Loss after mini batch    88: 13.036\n",
      "Loss after mini batch    88: 12.731\n",
      "Starting epoch 89\n",
      "Loss after mini batch    89: 12.629\n",
      "Loss after mini batch    89: 12.066\n",
      "Loss after mini batch    89: 12.736\n",
      "Loss after mini batch    89: 11.791\n",
      "Loss after mini batch    89: 12.682\n",
      "Loss after mini batch    89: 11.806\n",
      "Loss after mini batch    89: 10.954\n",
      "Loss after mini batch    89: 11.296\n",
      "Loss after mini batch    89: 13.240\n",
      "Loss after mini batch    89: 12.297\n",
      "Loss after mini batch    89: 12.580\n",
      "Starting epoch 90\n",
      "Loss after mini batch    90: 12.073\n",
      "Loss after mini batch    90: 11.193\n",
      "Loss after mini batch    90: 12.536\n",
      "Loss after mini batch    90: 12.715\n",
      "Loss after mini batch    90: 12.636\n",
      "Loss after mini batch    90: 11.478\n",
      "Loss after mini batch    90: 11.439\n",
      "Loss after mini batch    90: 12.513\n",
      "Loss after mini batch    90: 12.966\n",
      "Loss after mini batch    90: 12.851\n",
      "Loss after mini batch    90: 13.386\n",
      "Starting epoch 91\n",
      "Loss after mini batch    91: 13.600\n",
      "Loss after mini batch    91: 11.910\n",
      "Loss after mini batch    91: 12.937\n",
      "Loss after mini batch    91: 10.829\n",
      "Loss after mini batch    91: 11.471\n",
      "Loss after mini batch    91: 11.761\n",
      "Loss after mini batch    91: 12.025\n",
      "Loss after mini batch    91: 12.618\n",
      "Loss after mini batch    91: 13.270\n",
      "Loss after mini batch    91: 12.921\n",
      "Loss after mini batch    91: 12.083\n",
      "Starting epoch 92\n",
      "Loss after mini batch    92: 11.772\n",
      "Loss after mini batch    92: 12.622\n",
      "Loss after mini batch    92: 12.453\n",
      "Loss after mini batch    92: 12.313\n",
      "Loss after mini batch    92: 10.703\n",
      "Loss after mini batch    92: 13.410\n",
      "Loss after mini batch    92: 11.382\n",
      "Loss after mini batch    92: 13.155\n",
      "Loss after mini batch    92: 12.141\n",
      "Loss after mini batch    92: 13.136\n",
      "Loss after mini batch    92: 12.857\n",
      "Starting epoch 93\n",
      "Loss after mini batch    93: 11.986\n",
      "Loss after mini batch    93: 12.252\n",
      "Loss after mini batch    93: 12.832\n",
      "Loss after mini batch    93: 12.352\n",
      "Loss after mini batch    93: 11.887\n",
      "Loss after mini batch    93: 12.895\n",
      "Loss after mini batch    93: 12.285\n",
      "Loss after mini batch    93: 12.842\n",
      "Loss after mini batch    93: 12.536\n",
      "Loss after mini batch    93: 12.124\n",
      "Loss after mini batch    93: 12.131\n",
      "Starting epoch 94\n",
      "Loss after mini batch    94: 12.366\n",
      "Loss after mini batch    94: 13.092\n",
      "Loss after mini batch    94: 14.249\n",
      "Loss after mini batch    94: 12.471\n",
      "Loss after mini batch    94: 11.255\n",
      "Loss after mini batch    94: 12.053\n",
      "Loss after mini batch    94: 11.470\n",
      "Loss after mini batch    94: 12.361\n",
      "Loss after mini batch    94: 11.600\n",
      "Loss after mini batch    94: 11.902\n",
      "Loss after mini batch    94: 11.939\n",
      "Starting epoch 95\n",
      "Loss after mini batch    95: 13.524\n",
      "Loss after mini batch    95: 13.164\n",
      "Loss after mini batch    95: 11.986\n",
      "Loss after mini batch    95: 12.173\n",
      "Loss after mini batch    95: 12.316\n",
      "Loss after mini batch    95: 11.115\n",
      "Loss after mini batch    95: 12.320\n",
      "Loss after mini batch    95: 12.844\n",
      "Loss after mini batch    95: 12.669\n",
      "Loss after mini batch    95: 12.121\n",
      "Loss after mini batch    95: 11.462\n",
      "Starting epoch 96\n",
      "Loss after mini batch    96: 11.188\n",
      "Loss after mini batch    96: 12.502\n",
      "Loss after mini batch    96: 11.973\n",
      "Loss after mini batch    96: 12.467\n",
      "Loss after mini batch    96: 13.035\n",
      "Loss after mini batch    96: 13.901\n",
      "Loss after mini batch    96: 12.195\n",
      "Loss after mini batch    96: 11.368\n",
      "Loss after mini batch    96: 12.722\n",
      "Loss after mini batch    96: 12.696\n",
      "Loss after mini batch    96: 12.858\n",
      "Starting epoch 97\n",
      "Loss after mini batch    97: 12.972\n",
      "Loss after mini batch    97: 11.363\n",
      "Loss after mini batch    97: 12.921\n",
      "Loss after mini batch    97: 12.518\n",
      "Loss after mini batch    97: 13.310\n",
      "Loss after mini batch    97: 11.854\n",
      "Loss after mini batch    97: 11.810\n",
      "Loss after mini batch    97: 12.380\n",
      "Loss after mini batch    97: 12.326\n",
      "Loss after mini batch    97: 11.184\n",
      "Loss after mini batch    97: 12.977\n",
      "Starting epoch 98\n",
      "Loss after mini batch    98: 11.879\n",
      "Loss after mini batch    98: 12.537\n",
      "Loss after mini batch    98: 12.473\n",
      "Loss after mini batch    98: 12.715\n",
      "Loss after mini batch    98: 11.778\n",
      "Loss after mini batch    98: 11.269\n",
      "Loss after mini batch    98: 11.954\n",
      "Loss after mini batch    98: 12.019\n",
      "Loss after mini batch    98: 12.556\n",
      "Loss after mini batch    98: 13.618\n",
      "Loss after mini batch    98: 12.177\n",
      "Starting epoch 99\n",
      "Loss after mini batch    99: 12.594\n",
      "Loss after mini batch    99: 12.999\n",
      "Loss after mini batch    99: 11.066\n",
      "Loss after mini batch    99: 12.260\n",
      "Loss after mini batch    99: 11.295\n",
      "Loss after mini batch    99: 12.936\n",
      "Loss after mini batch    99: 13.724\n",
      "Loss after mini batch    99: 10.907\n",
      "Loss after mini batch    99: 12.703\n",
      "Loss after mini batch    99: 12.098\n",
      "Loss after mini batch    99: 12.174\n",
      "Starting epoch 100\n",
      "Loss after mini batch   100: 12.975\n",
      "Loss after mini batch   100: 12.805\n",
      "Loss after mini batch   100: 11.393\n",
      "Loss after mini batch   100: 11.782\n",
      "Loss after mini batch   100: 12.783\n",
      "Loss after mini batch   100: 12.392\n",
      "Loss after mini batch   100: 13.580\n",
      "Loss after mini batch   100: 12.394\n",
      "Loss after mini batch   100: 12.199\n",
      "Loss after mini batch   100: 11.735\n",
      "Loss after mini batch   100: 12.034\n",
      "Training Complete\n",
      "Starting Testing\n",
      "RMSE for fold 7: 3.462193383918484\n",
      "rRMSE for fold 7: 0.06624725912131196\n",
      "r for fold 7: 0.990271679942037\n",
      "Fast RMSE for fold 7: 3.0127037011127134\n",
      "Fast rRMSE for fold 7: 0.05931310200377454\n",
      "Fast r for fold 7: 0.9853121173732748\n",
      "Slow RMSE for fold 7: 4.8435078221968055\n",
      "Slow rRMSE for fold 7: 0.08867038020039612\n",
      "Slow r for fold 7: 0.9980869707368251\n",
      "Regular RMSE for fold 7: 2.2061960446079247\n",
      "Regular rRMSE for fold 7: 0.04266779273248627\n",
      "Regular r for fold 7: 0.9973317167895518\n",
      "Fold 8\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini batch     1: 331.526\n",
      "Loss after mini batch     1: 16.277\n",
      "Loss after mini batch     1: 18.525\n",
      "Loss after mini batch     1: 24.918\n",
      "Loss after mini batch     1: 20.057\n",
      "Loss after mini batch     1: 21.145\n",
      "Loss after mini batch     1: 16.342\n",
      "Loss after mini batch     1: 26.061\n",
      "Loss after mini batch     1: 31.855\n",
      "Loss after mini batch     1: 27.037\n",
      "Loss after mini batch     1: 33.541\n",
      "Starting epoch 2\n",
      "Loss after mini batch     2: 20.798\n",
      "Loss after mini batch     2: 26.916\n",
      "Loss after mini batch     2: 18.294\n",
      "Loss after mini batch     2: 19.096\n",
      "Loss after mini batch     2: 15.476\n",
      "Loss after mini batch     2: 30.497\n",
      "Loss after mini batch     2: 39.215\n",
      "Loss after mini batch     2: 19.147\n",
      "Loss after mini batch     2: 18.151\n",
      "Loss after mini batch     2: 19.235\n",
      "Loss after mini batch     2: 17.833\n",
      "Starting epoch 3\n",
      "Loss after mini batch     3: 20.061\n",
      "Loss after mini batch     3: 47.645\n",
      "Loss after mini batch     3: 16.791\n",
      "Loss after mini batch     3: 16.095\n",
      "Loss after mini batch     3: 17.248\n",
      "Loss after mini batch     3: 17.720\n",
      "Loss after mini batch     3: 16.400\n",
      "Loss after mini batch     3: 14.970\n",
      "Loss after mini batch     3: 18.071\n",
      "Loss after mini batch     3: 16.681\n",
      "Loss after mini batch     3: 16.028\n",
      "Starting epoch 4\n",
      "Loss after mini batch     4: 15.196\n",
      "Loss after mini batch     4: 16.850\n",
      "Loss after mini batch     4: 17.668\n",
      "Loss after mini batch     4: 13.599\n",
      "Loss after mini batch     4: 16.629\n",
      "Loss after mini batch     4: 20.008\n",
      "Loss after mini batch     4: 14.934\n",
      "Loss after mini batch     4: 19.858\n",
      "Loss after mini batch     4: 15.167\n",
      "Loss after mini batch     4: 18.067\n",
      "Loss after mini batch     4: 16.653\n",
      "Starting epoch 5\n",
      "Loss after mini batch     5: 15.243\n",
      "Loss after mini batch     5: 16.671\n",
      "Loss after mini batch     5: 18.341\n",
      "Loss after mini batch     5: 15.814\n",
      "Loss after mini batch     5: 23.007\n",
      "Loss after mini batch     5: 14.501\n",
      "Loss after mini batch     5: 13.772\n",
      "Loss after mini batch     5: 14.719\n",
      "Loss after mini batch     5: 15.394\n",
      "Loss after mini batch     5: 17.322\n",
      "Loss after mini batch     5: 14.593\n",
      "Starting epoch 6\n",
      "Loss after mini batch     6: 15.919\n",
      "Loss after mini batch     6: 15.777\n",
      "Loss after mini batch     6: 19.733\n",
      "Loss after mini batch     6: 18.241\n",
      "Loss after mini batch     6: 17.198\n",
      "Loss after mini batch     6: 15.821\n",
      "Loss after mini batch     6: 15.199\n",
      "Loss after mini batch     6: 14.452\n",
      "Loss after mini batch     6: 16.247\n",
      "Loss after mini batch     6: 18.295\n",
      "Loss after mini batch     6: 17.874\n",
      "Starting epoch 7\n",
      "Loss after mini batch     7: 14.139\n",
      "Loss after mini batch     7: 14.520\n",
      "Loss after mini batch     7: 15.612\n",
      "Loss after mini batch     7: 18.566\n",
      "Loss after mini batch     7: 17.552\n",
      "Loss after mini batch     7: 17.329\n",
      "Loss after mini batch     7: 14.390\n",
      "Loss after mini batch     7: 15.865\n",
      "Loss after mini batch     7: 13.705\n",
      "Loss after mini batch     7: 15.187\n",
      "Loss after mini batch     7: 17.382\n",
      "Starting epoch 8\n",
      "Loss after mini batch     8: 16.224\n",
      "Loss after mini batch     8: 14.321\n",
      "Loss after mini batch     8: 15.410\n",
      "Loss after mini batch     8: 14.419\n",
      "Loss after mini batch     8: 13.844\n",
      "Loss after mini batch     8: 15.370\n",
      "Loss after mini batch     8: 14.922\n",
      "Loss after mini batch     8: 14.379\n",
      "Loss after mini batch     8: 13.401\n",
      "Loss after mini batch     8: 15.970\n",
      "Loss after mini batch     8: 14.681\n",
      "Starting epoch 9\n",
      "Loss after mini batch     9: 13.837\n",
      "Loss after mini batch     9: 13.503\n",
      "Loss after mini batch     9: 16.759\n",
      "Loss after mini batch     9: 14.824\n",
      "Loss after mini batch     9: 17.090\n",
      "Loss after mini batch     9: 14.197\n",
      "Loss after mini batch     9: 18.090\n",
      "Loss after mini batch     9: 13.636\n",
      "Loss after mini batch     9: 16.848\n",
      "Loss after mini batch     9: 16.999\n",
      "Loss after mini batch     9: 13.840\n",
      "Starting epoch 10\n",
      "Loss after mini batch    10: 14.520\n",
      "Loss after mini batch    10: 15.504\n",
      "Loss after mini batch    10: 14.752\n",
      "Loss after mini batch    10: 13.381\n",
      "Loss after mini batch    10: 13.382\n",
      "Loss after mini batch    10: 13.623\n",
      "Loss after mini batch    10: 15.132\n",
      "Loss after mini batch    10: 14.666\n",
      "Loss after mini batch    10: 14.982\n",
      "Loss after mini batch    10: 13.281\n",
      "Loss after mini batch    10: 13.518\n",
      "Starting epoch 11\n",
      "Loss after mini batch    11: 14.578\n",
      "Loss after mini batch    11: 16.424\n",
      "Loss after mini batch    11: 16.593\n",
      "Loss after mini batch    11: 14.391\n",
      "Loss after mini batch    11: 17.496\n",
      "Loss after mini batch    11: 14.483\n",
      "Loss after mini batch    11: 15.734\n",
      "Loss after mini batch    11: 13.911\n",
      "Loss after mini batch    11: 12.841\n",
      "Loss after mini batch    11: 14.075\n",
      "Loss after mini batch    11: 17.051\n",
      "Starting epoch 12\n",
      "Loss after mini batch    12: 13.872\n",
      "Loss after mini batch    12: 15.135\n",
      "Loss after mini batch    12: 14.576\n",
      "Loss after mini batch    12: 13.337\n",
      "Loss after mini batch    12: 12.361\n",
      "Loss after mini batch    12: 13.282\n",
      "Loss after mini batch    12: 15.976\n",
      "Loss after mini batch    12: 15.729\n",
      "Loss after mini batch    12: 13.316\n",
      "Loss after mini batch    12: 14.003\n",
      "Loss after mini batch    12: 14.075\n",
      "Starting epoch 13\n",
      "Loss after mini batch    13: 16.164\n",
      "Loss after mini batch    13: 14.558\n",
      "Loss after mini batch    13: 13.616\n",
      "Loss after mini batch    13: 14.038\n",
      "Loss after mini batch    13: 14.625\n",
      "Loss after mini batch    13: 13.708\n",
      "Loss after mini batch    13: 14.925\n",
      "Loss after mini batch    13: 13.429\n",
      "Loss after mini batch    13: 14.525\n",
      "Loss after mini batch    13: 13.411\n",
      "Loss after mini batch    13: 15.891\n",
      "Starting epoch 14\n",
      "Loss after mini batch    14: 15.084\n",
      "Loss after mini batch    14: 12.702\n",
      "Loss after mini batch    14: 13.597\n",
      "Loss after mini batch    14: 15.189\n",
      "Loss after mini batch    14: 14.291\n",
      "Loss after mini batch    14: 14.751\n",
      "Loss after mini batch    14: 17.963\n",
      "Loss after mini batch    14: 13.167\n",
      "Loss after mini batch    14: 12.580\n",
      "Loss after mini batch    14: 14.833\n",
      "Loss after mini batch    14: 14.147\n",
      "Starting epoch 15\n",
      "Loss after mini batch    15: 12.721\n",
      "Loss after mini batch    15: 12.861\n",
      "Loss after mini batch    15: 13.806\n",
      "Loss after mini batch    15: 12.939\n",
      "Loss after mini batch    15: 12.862\n",
      "Loss after mini batch    15: 14.864\n",
      "Loss after mini batch    15: 17.419\n",
      "Loss after mini batch    15: 15.893\n",
      "Loss after mini batch    15: 14.947\n",
      "Loss after mini batch    15: 13.348\n",
      "Loss after mini batch    15: 13.487\n",
      "Starting epoch 16\n",
      "Loss after mini batch    16: 15.358\n",
      "Loss after mini batch    16: 17.514\n",
      "Loss after mini batch    16: 13.420\n",
      "Loss after mini batch    16: 14.638\n",
      "Loss after mini batch    16: 14.164\n",
      "Loss after mini batch    16: 15.714\n",
      "Loss after mini batch    16: 13.417\n",
      "Loss after mini batch    16: 13.426\n",
      "Loss after mini batch    16: 13.807\n",
      "Loss after mini batch    16: 13.026\n",
      "Loss after mini batch    16: 15.785\n",
      "Starting epoch 17\n",
      "Loss after mini batch    17: 14.065\n",
      "Loss after mini batch    17: 13.239\n",
      "Loss after mini batch    17: 13.931\n",
      "Loss after mini batch    17: 13.215\n",
      "Loss after mini batch    17: 12.670\n",
      "Loss after mini batch    17: 13.120\n",
      "Loss after mini batch    17: 15.530\n",
      "Loss after mini batch    17: 14.327\n",
      "Loss after mini batch    17: 15.819\n",
      "Loss after mini batch    17: 13.936\n",
      "Loss after mini batch    17: 15.149\n",
      "Starting epoch 18\n",
      "Loss after mini batch    18: 12.724\n",
      "Loss after mini batch    18: 14.063\n",
      "Loss after mini batch    18: 13.112\n",
      "Loss after mini batch    18: 13.768\n",
      "Loss after mini batch    18: 15.710\n",
      "Loss after mini batch    18: 14.382\n",
      "Loss after mini batch    18: 13.952\n",
      "Loss after mini batch    18: 12.940\n",
      "Loss after mini batch    18: 15.059\n",
      "Loss after mini batch    18: 12.736\n",
      "Loss after mini batch    18: 13.792\n",
      "Starting epoch 19\n",
      "Loss after mini batch    19: 13.341\n",
      "Loss after mini batch    19: 13.403\n",
      "Loss after mini batch    19: 16.396\n",
      "Loss after mini batch    19: 17.380\n",
      "Loss after mini batch    19: 13.922\n",
      "Loss after mini batch    19: 14.225\n",
      "Loss after mini batch    19: 13.738\n",
      "Loss after mini batch    19: 13.669\n",
      "Loss after mini batch    19: 13.282\n",
      "Loss after mini batch    19: 15.164\n",
      "Loss after mini batch    19: 15.113\n",
      "Starting epoch 20\n",
      "Loss after mini batch    20: 13.945\n",
      "Loss after mini batch    20: 14.501\n",
      "Loss after mini batch    20: 14.234\n",
      "Loss after mini batch    20: 16.923\n",
      "Loss after mini batch    20: 13.402\n",
      "Loss after mini batch    20: 12.644\n",
      "Loss after mini batch    20: 13.899\n",
      "Loss after mini batch    20: 13.345\n",
      "Loss after mini batch    20: 12.962\n",
      "Loss after mini batch    20: 13.371\n",
      "Loss after mini batch    20: 14.223\n",
      "Starting epoch 21\n",
      "Loss after mini batch    21: 13.642\n",
      "Loss after mini batch    21: 12.513\n",
      "Loss after mini batch    21: 12.808\n",
      "Loss after mini batch    21: 11.934\n",
      "Loss after mini batch    21: 14.011\n",
      "Loss after mini batch    21: 17.734\n",
      "Loss after mini batch    21: 15.090\n",
      "Loss after mini batch    21: 14.356\n",
      "Loss after mini batch    21: 12.852\n",
      "Loss after mini batch    21: 14.863\n",
      "Loss after mini batch    21: 14.257\n",
      "Starting epoch 22\n",
      "Loss after mini batch    22: 12.905\n",
      "Loss after mini batch    22: 13.329\n",
      "Loss after mini batch    22: 13.830\n",
      "Loss after mini batch    22: 12.870\n",
      "Loss after mini batch    22: 14.922\n",
      "Loss after mini batch    22: 13.003\n",
      "Loss after mini batch    22: 13.619\n",
      "Loss after mini batch    22: 12.352\n",
      "Loss after mini batch    22: 14.163\n",
      "Loss after mini batch    22: 12.836\n",
      "Loss after mini batch    22: 14.376\n",
      "Starting epoch 23\n",
      "Loss after mini batch    23: 14.116\n",
      "Loss after mini batch    23: 13.123\n",
      "Loss after mini batch    23: 15.950\n",
      "Loss after mini batch    23: 12.810\n",
      "Loss after mini batch    23: 13.902\n",
      "Loss after mini batch    23: 13.232\n",
      "Loss after mini batch    23: 13.061\n",
      "Loss after mini batch    23: 14.759\n",
      "Loss after mini batch    23: 14.248\n",
      "Loss after mini batch    23: 14.162\n",
      "Loss after mini batch    23: 13.562\n",
      "Starting epoch 24\n",
      "Loss after mini batch    24: 14.041\n",
      "Loss after mini batch    24: 13.466\n",
      "Loss after mini batch    24: 13.486\n",
      "Loss after mini batch    24: 13.297\n",
      "Loss after mini batch    24: 13.990\n",
      "Loss after mini batch    24: 12.611\n",
      "Loss after mini batch    24: 14.343\n",
      "Loss after mini batch    24: 13.487\n",
      "Loss after mini batch    24: 15.240\n",
      "Loss after mini batch    24: 12.875\n",
      "Loss after mini batch    24: 13.070\n",
      "Starting epoch 25\n",
      "Loss after mini batch    25: 14.257\n",
      "Loss after mini batch    25: 14.023\n",
      "Loss after mini batch    25: 13.035\n",
      "Loss after mini batch    25: 13.486\n",
      "Loss after mini batch    25: 12.861\n",
      "Loss after mini batch    25: 12.849\n",
      "Loss after mini batch    25: 11.910\n",
      "Loss after mini batch    25: 12.384\n",
      "Loss after mini batch    25: 13.567\n",
      "Loss after mini batch    25: 13.826\n",
      "Loss after mini batch    25: 14.054\n",
      "Starting epoch 26\n",
      "Loss after mini batch    26: 13.277\n",
      "Loss after mini batch    26: 13.813\n",
      "Loss after mini batch    26: 12.052\n",
      "Loss after mini batch    26: 13.047\n",
      "Loss after mini batch    26: 13.888\n",
      "Loss after mini batch    26: 14.319\n",
      "Loss after mini batch    26: 14.098\n",
      "Loss after mini batch    26: 12.636\n",
      "Loss after mini batch    26: 14.143\n",
      "Loss after mini batch    26: 14.373\n",
      "Loss after mini batch    26: 13.641\n",
      "Starting epoch 27\n",
      "Loss after mini batch    27: 13.604\n",
      "Loss after mini batch    27: 15.732\n",
      "Loss after mini batch    27: 11.721\n",
      "Loss after mini batch    27: 14.260\n",
      "Loss after mini batch    27: 14.675\n",
      "Loss after mini batch    27: 14.339\n",
      "Loss after mini batch    27: 14.299\n",
      "Loss after mini batch    27: 12.275\n",
      "Loss after mini batch    27: 13.327\n",
      "Loss after mini batch    27: 12.103\n",
      "Loss after mini batch    27: 14.488\n",
      "Starting epoch 28\n",
      "Loss after mini batch    28: 13.822\n",
      "Loss after mini batch    28: 12.375\n",
      "Loss after mini batch    28: 13.668\n",
      "Loss after mini batch    28: 12.571\n",
      "Loss after mini batch    28: 13.911\n",
      "Loss after mini batch    28: 12.938\n",
      "Loss after mini batch    28: 15.115\n",
      "Loss after mini batch    28: 14.593\n",
      "Loss after mini batch    28: 13.743\n",
      "Loss after mini batch    28: 13.741\n",
      "Loss after mini batch    28: 12.460\n",
      "Starting epoch 29\n",
      "Loss after mini batch    29: 13.717\n",
      "Loss after mini batch    29: 13.931\n",
      "Loss after mini batch    29: 12.964\n",
      "Loss after mini batch    29: 12.779\n",
      "Loss after mini batch    29: 14.138\n",
      "Loss after mini batch    29: 13.621\n",
      "Loss after mini batch    29: 13.740\n",
      "Loss after mini batch    29: 12.479\n",
      "Loss after mini batch    29: 14.484\n",
      "Loss after mini batch    29: 13.468\n",
      "Loss after mini batch    29: 12.143\n",
      "Starting epoch 30\n",
      "Loss after mini batch    30: 15.042\n",
      "Loss after mini batch    30: 14.455\n",
      "Loss after mini batch    30: 14.472\n",
      "Loss after mini batch    30: 13.475\n",
      "Loss after mini batch    30: 12.479\n",
      "Loss after mini batch    30: 14.735\n",
      "Loss after mini batch    30: 12.572\n",
      "Loss after mini batch    30: 12.950\n",
      "Loss after mini batch    30: 13.732\n",
      "Loss after mini batch    30: 13.361\n",
      "Loss after mini batch    30: 13.361\n",
      "Starting epoch 31\n",
      "Loss after mini batch    31: 12.095\n",
      "Loss after mini batch    31: 13.340\n",
      "Loss after mini batch    31: 14.137\n",
      "Loss after mini batch    31: 12.747\n",
      "Loss after mini batch    31: 14.318\n",
      "Loss after mini batch    31: 13.723\n",
      "Loss after mini batch    31: 12.896\n",
      "Loss after mini batch    31: 14.455\n",
      "Loss after mini batch    31: 13.536\n",
      "Loss after mini batch    31: 13.467\n",
      "Loss after mini batch    31: 12.000\n",
      "Starting epoch 32\n",
      "Loss after mini batch    32: 13.897\n",
      "Loss after mini batch    32: 15.288\n",
      "Loss after mini batch    32: 13.553\n",
      "Loss after mini batch    32: 13.125\n",
      "Loss after mini batch    32: 14.188\n",
      "Loss after mini batch    32: 14.849\n",
      "Loss after mini batch    32: 13.386\n",
      "Loss after mini batch    32: 14.123\n",
      "Loss after mini batch    32: 13.688\n",
      "Loss after mini batch    32: 14.122\n",
      "Loss after mini batch    32: 13.022\n",
      "Starting epoch 33\n",
      "Loss after mini batch    33: 11.525\n",
      "Loss after mini batch    33: 12.106\n",
      "Loss after mini batch    33: 14.116\n",
      "Loss after mini batch    33: 14.450\n",
      "Loss after mini batch    33: 13.679\n",
      "Loss after mini batch    33: 13.258\n",
      "Loss after mini batch    33: 14.728\n",
      "Loss after mini batch    33: 14.746\n",
      "Loss after mini batch    33: 13.256\n",
      "Loss after mini batch    33: 12.525\n",
      "Loss after mini batch    33: 14.186\n",
      "Starting epoch 34\n",
      "Loss after mini batch    34: 13.653\n",
      "Loss after mini batch    34: 13.566\n",
      "Loss after mini batch    34: 15.262\n",
      "Loss after mini batch    34: 13.192\n",
      "Loss after mini batch    34: 13.084\n",
      "Loss after mini batch    34: 13.302\n",
      "Loss after mini batch    34: 12.846\n",
      "Loss after mini batch    34: 15.293\n",
      "Loss after mini batch    34: 13.208\n",
      "Loss after mini batch    34: 11.777\n",
      "Loss after mini batch    34: 13.410\n",
      "Starting epoch 35\n",
      "Loss after mini batch    35: 13.781\n",
      "Loss after mini batch    35: 14.441\n",
      "Loss after mini batch    35: 13.981\n",
      "Loss after mini batch    35: 11.479\n",
      "Loss after mini batch    35: 11.780\n",
      "Loss after mini batch    35: 12.886\n",
      "Loss after mini batch    35: 12.344\n",
      "Loss after mini batch    35: 13.491\n",
      "Loss after mini batch    35: 14.049\n",
      "Loss after mini batch    35: 12.133\n",
      "Loss after mini batch    35: 12.746\n",
      "Starting epoch 36\n",
      "Loss after mini batch    36: 12.899\n",
      "Loss after mini batch    36: 14.252\n",
      "Loss after mini batch    36: 13.971\n",
      "Loss after mini batch    36: 13.495\n",
      "Loss after mini batch    36: 11.834\n",
      "Loss after mini batch    36: 13.130\n",
      "Loss after mini batch    36: 13.737\n",
      "Loss after mini batch    36: 12.852\n",
      "Loss after mini batch    36: 14.358\n",
      "Loss after mini batch    36: 12.008\n",
      "Loss after mini batch    36: 13.456\n",
      "Starting epoch 37\n",
      "Loss after mini batch    37: 13.493\n",
      "Loss after mini batch    37: 13.970\n",
      "Loss after mini batch    37: 13.281\n",
      "Loss after mini batch    37: 13.411\n",
      "Loss after mini batch    37: 14.593\n",
      "Loss after mini batch    37: 13.271\n",
      "Loss after mini batch    37: 12.680\n",
      "Loss after mini batch    37: 14.022\n",
      "Loss after mini batch    37: 14.048\n",
      "Loss after mini batch    37: 14.173\n",
      "Loss after mini batch    37: 12.860\n",
      "Starting epoch 38\n",
      "Loss after mini batch    38: 14.482\n",
      "Loss after mini batch    38: 13.154\n",
      "Loss after mini batch    38: 12.774\n",
      "Loss after mini batch    38: 11.821\n",
      "Loss after mini batch    38: 13.849\n",
      "Loss after mini batch    38: 12.850\n",
      "Loss after mini batch    38: 16.016\n",
      "Loss after mini batch    38: 13.254\n",
      "Loss after mini batch    38: 11.980\n",
      "Loss after mini batch    38: 13.009\n",
      "Loss after mini batch    38: 13.567\n",
      "Starting epoch 39\n",
      "Loss after mini batch    39: 12.360\n",
      "Loss after mini batch    39: 13.719\n",
      "Loss after mini batch    39: 12.589\n",
      "Loss after mini batch    39: 12.578\n",
      "Loss after mini batch    39: 11.750\n",
      "Loss after mini batch    39: 12.930\n",
      "Loss after mini batch    39: 13.406\n",
      "Loss after mini batch    39: 12.420\n",
      "Loss after mini batch    39: 12.450\n",
      "Loss after mini batch    39: 13.257\n",
      "Loss after mini batch    39: 13.657\n",
      "Starting epoch 40\n",
      "Loss after mini batch    40: 11.910\n",
      "Loss after mini batch    40: 12.504\n",
      "Loss after mini batch    40: 14.393\n",
      "Loss after mini batch    40: 13.694\n",
      "Loss after mini batch    40: 13.460\n",
      "Loss after mini batch    40: 13.379\n",
      "Loss after mini batch    40: 15.243\n",
      "Loss after mini batch    40: 14.007\n",
      "Loss after mini batch    40: 12.843\n",
      "Loss after mini batch    40: 12.781\n",
      "Loss after mini batch    40: 13.107\n",
      "Starting epoch 41\n",
      "Loss after mini batch    41: 12.343\n",
      "Loss after mini batch    41: 13.552\n",
      "Loss after mini batch    41: 12.846\n",
      "Loss after mini batch    41: 12.282\n",
      "Loss after mini batch    41: 13.079\n",
      "Loss after mini batch    41: 12.815\n",
      "Loss after mini batch    41: 14.249\n",
      "Loss after mini batch    41: 13.451\n",
      "Loss after mini batch    41: 13.222\n",
      "Loss after mini batch    41: 12.948\n",
      "Loss after mini batch    41: 12.784\n",
      "Starting epoch 42\n",
      "Loss after mini batch    42: 12.429\n",
      "Loss after mini batch    42: 13.338\n",
      "Loss after mini batch    42: 13.477\n",
      "Loss after mini batch    42: 13.976\n",
      "Loss after mini batch    42: 12.599\n",
      "Loss after mini batch    42: 14.763\n",
      "Loss after mini batch    42: 13.497\n",
      "Loss after mini batch    42: 15.000\n",
      "Loss after mini batch    42: 13.049\n",
      "Loss after mini batch    42: 15.379\n",
      "Loss after mini batch    42: 12.745\n",
      "Starting epoch 43\n",
      "Loss after mini batch    43: 14.231\n",
      "Loss after mini batch    43: 12.094\n",
      "Loss after mini batch    43: 14.246\n",
      "Loss after mini batch    43: 13.949\n",
      "Loss after mini batch    43: 12.087\n",
      "Loss after mini batch    43: 11.794\n",
      "Loss after mini batch    43: 12.890\n",
      "Loss after mini batch    43: 12.618\n",
      "Loss after mini batch    43: 11.693\n",
      "Loss after mini batch    43: 14.833\n",
      "Loss after mini batch    43: 12.795\n",
      "Starting epoch 44\n",
      "Loss after mini batch    44: 14.496\n",
      "Loss after mini batch    44: 12.224\n",
      "Loss after mini batch    44: 11.911\n",
      "Loss after mini batch    44: 15.173\n",
      "Loss after mini batch    44: 13.106\n",
      "Loss after mini batch    44: 13.558\n",
      "Loss after mini batch    44: 13.528\n",
      "Loss after mini batch    44: 11.850\n",
      "Loss after mini batch    44: 13.021\n",
      "Loss after mini batch    44: 13.290\n",
      "Loss after mini batch    44: 12.834\n",
      "Starting epoch 45\n",
      "Loss after mini batch    45: 12.062\n",
      "Loss after mini batch    45: 14.513\n",
      "Loss after mini batch    45: 12.895\n",
      "Loss after mini batch    45: 12.926\n",
      "Loss after mini batch    45: 13.904\n",
      "Loss after mini batch    45: 13.400\n",
      "Loss after mini batch    45: 12.644\n",
      "Loss after mini batch    45: 12.785\n",
      "Loss after mini batch    45: 12.394\n",
      "Loss after mini batch    45: 12.741\n",
      "Loss after mini batch    45: 12.813\n",
      "Starting epoch 46\n",
      "Loss after mini batch    46: 12.551\n",
      "Loss after mini batch    46: 12.850\n",
      "Loss after mini batch    46: 13.279\n",
      "Loss after mini batch    46: 14.498\n",
      "Loss after mini batch    46: 11.752\n",
      "Loss after mini batch    46: 12.704\n",
      "Loss after mini batch    46: 11.680\n",
      "Loss after mini batch    46: 12.929\n",
      "Loss after mini batch    46: 13.726\n",
      "Loss after mini batch    46: 12.717\n",
      "Loss after mini batch    46: 13.346\n",
      "Starting epoch 47\n",
      "Loss after mini batch    47: 12.581\n",
      "Loss after mini batch    47: 13.657\n",
      "Loss after mini batch    47: 12.828\n",
      "Loss after mini batch    47: 12.547\n",
      "Loss after mini batch    47: 14.022\n",
      "Loss after mini batch    47: 12.677\n",
      "Loss after mini batch    47: 14.248\n",
      "Loss after mini batch    47: 14.079\n",
      "Loss after mini batch    47: 11.583\n",
      "Loss after mini batch    47: 14.155\n",
      "Loss after mini batch    47: 12.433\n",
      "Starting epoch 48\n",
      "Loss after mini batch    48: 12.589\n",
      "Loss after mini batch    48: 12.899\n",
      "Loss after mini batch    48: 14.164\n",
      "Loss after mini batch    48: 12.275\n",
      "Loss after mini batch    48: 13.607\n",
      "Loss after mini batch    48: 12.821\n",
      "Loss after mini batch    48: 13.538\n",
      "Loss after mini batch    48: 14.258\n",
      "Loss after mini batch    48: 12.315\n",
      "Loss after mini batch    48: 12.410\n",
      "Loss after mini batch    48: 16.376\n",
      "Starting epoch 49\n",
      "Loss after mini batch    49: 11.496\n",
      "Loss after mini batch    49: 13.581\n",
      "Loss after mini batch    49: 14.001\n",
      "Loss after mini batch    49: 12.850\n",
      "Loss after mini batch    49: 13.589\n",
      "Loss after mini batch    49: 13.529\n",
      "Loss after mini batch    49: 13.006\n",
      "Loss after mini batch    49: 12.371\n",
      "Loss after mini batch    49: 14.212\n",
      "Loss after mini batch    49: 11.785\n",
      "Loss after mini batch    49: 14.605\n",
      "Starting epoch 50\n",
      "Loss after mini batch    50: 12.005\n",
      "Loss after mini batch    50: 17.028\n",
      "Loss after mini batch    50: 13.208\n",
      "Loss after mini batch    50: 13.036\n",
      "Loss after mini batch    50: 12.223\n",
      "Loss after mini batch    50: 12.683\n",
      "Loss after mini batch    50: 11.987\n",
      "Loss after mini batch    50: 13.947\n",
      "Loss after mini batch    50: 12.422\n",
      "Loss after mini batch    50: 13.925\n",
      "Loss after mini batch    50: 13.142\n",
      "Starting epoch 51\n",
      "Loss after mini batch    51: 13.668\n",
      "Loss after mini batch    51: 14.043\n",
      "Loss after mini batch    51: 12.838\n",
      "Loss after mini batch    51: 13.418\n",
      "Loss after mini batch    51: 13.351\n",
      "Loss after mini batch    51: 12.773\n",
      "Loss after mini batch    51: 12.347\n",
      "Loss after mini batch    51: 12.259\n",
      "Loss after mini batch    51: 12.906\n",
      "Loss after mini batch    51: 12.890\n",
      "Loss after mini batch    51: 13.316\n",
      "Starting epoch 52\n",
      "Loss after mini batch    52: 12.929\n",
      "Loss after mini batch    52: 12.818\n",
      "Loss after mini batch    52: 14.778\n",
      "Loss after mini batch    52: 12.939\n",
      "Loss after mini batch    52: 13.555\n",
      "Loss after mini batch    52: 13.578\n",
      "Loss after mini batch    52: 12.795\n",
      "Loss after mini batch    52: 12.930\n",
      "Loss after mini batch    52: 13.009\n",
      "Loss after mini batch    52: 13.441\n",
      "Loss after mini batch    52: 13.113\n",
      "Starting epoch 53\n",
      "Loss after mini batch    53: 12.915\n",
      "Loss after mini batch    53: 12.641\n",
      "Loss after mini batch    53: 12.513\n",
      "Loss after mini batch    53: 12.442\n",
      "Loss after mini batch    53: 13.675\n",
      "Loss after mini batch    53: 13.123\n",
      "Loss after mini batch    53: 13.462\n",
      "Loss after mini batch    53: 11.556\n",
      "Loss after mini batch    53: 12.501\n",
      "Loss after mini batch    53: 12.961\n",
      "Loss after mini batch    53: 12.322\n",
      "Starting epoch 54\n",
      "Loss after mini batch    54: 12.257\n",
      "Loss after mini batch    54: 13.169\n",
      "Loss after mini batch    54: 13.859\n",
      "Loss after mini batch    54: 12.990\n",
      "Loss after mini batch    54: 12.400\n",
      "Loss after mini batch    54: 13.365\n",
      "Loss after mini batch    54: 11.884\n",
      "Loss after mini batch    54: 12.145\n",
      "Loss after mini batch    54: 14.878\n",
      "Loss after mini batch    54: 11.765\n",
      "Loss after mini batch    54: 11.997\n",
      "Starting epoch 55\n",
      "Loss after mini batch    55: 12.205\n",
      "Loss after mini batch    55: 13.779\n",
      "Loss after mini batch    55: 12.524\n",
      "Loss after mini batch    55: 13.838\n",
      "Loss after mini batch    55: 12.766\n",
      "Loss after mini batch    55: 13.170\n",
      "Loss after mini batch    55: 12.574\n",
      "Loss after mini batch    55: 11.984\n",
      "Loss after mini batch    55: 13.465\n",
      "Loss after mini batch    55: 13.828\n",
      "Loss after mini batch    55: 12.597\n",
      "Starting epoch 56\n",
      "Loss after mini batch    56: 13.051\n",
      "Loss after mini batch    56: 13.263\n",
      "Loss after mini batch    56: 14.031\n",
      "Loss after mini batch    56: 15.102\n",
      "Loss after mini batch    56: 12.672\n",
      "Loss after mini batch    56: 12.243\n",
      "Loss after mini batch    56: 12.593\n",
      "Loss after mini batch    56: 13.788\n",
      "Loss after mini batch    56: 13.318\n",
      "Loss after mini batch    56: 12.530\n",
      "Loss after mini batch    56: 12.959\n",
      "Starting epoch 57\n",
      "Loss after mini batch    57: 12.638\n",
      "Loss after mini batch    57: 13.135\n",
      "Loss after mini batch    57: 14.942\n",
      "Loss after mini batch    57: 12.938\n",
      "Loss after mini batch    57: 12.749\n",
      "Loss after mini batch    57: 13.607\n",
      "Loss after mini batch    57: 13.848\n",
      "Loss after mini batch    57: 13.472\n",
      "Loss after mini batch    57: 13.065\n",
      "Loss after mini batch    57: 12.355\n",
      "Loss after mini batch    57: 12.378\n",
      "Starting epoch 58\n",
      "Loss after mini batch    58: 14.093\n",
      "Loss after mini batch    58: 13.205\n",
      "Loss after mini batch    58: 12.512\n",
      "Loss after mini batch    58: 11.926\n",
      "Loss after mini batch    58: 11.722\n",
      "Loss after mini batch    58: 12.604\n",
      "Loss after mini batch    58: 11.672\n",
      "Loss after mini batch    58: 12.113\n",
      "Loss after mini batch    58: 13.141\n",
      "Loss after mini batch    58: 13.420\n",
      "Loss after mini batch    58: 13.707\n",
      "Starting epoch 59\n",
      "Loss after mini batch    59: 12.370\n",
      "Loss after mini batch    59: 14.055\n",
      "Loss after mini batch    59: 14.524\n",
      "Loss after mini batch    59: 14.359\n",
      "Loss after mini batch    59: 12.626\n",
      "Loss after mini batch    59: 12.264\n",
      "Loss after mini batch    59: 12.324\n",
      "Loss after mini batch    59: 12.574\n",
      "Loss after mini batch    59: 13.280\n",
      "Loss after mini batch    59: 13.177\n",
      "Loss after mini batch    59: 12.359\n",
      "Starting epoch 60\n",
      "Loss after mini batch    60: 13.899\n",
      "Loss after mini batch    60: 13.381\n",
      "Loss after mini batch    60: 13.123\n",
      "Loss after mini batch    60: 13.534\n",
      "Loss after mini batch    60: 11.995\n",
      "Loss after mini batch    60: 13.365\n",
      "Loss after mini batch    60: 14.608\n",
      "Loss after mini batch    60: 12.602\n",
      "Loss after mini batch    60: 12.680\n",
      "Loss after mini batch    60: 12.147\n",
      "Loss after mini batch    60: 11.873\n",
      "Starting epoch 61\n",
      "Loss after mini batch    61: 12.767\n",
      "Loss after mini batch    61: 12.675\n",
      "Loss after mini batch    61: 12.019\n",
      "Loss after mini batch    61: 13.659\n",
      "Loss after mini batch    61: 12.350\n",
      "Loss after mini batch    61: 13.197\n",
      "Loss after mini batch    61: 12.948\n",
      "Loss after mini batch    61: 13.683\n",
      "Loss after mini batch    61: 12.903\n",
      "Loss after mini batch    61: 12.086\n",
      "Loss after mini batch    61: 13.587\n",
      "Starting epoch 62\n",
      "Loss after mini batch    62: 12.795\n",
      "Loss after mini batch    62: 12.589\n",
      "Loss after mini batch    62: 11.710\n",
      "Loss after mini batch    62: 12.939\n",
      "Loss after mini batch    62: 14.161\n",
      "Loss after mini batch    62: 14.574\n",
      "Loss after mini batch    62: 13.214\n",
      "Loss after mini batch    62: 12.755\n",
      "Loss after mini batch    62: 11.571\n",
      "Loss after mini batch    62: 11.622\n",
      "Loss after mini batch    62: 13.012\n",
      "Starting epoch 63\n",
      "Loss after mini batch    63: 14.078\n",
      "Loss after mini batch    63: 12.096\n",
      "Loss after mini batch    63: 12.556\n",
      "Loss after mini batch    63: 12.998\n",
      "Loss after mini batch    63: 13.122\n",
      "Loss after mini batch    63: 12.950\n",
      "Loss after mini batch    63: 12.734\n",
      "Loss after mini batch    63: 12.639\n",
      "Loss after mini batch    63: 13.212\n",
      "Loss after mini batch    63: 12.604\n",
      "Loss after mini batch    63: 12.753\n",
      "Starting epoch 64\n",
      "Loss after mini batch    64: 11.539\n",
      "Loss after mini batch    64: 13.697\n",
      "Loss after mini batch    64: 13.127\n",
      "Loss after mini batch    64: 12.089\n",
      "Loss after mini batch    64: 13.753\n",
      "Loss after mini batch    64: 14.103\n",
      "Loss after mini batch    64: 13.223\n",
      "Loss after mini batch    64: 12.396\n",
      "Loss after mini batch    64: 13.179\n",
      "Loss after mini batch    64: 12.559\n",
      "Loss after mini batch    64: 12.819\n",
      "Starting epoch 65\n",
      "Loss after mini batch    65: 13.278\n",
      "Loss after mini batch    65: 12.518\n",
      "Loss after mini batch    65: 12.769\n",
      "Loss after mini batch    65: 12.532\n",
      "Loss after mini batch    65: 12.858\n",
      "Loss after mini batch    65: 11.990\n",
      "Loss after mini batch    65: 12.903\n",
      "Loss after mini batch    65: 12.182\n",
      "Loss after mini batch    65: 13.083\n",
      "Loss after mini batch    65: 13.268\n",
      "Loss after mini batch    65: 12.852\n",
      "Starting epoch 66\n",
      "Loss after mini batch    66: 12.146\n",
      "Loss after mini batch    66: 12.767\n",
      "Loss after mini batch    66: 12.695\n",
      "Loss after mini batch    66: 13.168\n",
      "Loss after mini batch    66: 11.405\n",
      "Loss after mini batch    66: 14.682\n",
      "Loss after mini batch    66: 12.826\n",
      "Loss after mini batch    66: 11.775\n",
      "Loss after mini batch    66: 12.689\n",
      "Loss after mini batch    66: 12.768\n",
      "Loss after mini batch    66: 13.712\n",
      "Starting epoch 67\n",
      "Loss after mini batch    67: 12.417\n",
      "Loss after mini batch    67: 12.437\n",
      "Loss after mini batch    67: 13.068\n",
      "Loss after mini batch    67: 12.955\n",
      "Loss after mini batch    67: 13.070\n",
      "Loss after mini batch    67: 13.993\n",
      "Loss after mini batch    67: 11.756\n",
      "Loss after mini batch    67: 12.127\n",
      "Loss after mini batch    67: 12.812\n",
      "Loss after mini batch    67: 13.127\n",
      "Loss after mini batch    67: 14.051\n",
      "Starting epoch 68\n",
      "Loss after mini batch    68: 11.646\n",
      "Loss after mini batch    68: 12.945\n",
      "Loss after mini batch    68: 12.530\n",
      "Loss after mini batch    68: 13.444\n",
      "Loss after mini batch    68: 11.731\n",
      "Loss after mini batch    68: 12.564\n",
      "Loss after mini batch    68: 13.554\n",
      "Loss after mini batch    68: 13.177\n",
      "Loss after mini batch    68: 12.931\n",
      "Loss after mini batch    68: 12.718\n",
      "Loss after mini batch    68: 11.666\n",
      "Starting epoch 69\n",
      "Loss after mini batch    69: 12.866\n",
      "Loss after mini batch    69: 15.099\n",
      "Loss after mini batch    69: 13.906\n",
      "Loss after mini batch    69: 12.099\n",
      "Loss after mini batch    69: 11.491\n",
      "Loss after mini batch    69: 13.707\n",
      "Loss after mini batch    69: 13.034\n",
      "Loss after mini batch    69: 12.031\n",
      "Loss after mini batch    69: 12.474\n",
      "Loss after mini batch    69: 13.037\n",
      "Loss after mini batch    69: 11.269\n",
      "Starting epoch 70\n",
      "Loss after mini batch    70: 13.245\n",
      "Loss after mini batch    70: 13.173\n",
      "Loss after mini batch    70: 12.617\n",
      "Loss after mini batch    70: 12.273\n",
      "Loss after mini batch    70: 11.602\n",
      "Loss after mini batch    70: 14.330\n",
      "Loss after mini batch    70: 12.020\n",
      "Loss after mini batch    70: 11.816\n",
      "Loss after mini batch    70: 12.415\n",
      "Loss after mini batch    70: 11.865\n",
      "Loss after mini batch    70: 11.733\n",
      "Starting epoch 71\n",
      "Loss after mini batch    71: 13.835\n",
      "Loss after mini batch    71: 13.308\n",
      "Loss after mini batch    71: 12.530\n",
      "Loss after mini batch    71: 11.914\n",
      "Loss after mini batch    71: 13.184\n",
      "Loss after mini batch    71: 12.517\n",
      "Loss after mini batch    71: 12.116\n",
      "Loss after mini batch    71: 12.001\n",
      "Loss after mini batch    71: 12.960\n",
      "Loss after mini batch    71: 13.378\n",
      "Loss after mini batch    71: 13.376\n",
      "Starting epoch 72\n",
      "Loss after mini batch    72: 11.633\n",
      "Loss after mini batch    72: 12.465\n",
      "Loss after mini batch    72: 13.980\n",
      "Loss after mini batch    72: 13.911\n",
      "Loss after mini batch    72: 13.429\n",
      "Loss after mini batch    72: 12.181\n",
      "Loss after mini batch    72: 12.102\n",
      "Loss after mini batch    72: 11.752\n",
      "Loss after mini batch    72: 12.466\n",
      "Loss after mini batch    72: 12.701\n",
      "Loss after mini batch    72: 13.617\n",
      "Starting epoch 73\n",
      "Loss after mini batch    73: 11.868\n",
      "Loss after mini batch    73: 12.767\n",
      "Loss after mini batch    73: 12.890\n",
      "Loss after mini batch    73: 12.230\n",
      "Loss after mini batch    73: 12.987\n",
      "Loss after mini batch    73: 12.842\n",
      "Loss after mini batch    73: 11.954\n",
      "Loss after mini batch    73: 13.043\n",
      "Loss after mini batch    73: 13.135\n",
      "Loss after mini batch    73: 11.804\n",
      "Loss after mini batch    73: 11.861\n",
      "Starting epoch 74\n",
      "Loss after mini batch    74: 12.898\n",
      "Loss after mini batch    74: 12.356\n",
      "Loss after mini batch    74: 13.603\n",
      "Loss after mini batch    74: 12.167\n",
      "Loss after mini batch    74: 12.452\n",
      "Loss after mini batch    74: 13.407\n",
      "Loss after mini batch    74: 13.218\n",
      "Loss after mini batch    74: 11.618\n",
      "Loss after mini batch    74: 11.916\n",
      "Loss after mini batch    74: 11.631\n",
      "Loss after mini batch    74: 12.591\n",
      "Starting epoch 75\n",
      "Loss after mini batch    75: 12.231\n",
      "Loss after mini batch    75: 12.964\n",
      "Loss after mini batch    75: 14.612\n",
      "Loss after mini batch    75: 12.217\n",
      "Loss after mini batch    75: 12.108\n",
      "Loss after mini batch    75: 13.857\n",
      "Loss after mini batch    75: 12.196\n",
      "Loss after mini batch    75: 11.889\n",
      "Loss after mini batch    75: 13.322\n",
      "Loss after mini batch    75: 12.357\n",
      "Loss after mini batch    75: 12.331\n",
      "Starting epoch 76\n",
      "Loss after mini batch    76: 13.622\n",
      "Loss after mini batch    76: 12.275\n",
      "Loss after mini batch    76: 12.057\n",
      "Loss after mini batch    76: 12.156\n",
      "Loss after mini batch    76: 13.588\n",
      "Loss after mini batch    76: 12.866\n",
      "Loss after mini batch    76: 12.892\n",
      "Loss after mini batch    76: 12.677\n",
      "Loss after mini batch    76: 12.005\n",
      "Loss after mini batch    76: 11.862\n",
      "Loss after mini batch    76: 12.447\n",
      "Starting epoch 77\n",
      "Loss after mini batch    77: 14.042\n",
      "Loss after mini batch    77: 13.011\n",
      "Loss after mini batch    77: 12.536\n",
      "Loss after mini batch    77: 12.708\n",
      "Loss after mini batch    77: 12.428\n",
      "Loss after mini batch    77: 13.189\n",
      "Loss after mini batch    77: 11.784\n",
      "Loss after mini batch    77: 11.857\n",
      "Loss after mini batch    77: 11.584\n",
      "Loss after mini batch    77: 12.853\n",
      "Loss after mini batch    77: 12.024\n",
      "Starting epoch 78\n",
      "Loss after mini batch    78: 11.783\n",
      "Loss after mini batch    78: 13.805\n",
      "Loss after mini batch    78: 12.353\n",
      "Loss after mini batch    78: 11.926\n",
      "Loss after mini batch    78: 11.947\n",
      "Loss after mini batch    78: 14.626\n",
      "Loss after mini batch    78: 11.537\n",
      "Loss after mini batch    78: 13.417\n",
      "Loss after mini batch    78: 11.910\n",
      "Loss after mini batch    78: 12.720\n",
      "Loss after mini batch    78: 12.237\n",
      "Starting epoch 79\n",
      "Loss after mini batch    79: 13.288\n",
      "Loss after mini batch    79: 11.909\n",
      "Loss after mini batch    79: 12.243\n",
      "Loss after mini batch    79: 12.230\n",
      "Loss after mini batch    79: 12.068\n",
      "Loss after mini batch    79: 12.596\n",
      "Loss after mini batch    79: 12.947\n",
      "Loss after mini batch    79: 13.973\n",
      "Loss after mini batch    79: 11.555\n",
      "Loss after mini batch    79: 11.875\n",
      "Loss after mini batch    79: 13.357\n",
      "Starting epoch 80\n",
      "Loss after mini batch    80: 11.506\n",
      "Loss after mini batch    80: 12.253\n",
      "Loss after mini batch    80: 13.259\n",
      "Loss after mini batch    80: 11.186\n",
      "Loss after mini batch    80: 12.328\n",
      "Loss after mini batch    80: 14.268\n",
      "Loss after mini batch    80: 14.004\n",
      "Loss after mini batch    80: 12.679\n",
      "Loss after mini batch    80: 12.316\n",
      "Loss after mini batch    80: 13.178\n",
      "Loss after mini batch    80: 13.461\n",
      "Starting epoch 81\n",
      "Loss after mini batch    81: 12.778\n",
      "Loss after mini batch    81: 12.808\n",
      "Loss after mini batch    81: 11.630\n",
      "Loss after mini batch    81: 11.501\n",
      "Loss after mini batch    81: 13.218\n",
      "Loss after mini batch    81: 12.816\n",
      "Loss after mini batch    81: 12.602\n",
      "Loss after mini batch    81: 13.261\n",
      "Loss after mini batch    81: 12.801\n",
      "Loss after mini batch    81: 13.240\n",
      "Loss after mini batch    81: 13.236\n",
      "Starting epoch 82\n",
      "Loss after mini batch    82: 13.103\n",
      "Loss after mini batch    82: 12.973\n",
      "Loss after mini batch    82: 11.155\n",
      "Loss after mini batch    82: 11.185\n",
      "Loss after mini batch    82: 12.300\n",
      "Loss after mini batch    82: 13.701\n",
      "Loss after mini batch    82: 12.306\n",
      "Loss after mini batch    82: 12.418\n",
      "Loss after mini batch    82: 12.333\n",
      "Loss after mini batch    82: 13.107\n",
      "Loss after mini batch    82: 13.199\n",
      "Starting epoch 83\n",
      "Loss after mini batch    83: 11.236\n",
      "Loss after mini batch    83: 12.873\n",
      "Loss after mini batch    83: 13.912\n",
      "Loss after mini batch    83: 12.444\n",
      "Loss after mini batch    83: 12.448\n",
      "Loss after mini batch    83: 12.461\n",
      "Loss after mini batch    83: 12.731\n",
      "Loss after mini batch    83: 11.518\n",
      "Loss after mini batch    83: 12.704\n",
      "Loss after mini batch    83: 12.062\n",
      "Loss after mini batch    83: 13.159\n",
      "Starting epoch 84\n",
      "Loss after mini batch    84: 12.178\n",
      "Loss after mini batch    84: 12.401\n",
      "Loss after mini batch    84: 12.782\n",
      "Loss after mini batch    84: 11.506\n",
      "Loss after mini batch    84: 11.861\n",
      "Loss after mini batch    84: 13.712\n",
      "Loss after mini batch    84: 12.962\n",
      "Loss after mini batch    84: 12.807\n",
      "Loss after mini batch    84: 12.207\n",
      "Loss after mini batch    84: 12.141\n",
      "Loss after mini batch    84: 13.214\n",
      "Starting epoch 85\n",
      "Loss after mini batch    85: 12.045\n",
      "Loss after mini batch    85: 11.640\n",
      "Loss after mini batch    85: 13.206\n",
      "Loss after mini batch    85: 12.097\n",
      "Loss after mini batch    85: 12.150\n",
      "Loss after mini batch    85: 13.195\n",
      "Loss after mini batch    85: 12.319\n",
      "Loss after mini batch    85: 12.636\n",
      "Loss after mini batch    85: 14.024\n",
      "Loss after mini batch    85: 11.498\n",
      "Loss after mini batch    85: 11.920\n",
      "Starting epoch 86\n",
      "Loss after mini batch    86: 11.511\n",
      "Loss after mini batch    86: 13.449\n",
      "Loss after mini batch    86: 12.538\n",
      "Loss after mini batch    86: 12.962\n",
      "Loss after mini batch    86: 12.993\n",
      "Loss after mini batch    86: 11.858\n",
      "Loss after mini batch    86: 11.326\n",
      "Loss after mini batch    86: 12.491\n",
      "Loss after mini batch    86: 12.493\n",
      "Loss after mini batch    86: 12.545\n",
      "Loss after mini batch    86: 12.870\n",
      "Starting epoch 87\n",
      "Loss after mini batch    87: 12.221\n",
      "Loss after mini batch    87: 13.338\n",
      "Loss after mini batch    87: 13.003\n",
      "Loss after mini batch    87: 12.077\n",
      "Loss after mini batch    87: 12.081\n",
      "Loss after mini batch    87: 12.564\n",
      "Loss after mini batch    87: 10.526\n",
      "Loss after mini batch    87: 12.338\n",
      "Loss after mini batch    87: 12.293\n",
      "Loss after mini batch    87: 12.230\n",
      "Loss after mini batch    87: 12.715\n",
      "Starting epoch 88\n",
      "Loss after mini batch    88: 11.872\n",
      "Loss after mini batch    88: 11.581\n",
      "Loss after mini batch    88: 11.468\n",
      "Loss after mini batch    88: 12.128\n",
      "Loss after mini batch    88: 12.494\n",
      "Loss after mini batch    88: 12.602\n",
      "Loss after mini batch    88: 12.211\n",
      "Loss after mini batch    88: 12.884\n",
      "Loss after mini batch    88: 11.966\n",
      "Loss after mini batch    88: 11.351\n",
      "Loss after mini batch    88: 12.724\n",
      "Starting epoch 89\n",
      "Loss after mini batch    89: 11.945\n",
      "Loss after mini batch    89: 11.770\n",
      "Loss after mini batch    89: 12.655\n",
      "Loss after mini batch    89: 13.185\n",
      "Loss after mini batch    89: 13.271\n",
      "Loss after mini batch    89: 11.831\n",
      "Loss after mini batch    89: 12.031\n",
      "Loss after mini batch    89: 13.052\n",
      "Loss after mini batch    89: 12.254\n",
      "Loss after mini batch    89: 11.717\n",
      "Loss after mini batch    89: 13.268\n",
      "Starting epoch 90\n",
      "Loss after mini batch    90: 12.818\n",
      "Loss after mini batch    90: 12.144\n",
      "Loss after mini batch    90: 12.227\n",
      "Loss after mini batch    90: 11.859\n",
      "Loss after mini batch    90: 11.730\n",
      "Loss after mini batch    90: 10.754\n",
      "Loss after mini batch    90: 12.829\n",
      "Loss after mini batch    90: 12.399\n",
      "Loss after mini batch    90: 12.937\n",
      "Loss after mini batch    90: 13.637\n",
      "Loss after mini batch    90: 13.263\n",
      "Starting epoch 91\n",
      "Loss after mini batch    91: 11.705\n",
      "Loss after mini batch    91: 12.369\n",
      "Loss after mini batch    91: 12.744\n",
      "Loss after mini batch    91: 12.663\n",
      "Loss after mini batch    91: 12.857\n",
      "Loss after mini batch    91: 12.873\n",
      "Loss after mini batch    91: 13.457\n",
      "Loss after mini batch    91: 12.057\n",
      "Loss after mini batch    91: 11.316\n",
      "Loss after mini batch    91: 11.941\n",
      "Loss after mini batch    91: 13.204\n",
      "Starting epoch 92\n",
      "Loss after mini batch    92: 12.520\n",
      "Loss after mini batch    92: 12.844\n",
      "Loss after mini batch    92: 11.755\n",
      "Loss after mini batch    92: 11.230\n",
      "Loss after mini batch    92: 12.016\n",
      "Loss after mini batch    92: 14.608\n",
      "Loss after mini batch    92: 12.132\n",
      "Loss after mini batch    92: 12.238\n",
      "Loss after mini batch    92: 12.584\n",
      "Loss after mini batch    92: 13.137\n",
      "Loss after mini batch    92: 12.506\n",
      "Starting epoch 93\n",
      "Loss after mini batch    93: 12.008\n",
      "Loss after mini batch    93: 11.845\n",
      "Loss after mini batch    93: 12.496\n",
      "Loss after mini batch    93: 12.343\n",
      "Loss after mini batch    93: 12.699\n",
      "Loss after mini batch    93: 11.855\n",
      "Loss after mini batch    93: 11.440\n",
      "Loss after mini batch    93: 12.676\n",
      "Loss after mini batch    93: 12.710\n",
      "Loss after mini batch    93: 12.084\n",
      "Loss after mini batch    93: 12.716\n",
      "Starting epoch 94\n",
      "Loss after mini batch    94: 11.732\n",
      "Loss after mini batch    94: 12.608\n",
      "Loss after mini batch    94: 12.637\n",
      "Loss after mini batch    94: 12.039\n",
      "Loss after mini batch    94: 12.324\n",
      "Loss after mini batch    94: 12.374\n",
      "Loss after mini batch    94: 13.892\n",
      "Loss after mini batch    94: 12.170\n",
      "Loss after mini batch    94: 12.367\n",
      "Loss after mini batch    94: 12.796\n",
      "Loss after mini batch    94: 12.780\n",
      "Starting epoch 95\n",
      "Loss after mini batch    95: 12.138\n",
      "Loss after mini batch    95: 12.465\n",
      "Loss after mini batch    95: 11.642\n",
      "Loss after mini batch    95: 12.396\n",
      "Loss after mini batch    95: 12.880\n",
      "Loss after mini batch    95: 13.564\n",
      "Loss after mini batch    95: 12.376\n",
      "Loss after mini batch    95: 11.450\n",
      "Loss after mini batch    95: 11.396\n",
      "Loss after mini batch    95: 13.173\n",
      "Loss after mini batch    95: 12.969\n",
      "Starting epoch 96\n",
      "Loss after mini batch    96: 12.824\n",
      "Loss after mini batch    96: 12.415\n",
      "Loss after mini batch    96: 13.056\n",
      "Loss after mini batch    96: 11.792\n",
      "Loss after mini batch    96: 11.564\n",
      "Loss after mini batch    96: 11.990\n",
      "Loss after mini batch    96: 13.651\n",
      "Loss after mini batch    96: 12.430\n",
      "Loss after mini batch    96: 12.295\n",
      "Loss after mini batch    96: 12.906\n",
      "Loss after mini batch    96: 12.328\n",
      "Starting epoch 97\n",
      "Loss after mini batch    97: 10.871\n",
      "Loss after mini batch    97: 11.293\n",
      "Loss after mini batch    97: 13.650\n",
      "Loss after mini batch    97: 12.908\n",
      "Loss after mini batch    97: 12.050\n",
      "Loss after mini batch    97: 12.535\n",
      "Loss after mini batch    97: 12.862\n",
      "Loss after mini batch    97: 12.645\n",
      "Loss after mini batch    97: 12.282\n",
      "Loss after mini batch    97: 11.301\n",
      "Loss after mini batch    97: 12.938\n",
      "Starting epoch 98\n",
      "Loss after mini batch    98: 12.212\n",
      "Loss after mini batch    98: 12.781\n",
      "Loss after mini batch    98: 11.187\n",
      "Loss after mini batch    98: 13.903\n",
      "Loss after mini batch    98: 13.760\n",
      "Loss after mini batch    98: 11.639\n",
      "Loss after mini batch    98: 12.045\n",
      "Loss after mini batch    98: 12.287\n",
      "Loss after mini batch    98: 13.286\n",
      "Loss after mini batch    98: 11.901\n",
      "Loss after mini batch    98: 12.531\n",
      "Starting epoch 99\n",
      "Loss after mini batch    99: 13.123\n",
      "Loss after mini batch    99: 11.176\n",
      "Loss after mini batch    99: 11.919\n",
      "Loss after mini batch    99: 13.597\n",
      "Loss after mini batch    99: 12.544\n",
      "Loss after mini batch    99: 11.999\n",
      "Loss after mini batch    99: 12.999\n",
      "Loss after mini batch    99: 12.093\n",
      "Loss after mini batch    99: 12.253\n",
      "Loss after mini batch    99: 11.879\n",
      "Loss after mini batch    99: 12.184\n",
      "Starting epoch 100\n",
      "Loss after mini batch   100: 11.484\n",
      "Loss after mini batch   100: 14.118\n",
      "Loss after mini batch   100: 11.848\n",
      "Loss after mini batch   100: 12.655\n",
      "Loss after mini batch   100: 12.891\n",
      "Loss after mini batch   100: 11.140\n",
      "Loss after mini batch   100: 11.568\n",
      "Loss after mini batch   100: 11.935\n",
      "Loss after mini batch   100: 12.842\n",
      "Loss after mini batch   100: 12.739\n",
      "Loss after mini batch   100: 12.752\n",
      "Training Complete\n",
      "Starting Testing\n",
      "RMSE for fold 8: 3.4751733640608955\n",
      "rRMSE for fold 8: 0.06777520745008574\n",
      "r for fold 8: 0.9904498614055671\n",
      "Fast RMSE for fold 8: 3.0488734807044344\n",
      "Fast rRMSE for fold 8: 0.05871601174876621\n",
      "Fast r for fold 8: 0.9844009878890692\n",
      "Slow RMSE for fold 8: 4.046908227496916\n",
      "Slow rRMSE for fold 8: 0.08106412328333223\n",
      "Slow r for fold 8: 0.9984234047726369\n",
      "Regular RMSE for fold 8: 3.3200531263581943\n",
      "Regular rRMSE for fold 8: 0.06405200299163893\n",
      "Regular r for fold 8: 0.9963745408050835\n",
      "Fold 9\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini batch     1: 374.026\n",
      "Loss after mini batch     1: 29.040\n",
      "Loss after mini batch     1: 20.634\n",
      "Loss after mini batch     1: 19.252\n",
      "Loss after mini batch     1: 18.941\n",
      "Loss after mini batch     1: 20.068\n",
      "Loss after mini batch     1: 17.197\n",
      "Loss after mini batch     1: 22.141\n",
      "Loss after mini batch     1: 23.110\n",
      "Loss after mini batch     1: 21.865\n",
      "Loss after mini batch     1: 21.894\n",
      "Starting epoch 2\n",
      "Loss after mini batch     2: 22.656\n",
      "Loss after mini batch     2: 20.757\n",
      "Loss after mini batch     2: 23.303\n",
      "Loss after mini batch     2: 20.842\n",
      "Loss after mini batch     2: 23.836\n",
      "Loss after mini batch     2: 35.728\n",
      "Loss after mini batch     2: 17.846\n",
      "Loss after mini batch     2: 19.986\n",
      "Loss after mini batch     2: 17.809\n",
      "Loss after mini batch     2: 19.342\n",
      "Loss after mini batch     2: 18.496\n",
      "Starting epoch 3\n",
      "Loss after mini batch     3: 18.259\n",
      "Loss after mini batch     3: 23.394\n",
      "Loss after mini batch     3: 27.710\n",
      "Loss after mini batch     3: 17.069\n",
      "Loss after mini batch     3: 15.248\n",
      "Loss after mini batch     3: 22.443\n",
      "Loss after mini batch     3: 19.011\n",
      "Loss after mini batch     3: 16.784\n",
      "Loss after mini batch     3: 19.636\n",
      "Loss after mini batch     3: 17.946\n",
      "Loss after mini batch     3: 18.369\n",
      "Starting epoch 4\n",
      "Loss after mini batch     4: 21.190\n",
      "Loss after mini batch     4: 20.364\n",
      "Loss after mini batch     4: 17.680\n",
      "Loss after mini batch     4: 19.693\n",
      "Loss after mini batch     4: 18.193\n",
      "Loss after mini batch     4: 20.843\n",
      "Loss after mini batch     4: 26.929\n",
      "Loss after mini batch     4: 16.094\n",
      "Loss after mini batch     4: 16.143\n",
      "Loss after mini batch     4: 15.456\n",
      "Loss after mini batch     4: 14.858\n",
      "Starting epoch 5\n",
      "Loss after mini batch     5: 15.239\n",
      "Loss after mini batch     5: 13.235\n",
      "Loss after mini batch     5: 16.183\n",
      "Loss after mini batch     5: 17.706\n",
      "Loss after mini batch     5: 16.004\n",
      "Loss after mini batch     5: 27.577\n",
      "Loss after mini batch     5: 16.333\n",
      "Loss after mini batch     5: 21.995\n",
      "Loss after mini batch     5: 17.604\n",
      "Loss after mini batch     5: 12.473\n",
      "Loss after mini batch     5: 12.053\n",
      "Starting epoch 6\n",
      "Loss after mini batch     6: 15.833\n",
      "Loss after mini batch     6: 14.776\n",
      "Loss after mini batch     6: 14.157\n",
      "Loss after mini batch     6: 15.193\n",
      "Loss after mini batch     6: 14.570\n",
      "Loss after mini batch     6: 13.949\n",
      "Loss after mini batch     6: 15.544\n",
      "Loss after mini batch     6: 17.210\n",
      "Loss after mini batch     6: 14.890\n",
      "Loss after mini batch     6: 17.722\n",
      "Loss after mini batch     6: 14.238\n",
      "Starting epoch 7\n",
      "Loss after mini batch     7: 14.703\n",
      "Loss after mini batch     7: 13.968\n",
      "Loss after mini batch     7: 13.691\n",
      "Loss after mini batch     7: 18.200\n",
      "Loss after mini batch     7: 18.187\n",
      "Loss after mini batch     7: 14.918\n",
      "Loss after mini batch     7: 15.598\n",
      "Loss after mini batch     7: 17.017\n",
      "Loss after mini batch     7: 14.386\n",
      "Loss after mini batch     7: 16.458\n",
      "Loss after mini batch     7: 16.785\n",
      "Starting epoch 8\n",
      "Loss after mini batch     8: 14.881\n",
      "Loss after mini batch     8: 14.881\n",
      "Loss after mini batch     8: 16.569\n",
      "Loss after mini batch     8: 16.040\n",
      "Loss after mini batch     8: 15.518\n",
      "Loss after mini batch     8: 15.317\n",
      "Loss after mini batch     8: 15.358\n",
      "Loss after mini batch     8: 18.033\n",
      "Loss after mini batch     8: 17.035\n",
      "Loss after mini batch     8: 18.655\n",
      "Loss after mini batch     8: 13.832\n",
      "Starting epoch 9\n",
      "Loss after mini batch     9: 14.547\n",
      "Loss after mini batch     9: 15.774\n",
      "Loss after mini batch     9: 15.151\n",
      "Loss after mini batch     9: 18.335\n",
      "Loss after mini batch     9: 17.010\n",
      "Loss after mini batch     9: 14.544\n",
      "Loss after mini batch     9: 15.056\n",
      "Loss after mini batch     9: 12.705\n",
      "Loss after mini batch     9: 13.294\n",
      "Loss after mini batch     9: 20.200\n",
      "Loss after mini batch     9: 15.786\n",
      "Starting epoch 10\n",
      "Loss after mini batch    10: 14.180\n",
      "Loss after mini batch    10: 12.370\n",
      "Loss after mini batch    10: 15.315\n",
      "Loss after mini batch    10: 14.365\n",
      "Loss after mini batch    10: 14.937\n",
      "Loss after mini batch    10: 11.617\n",
      "Loss after mini batch    10: 13.883\n",
      "Loss after mini batch    10: 16.767\n",
      "Loss after mini batch    10: 14.334\n",
      "Loss after mini batch    10: 17.898\n",
      "Loss after mini batch    10: 15.100\n",
      "Starting epoch 11\n",
      "Loss after mini batch    11: 14.600\n",
      "Loss after mini batch    11: 15.227\n",
      "Loss after mini batch    11: 16.145\n",
      "Loss after mini batch    11: 15.194\n",
      "Loss after mini batch    11: 15.726\n",
      "Loss after mini batch    11: 13.875\n",
      "Loss after mini batch    11: 14.878\n",
      "Loss after mini batch    11: 15.705\n",
      "Loss after mini batch    11: 13.041\n",
      "Loss after mini batch    11: 13.349\n",
      "Loss after mini batch    11: 15.085\n",
      "Starting epoch 12\n",
      "Loss after mini batch    12: 14.132\n",
      "Loss after mini batch    12: 13.938\n",
      "Loss after mini batch    12: 12.993\n",
      "Loss after mini batch    12: 14.497\n",
      "Loss after mini batch    12: 17.950\n",
      "Loss after mini batch    12: 15.752\n",
      "Loss after mini batch    12: 14.034\n",
      "Loss after mini batch    12: 17.397\n",
      "Loss after mini batch    12: 14.276\n",
      "Loss after mini batch    12: 14.829\n",
      "Loss after mini batch    12: 14.704\n",
      "Starting epoch 13\n",
      "Loss after mini batch    13: 13.703\n",
      "Loss after mini batch    13: 15.744\n",
      "Loss after mini batch    13: 13.433\n",
      "Loss after mini batch    13: 14.340\n",
      "Loss after mini batch    13: 12.881\n",
      "Loss after mini batch    13: 12.938\n",
      "Loss after mini batch    13: 13.395\n",
      "Loss after mini batch    13: 15.894\n",
      "Loss after mini batch    13: 16.136\n",
      "Loss after mini batch    13: 15.021\n",
      "Loss after mini batch    13: 13.197\n",
      "Starting epoch 14\n",
      "Loss after mini batch    14: 15.269\n",
      "Loss after mini batch    14: 16.703\n",
      "Loss after mini batch    14: 15.107\n",
      "Loss after mini batch    14: 13.650\n",
      "Loss after mini batch    14: 12.507\n",
      "Loss after mini batch    14: 15.994\n",
      "Loss after mini batch    14: 13.468\n",
      "Loss after mini batch    14: 13.745\n",
      "Loss after mini batch    14: 12.998\n",
      "Loss after mini batch    14: 12.679\n",
      "Loss after mini batch    14: 13.778\n",
      "Starting epoch 15\n",
      "Loss after mini batch    15: 15.407\n",
      "Loss after mini batch    15: 12.962\n",
      "Loss after mini batch    15: 15.111\n",
      "Loss after mini batch    15: 14.440\n",
      "Loss after mini batch    15: 18.145\n",
      "Loss after mini batch    15: 13.111\n",
      "Loss after mini batch    15: 12.720\n",
      "Loss after mini batch    15: 14.886\n",
      "Loss after mini batch    15: 12.862\n",
      "Loss after mini batch    15: 15.114\n",
      "Loss after mini batch    15: 13.853\n",
      "Starting epoch 16\n",
      "Loss after mini batch    16: 14.547\n",
      "Loss after mini batch    16: 15.132\n",
      "Loss after mini batch    16: 13.078\n",
      "Loss after mini batch    16: 13.822\n",
      "Loss after mini batch    16: 14.472\n",
      "Loss after mini batch    16: 12.709\n",
      "Loss after mini batch    16: 15.134\n",
      "Loss after mini batch    16: 13.615\n",
      "Loss after mini batch    16: 14.228\n",
      "Loss after mini batch    16: 13.090\n",
      "Loss after mini batch    16: 16.917\n",
      "Starting epoch 17\n",
      "Loss after mini batch    17: 12.741\n",
      "Loss after mini batch    17: 13.541\n",
      "Loss after mini batch    17: 13.512\n",
      "Loss after mini batch    17: 13.735\n",
      "Loss after mini batch    17: 12.896\n",
      "Loss after mini batch    17: 16.937\n",
      "Loss after mini batch    17: 15.078\n",
      "Loss after mini batch    17: 14.123\n",
      "Loss after mini batch    17: 12.691\n",
      "Loss after mini batch    17: 13.041\n",
      "Loss after mini batch    17: 12.892\n",
      "Starting epoch 18\n",
      "Loss after mini batch    18: 13.721\n",
      "Loss after mini batch    18: 13.247\n",
      "Loss after mini batch    18: 13.355\n",
      "Loss after mini batch    18: 12.421\n",
      "Loss after mini batch    18: 16.482\n",
      "Loss after mini batch    18: 14.656\n",
      "Loss after mini batch    18: 14.985\n",
      "Loss after mini batch    18: 13.491\n",
      "Loss after mini batch    18: 12.486\n",
      "Loss after mini batch    18: 13.068\n",
      "Loss after mini batch    18: 13.486\n",
      "Starting epoch 19\n",
      "Loss after mini batch    19: 12.899\n",
      "Loss after mini batch    19: 14.133\n",
      "Loss after mini batch    19: 13.249\n",
      "Loss after mini batch    19: 14.169\n",
      "Loss after mini batch    19: 12.943\n",
      "Loss after mini batch    19: 13.478\n",
      "Loss after mini batch    19: 12.865\n",
      "Loss after mini batch    19: 15.132\n",
      "Loss after mini batch    19: 14.457\n",
      "Loss after mini batch    19: 13.655\n",
      "Loss after mini batch    19: 11.546\n",
      "Starting epoch 20\n",
      "Loss after mini batch    20: 14.722\n",
      "Loss after mini batch    20: 12.116\n",
      "Loss after mini batch    20: 13.870\n",
      "Loss after mini batch    20: 11.914\n",
      "Loss after mini batch    20: 15.418\n",
      "Loss after mini batch    20: 14.468\n",
      "Loss after mini batch    20: 13.264\n",
      "Loss after mini batch    20: 13.677\n",
      "Loss after mini batch    20: 13.716\n",
      "Loss after mini batch    20: 13.517\n",
      "Loss after mini batch    20: 15.671\n",
      "Starting epoch 21\n",
      "Loss after mini batch    21: 12.358\n",
      "Loss after mini batch    21: 14.491\n",
      "Loss after mini batch    21: 12.648\n",
      "Loss after mini batch    21: 13.065\n",
      "Loss after mini batch    21: 13.027\n",
      "Loss after mini batch    21: 15.276\n",
      "Loss after mini batch    21: 13.707\n",
      "Loss after mini batch    21: 13.408\n",
      "Loss after mini batch    21: 14.054\n",
      "Loss after mini batch    21: 13.919\n",
      "Loss after mini batch    21: 12.622\n",
      "Starting epoch 22\n",
      "Loss after mini batch    22: 13.472\n",
      "Loss after mini batch    22: 12.583\n",
      "Loss after mini batch    22: 13.252\n",
      "Loss after mini batch    22: 13.811\n",
      "Loss after mini batch    22: 14.158\n",
      "Loss after mini batch    22: 13.633\n",
      "Loss after mini batch    22: 14.893\n",
      "Loss after mini batch    22: 12.165\n",
      "Loss after mini batch    22: 13.505\n",
      "Loss after mini batch    22: 12.861\n",
      "Loss after mini batch    22: 14.196\n",
      "Starting epoch 23\n",
      "Loss after mini batch    23: 13.521\n",
      "Loss after mini batch    23: 14.254\n",
      "Loss after mini batch    23: 14.226\n",
      "Loss after mini batch    23: 13.732\n",
      "Loss after mini batch    23: 13.641\n",
      "Loss after mini batch    23: 13.023\n",
      "Loss after mini batch    23: 13.710\n",
      "Loss after mini batch    23: 13.620\n",
      "Loss after mini batch    23: 13.182\n",
      "Loss after mini batch    23: 13.515\n",
      "Loss after mini batch    23: 13.775\n",
      "Starting epoch 24\n",
      "Loss after mini batch    24: 12.769\n",
      "Loss after mini batch    24: 13.286\n",
      "Loss after mini batch    24: 15.008\n",
      "Loss after mini batch    24: 13.834\n",
      "Loss after mini batch    24: 14.968\n",
      "Loss after mini batch    24: 13.882\n",
      "Loss after mini batch    24: 13.478\n",
      "Loss after mini batch    24: 13.328\n",
      "Loss after mini batch    24: 13.973\n",
      "Loss after mini batch    24: 14.236\n",
      "Loss after mini batch    24: 13.561\n",
      "Starting epoch 25\n",
      "Loss after mini batch    25: 15.329\n",
      "Loss after mini batch    25: 13.471\n",
      "Loss after mini batch    25: 14.545\n",
      "Loss after mini batch    25: 13.211\n",
      "Loss after mini batch    25: 13.873\n",
      "Loss after mini batch    25: 12.678\n",
      "Loss after mini batch    25: 14.088\n",
      "Loss after mini batch    25: 14.633\n",
      "Loss after mini batch    25: 12.101\n",
      "Loss after mini batch    25: 11.745\n",
      "Loss after mini batch    25: 13.484\n",
      "Starting epoch 26\n",
      "Loss after mini batch    26: 16.715\n",
      "Loss after mini batch    26: 15.584\n",
      "Loss after mini batch    26: 13.192\n",
      "Loss after mini batch    26: 12.514\n",
      "Loss after mini batch    26: 13.580\n",
      "Loss after mini batch    26: 14.010\n",
      "Loss after mini batch    26: 12.226\n",
      "Loss after mini batch    26: 12.494\n",
      "Loss after mini batch    26: 12.926\n",
      "Loss after mini batch    26: 13.156\n",
      "Loss after mini batch    26: 12.907\n",
      "Starting epoch 27\n",
      "Loss after mini batch    27: 14.724\n",
      "Loss after mini batch    27: 14.390\n",
      "Loss after mini batch    27: 12.911\n",
      "Loss after mini batch    27: 13.228\n",
      "Loss after mini batch    27: 13.208\n",
      "Loss after mini batch    27: 14.539\n",
      "Loss after mini batch    27: 13.845\n",
      "Loss after mini batch    27: 12.786\n",
      "Loss after mini batch    27: 11.929\n",
      "Loss after mini batch    27: 12.940\n",
      "Loss after mini batch    27: 13.153\n",
      "Starting epoch 28\n",
      "Loss after mini batch    28: 13.112\n",
      "Loss after mini batch    28: 13.747\n",
      "Loss after mini batch    28: 13.809\n",
      "Loss after mini batch    28: 13.119\n",
      "Loss after mini batch    28: 13.993\n",
      "Loss after mini batch    28: 13.478\n",
      "Loss after mini batch    28: 12.978\n",
      "Loss after mini batch    28: 15.310\n",
      "Loss after mini batch    28: 12.277\n",
      "Loss after mini batch    28: 12.744\n",
      "Loss after mini batch    28: 14.749\n",
      "Starting epoch 29\n",
      "Loss after mini batch    29: 12.660\n",
      "Loss after mini batch    29: 13.163\n",
      "Loss after mini batch    29: 13.786\n",
      "Loss after mini batch    29: 13.012\n",
      "Loss after mini batch    29: 13.494\n",
      "Loss after mini batch    29: 15.038\n",
      "Loss after mini batch    29: 14.310\n",
      "Loss after mini batch    29: 13.487\n",
      "Loss after mini batch    29: 13.502\n",
      "Loss after mini batch    29: 12.517\n",
      "Loss after mini batch    29: 12.919\n",
      "Starting epoch 30\n",
      "Loss after mini batch    30: 13.307\n",
      "Loss after mini batch    30: 13.142\n",
      "Loss after mini batch    30: 14.160\n",
      "Loss after mini batch    30: 13.855\n",
      "Loss after mini batch    30: 12.358\n",
      "Loss after mini batch    30: 13.015\n",
      "Loss after mini batch    30: 12.632\n",
      "Loss after mini batch    30: 13.823\n",
      "Loss after mini batch    30: 12.929\n",
      "Loss after mini batch    30: 13.139\n",
      "Loss after mini batch    30: 12.905\n",
      "Starting epoch 31\n",
      "Loss after mini batch    31: 12.415\n",
      "Loss after mini batch    31: 12.997\n",
      "Loss after mini batch    31: 12.400\n",
      "Loss after mini batch    31: 13.084\n",
      "Loss after mini batch    31: 12.369\n",
      "Loss after mini batch    31: 13.284\n",
      "Loss after mini batch    31: 13.231\n",
      "Loss after mini batch    31: 16.031\n",
      "Loss after mini batch    31: 12.406\n",
      "Loss after mini batch    31: 13.260\n",
      "Loss after mini batch    31: 14.151\n",
      "Starting epoch 32\n",
      "Loss after mini batch    32: 13.557\n",
      "Loss after mini batch    32: 13.956\n",
      "Loss after mini batch    32: 11.357\n",
      "Loss after mini batch    32: 13.144\n",
      "Loss after mini batch    32: 13.863\n",
      "Loss after mini batch    32: 12.670\n",
      "Loss after mini batch    32: 13.222\n",
      "Loss after mini batch    32: 12.904\n",
      "Loss after mini batch    32: 15.382\n",
      "Loss after mini batch    32: 13.361\n",
      "Loss after mini batch    32: 12.739\n",
      "Starting epoch 33\n",
      "Loss after mini batch    33: 15.703\n",
      "Loss after mini batch    33: 13.264\n",
      "Loss after mini batch    33: 12.761\n",
      "Loss after mini batch    33: 13.290\n",
      "Loss after mini batch    33: 13.831\n",
      "Loss after mini batch    33: 12.355\n",
      "Loss after mini batch    33: 12.984\n",
      "Loss after mini batch    33: 13.258\n",
      "Loss after mini batch    33: 14.774\n",
      "Loss after mini batch    33: 12.189\n",
      "Loss after mini batch    33: 13.746\n",
      "Starting epoch 34\n",
      "Loss after mini batch    34: 12.731\n",
      "Loss after mini batch    34: 13.159\n",
      "Loss after mini batch    34: 14.327\n",
      "Loss after mini batch    34: 12.447\n",
      "Loss after mini batch    34: 12.693\n",
      "Loss after mini batch    34: 14.218\n",
      "Loss after mini batch    34: 13.046\n",
      "Loss after mini batch    34: 13.588\n",
      "Loss after mini batch    34: 12.546\n",
      "Loss after mini batch    34: 13.651\n",
      "Loss after mini batch    34: 13.335\n",
      "Starting epoch 35\n",
      "Loss after mini batch    35: 13.843\n",
      "Loss after mini batch    35: 12.815\n",
      "Loss after mini batch    35: 12.735\n",
      "Loss after mini batch    35: 12.886\n",
      "Loss after mini batch    35: 12.674\n",
      "Loss after mini batch    35: 11.769\n",
      "Loss after mini batch    35: 13.058\n",
      "Loss after mini batch    35: 14.004\n",
      "Loss after mini batch    35: 13.045\n",
      "Loss after mini batch    35: 13.988\n",
      "Loss after mini batch    35: 12.072\n",
      "Starting epoch 36\n",
      "Loss after mini batch    36: 14.269\n",
      "Loss after mini batch    36: 11.931\n",
      "Loss after mini batch    36: 13.125\n",
      "Loss after mini batch    36: 13.292\n",
      "Loss after mini batch    36: 12.328\n",
      "Loss after mini batch    36: 12.889\n",
      "Loss after mini batch    36: 12.920\n",
      "Loss after mini batch    36: 12.828\n",
      "Loss after mini batch    36: 12.363\n",
      "Loss after mini batch    36: 14.243\n",
      "Loss after mini batch    36: 15.763\n",
      "Starting epoch 37\n",
      "Loss after mini batch    37: 14.313\n",
      "Loss after mini batch    37: 13.843\n",
      "Loss after mini batch    37: 12.792\n",
      "Loss after mini batch    37: 14.245\n",
      "Loss after mini batch    37: 13.461\n",
      "Loss after mini batch    37: 12.640\n",
      "Loss after mini batch    37: 13.488\n",
      "Loss after mini batch    37: 12.688\n",
      "Loss after mini batch    37: 13.979\n",
      "Loss after mini batch    37: 12.176\n",
      "Loss after mini batch    37: 12.464\n",
      "Starting epoch 38\n",
      "Loss after mini batch    38: 11.925\n",
      "Loss after mini batch    38: 12.331\n",
      "Loss after mini batch    38: 13.561\n",
      "Loss after mini batch    38: 13.288\n",
      "Loss after mini batch    38: 12.887\n",
      "Loss after mini batch    38: 13.634\n",
      "Loss after mini batch    38: 12.879\n",
      "Loss after mini batch    38: 13.697\n",
      "Loss after mini batch    38: 13.469\n",
      "Loss after mini batch    38: 16.221\n",
      "Loss after mini batch    38: 12.728\n",
      "Starting epoch 39\n",
      "Loss after mini batch    39: 13.558\n",
      "Loss after mini batch    39: 14.325\n",
      "Loss after mini batch    39: 15.901\n",
      "Loss after mini batch    39: 14.161\n",
      "Loss after mini batch    39: 12.750\n",
      "Loss after mini batch    39: 12.884\n",
      "Loss after mini batch    39: 12.293\n",
      "Loss after mini batch    39: 12.915\n",
      "Loss after mini batch    39: 12.992\n",
      "Loss after mini batch    39: 13.212\n",
      "Loss after mini batch    39: 12.532\n",
      "Starting epoch 40\n",
      "Loss after mini batch    40: 13.215\n",
      "Loss after mini batch    40: 11.880\n",
      "Loss after mini batch    40: 14.172\n",
      "Loss after mini batch    40: 12.416\n",
      "Loss after mini batch    40: 14.325\n",
      "Loss after mini batch    40: 13.421\n",
      "Loss after mini batch    40: 12.396\n",
      "Loss after mini batch    40: 13.028\n",
      "Loss after mini batch    40: 12.532\n",
      "Loss after mini batch    40: 11.858\n",
      "Loss after mini batch    40: 13.379\n",
      "Starting epoch 41\n",
      "Loss after mini batch    41: 13.272\n",
      "Loss after mini batch    41: 12.741\n",
      "Loss after mini batch    41: 13.093\n",
      "Loss after mini batch    41: 14.349\n",
      "Loss after mini batch    41: 13.229\n",
      "Loss after mini batch    41: 11.281\n",
      "Loss after mini batch    41: 14.178\n",
      "Loss after mini batch    41: 13.755\n",
      "Loss after mini batch    41: 14.889\n",
      "Loss after mini batch    41: 12.593\n",
      "Loss after mini batch    41: 11.398\n",
      "Starting epoch 42\n",
      "Loss after mini batch    42: 12.608\n",
      "Loss after mini batch    42: 12.815\n",
      "Loss after mini batch    42: 13.029\n",
      "Loss after mini batch    42: 12.856\n",
      "Loss after mini batch    42: 11.931\n",
      "Loss after mini batch    42: 12.923\n",
      "Loss after mini batch    42: 13.785\n",
      "Loss after mini batch    42: 12.834\n",
      "Loss after mini batch    42: 12.522\n",
      "Loss after mini batch    42: 13.668\n",
      "Loss after mini batch    42: 14.779\n",
      "Starting epoch 43\n",
      "Loss after mini batch    43: 12.307\n",
      "Loss after mini batch    43: 12.561\n",
      "Loss after mini batch    43: 11.992\n",
      "Loss after mini batch    43: 13.413\n",
      "Loss after mini batch    43: 13.290\n",
      "Loss after mini batch    43: 13.385\n",
      "Loss after mini batch    43: 12.957\n",
      "Loss after mini batch    43: 13.743\n",
      "Loss after mini batch    43: 13.159\n",
      "Loss after mini batch    43: 12.101\n",
      "Loss after mini batch    43: 11.441\n",
      "Starting epoch 44\n",
      "Loss after mini batch    44: 13.706\n",
      "Loss after mini batch    44: 12.519\n",
      "Loss after mini batch    44: 12.770\n",
      "Loss after mini batch    44: 13.693\n",
      "Loss after mini batch    44: 12.494\n",
      "Loss after mini batch    44: 13.284\n",
      "Loss after mini batch    44: 13.004\n",
      "Loss after mini batch    44: 13.013\n",
      "Loss after mini batch    44: 14.406\n",
      "Loss after mini batch    44: 12.492\n",
      "Loss after mini batch    44: 12.643\n",
      "Starting epoch 45\n",
      "Loss after mini batch    45: 12.869\n",
      "Loss after mini batch    45: 11.771\n",
      "Loss after mini batch    45: 12.108\n",
      "Loss after mini batch    45: 13.634\n",
      "Loss after mini batch    45: 12.572\n",
      "Loss after mini batch    45: 14.573\n",
      "Loss after mini batch    45: 12.283\n",
      "Loss after mini batch    45: 11.797\n",
      "Loss after mini batch    45: 13.261\n",
      "Loss after mini batch    45: 12.215\n",
      "Loss after mini batch    45: 13.443\n",
      "Starting epoch 46\n",
      "Loss after mini batch    46: 13.349\n",
      "Loss after mini batch    46: 13.970\n",
      "Loss after mini batch    46: 11.855\n",
      "Loss after mini batch    46: 12.467\n",
      "Loss after mini batch    46: 13.312\n",
      "Loss after mini batch    46: 11.806\n",
      "Loss after mini batch    46: 12.683\n",
      "Loss after mini batch    46: 13.795\n",
      "Loss after mini batch    46: 13.265\n",
      "Loss after mini batch    46: 13.226\n",
      "Loss after mini batch    46: 11.880\n",
      "Starting epoch 47\n",
      "Loss after mini batch    47: 13.249\n",
      "Loss after mini batch    47: 12.921\n",
      "Loss after mini batch    47: 14.388\n",
      "Loss after mini batch    47: 12.890\n",
      "Loss after mini batch    47: 12.474\n",
      "Loss after mini batch    47: 13.682\n",
      "Loss after mini batch    47: 12.933\n",
      "Loss after mini batch    47: 12.251\n",
      "Loss after mini batch    47: 12.433\n",
      "Loss after mini batch    47: 12.065\n",
      "Loss after mini batch    47: 13.252\n",
      "Starting epoch 48\n",
      "Loss after mini batch    48: 14.049\n",
      "Loss after mini batch    48: 13.227\n",
      "Loss after mini batch    48: 12.358\n",
      "Loss after mini batch    48: 14.158\n",
      "Loss after mini batch    48: 14.022\n",
      "Loss after mini batch    48: 13.443\n",
      "Loss after mini batch    48: 12.235\n",
      "Loss after mini batch    48: 13.749\n",
      "Loss after mini batch    48: 12.504\n",
      "Loss after mini batch    48: 12.459\n",
      "Loss after mini batch    48: 12.655\n",
      "Starting epoch 49\n",
      "Loss after mini batch    49: 14.150\n",
      "Loss after mini batch    49: 12.290\n",
      "Loss after mini batch    49: 11.844\n",
      "Loss after mini batch    49: 11.285\n",
      "Loss after mini batch    49: 12.062\n",
      "Loss after mini batch    49: 12.944\n",
      "Loss after mini batch    49: 14.380\n",
      "Loss after mini batch    49: 12.994\n",
      "Loss after mini batch    49: 13.094\n",
      "Loss after mini batch    49: 12.459\n",
      "Loss after mini batch    49: 13.286\n",
      "Starting epoch 50\n",
      "Loss after mini batch    50: 11.723\n",
      "Loss after mini batch    50: 13.953\n",
      "Loss after mini batch    50: 13.896\n",
      "Loss after mini batch    50: 13.306\n",
      "Loss after mini batch    50: 11.644\n",
      "Loss after mini batch    50: 12.496\n",
      "Loss after mini batch    50: 12.662\n",
      "Loss after mini batch    50: 14.323\n",
      "Loss after mini batch    50: 11.957\n",
      "Loss after mini batch    50: 13.535\n",
      "Loss after mini batch    50: 12.363\n",
      "Starting epoch 51\n",
      "Loss after mini batch    51: 12.568\n",
      "Loss after mini batch    51: 12.135\n",
      "Loss after mini batch    51: 12.906\n",
      "Loss after mini batch    51: 11.567\n",
      "Loss after mini batch    51: 13.613\n",
      "Loss after mini batch    51: 14.312\n",
      "Loss after mini batch    51: 13.386\n",
      "Loss after mini batch    51: 12.645\n",
      "Loss after mini batch    51: 13.829\n",
      "Loss after mini batch    51: 12.359\n",
      "Loss after mini batch    51: 12.100\n",
      "Starting epoch 52\n",
      "Loss after mini batch    52: 12.787\n",
      "Loss after mini batch    52: 12.878\n",
      "Loss after mini batch    52: 12.189\n",
      "Loss after mini batch    52: 12.750\n",
      "Loss after mini batch    52: 11.678\n",
      "Loss after mini batch    52: 13.387\n",
      "Loss after mini batch    52: 12.416\n",
      "Loss after mini batch    52: 12.827\n",
      "Loss after mini batch    52: 11.930\n",
      "Loss after mini batch    52: 12.111\n",
      "Loss after mini batch    52: 12.869\n",
      "Starting epoch 53\n",
      "Loss after mini batch    53: 12.784\n",
      "Loss after mini batch    53: 13.413\n",
      "Loss after mini batch    53: 11.357\n",
      "Loss after mini batch    53: 13.639\n",
      "Loss after mini batch    53: 13.051\n",
      "Loss after mini batch    53: 13.725\n",
      "Loss after mini batch    53: 13.395\n",
      "Loss after mini batch    53: 12.280\n",
      "Loss after mini batch    53: 11.941\n",
      "Loss after mini batch    53: 12.641\n",
      "Loss after mini batch    53: 13.102\n",
      "Starting epoch 54\n",
      "Loss after mini batch    54: 12.386\n",
      "Loss after mini batch    54: 11.599\n",
      "Loss after mini batch    54: 13.037\n",
      "Loss after mini batch    54: 12.425\n",
      "Loss after mini batch    54: 12.665\n",
      "Loss after mini batch    54: 12.460\n",
      "Loss after mini batch    54: 13.275\n",
      "Loss after mini batch    54: 11.952\n",
      "Loss after mini batch    54: 12.073\n",
      "Loss after mini batch    54: 12.217\n",
      "Loss after mini batch    54: 13.938\n",
      "Starting epoch 55\n",
      "Loss after mini batch    55: 13.726\n",
      "Loss after mini batch    55: 12.775\n",
      "Loss after mini batch    55: 11.583\n",
      "Loss after mini batch    55: 11.708\n",
      "Loss after mini batch    55: 12.925\n",
      "Loss after mini batch    55: 11.910\n",
      "Loss after mini batch    55: 13.901\n",
      "Loss after mini batch    55: 13.305\n",
      "Loss after mini batch    55: 12.286\n",
      "Loss after mini batch    55: 12.521\n",
      "Loss after mini batch    55: 12.470\n",
      "Starting epoch 56\n",
      "Loss after mini batch    56: 13.236\n",
      "Loss after mini batch    56: 12.963\n",
      "Loss after mini batch    56: 13.231\n",
      "Loss after mini batch    56: 12.840\n",
      "Loss after mini batch    56: 13.120\n",
      "Loss after mini batch    56: 10.676\n",
      "Loss after mini batch    56: 12.154\n",
      "Loss after mini batch    56: 12.890\n",
      "Loss after mini batch    56: 11.746\n",
      "Loss after mini batch    56: 12.795\n",
      "Loss after mini batch    56: 12.763\n",
      "Starting epoch 57\n",
      "Loss after mini batch    57: 11.787\n",
      "Loss after mini batch    57: 14.587\n",
      "Loss after mini batch    57: 14.107\n",
      "Loss after mini batch    57: 12.251\n",
      "Loss after mini batch    57: 12.457\n",
      "Loss after mini batch    57: 12.634\n",
      "Loss after mini batch    57: 14.304\n",
      "Loss after mini batch    57: 12.352\n",
      "Loss after mini batch    57: 11.635\n",
      "Loss after mini batch    57: 14.202\n",
      "Loss after mini batch    57: 11.749\n",
      "Starting epoch 58\n",
      "Loss after mini batch    58: 11.335\n",
      "Loss after mini batch    58: 13.729\n",
      "Loss after mini batch    58: 13.176\n",
      "Loss after mini batch    58: 11.881\n",
      "Loss after mini batch    58: 11.483\n",
      "Loss after mini batch    58: 12.857\n",
      "Loss after mini batch    58: 12.563\n",
      "Loss after mini batch    58: 12.288\n",
      "Loss after mini batch    58: 14.192\n",
      "Loss after mini batch    58: 13.433\n",
      "Loss after mini batch    58: 13.529\n",
      "Starting epoch 59\n",
      "Loss after mini batch    59: 13.330\n",
      "Loss after mini batch    59: 13.490\n",
      "Loss after mini batch    59: 11.783\n",
      "Loss after mini batch    59: 11.681\n",
      "Loss after mini batch    59: 12.647\n",
      "Loss after mini batch    59: 12.883\n",
      "Loss after mini batch    59: 12.276\n",
      "Loss after mini batch    59: 12.680\n",
      "Loss after mini batch    59: 12.381\n",
      "Loss after mini batch    59: 13.154\n",
      "Loss after mini batch    59: 11.982\n",
      "Starting epoch 60\n",
      "Loss after mini batch    60: 12.952\n",
      "Loss after mini batch    60: 12.607\n",
      "Loss after mini batch    60: 13.000\n",
      "Loss after mini batch    60: 12.498\n",
      "Loss after mini batch    60: 13.162\n",
      "Loss after mini batch    60: 12.048\n",
      "Loss after mini batch    60: 12.388\n",
      "Loss after mini batch    60: 12.350\n",
      "Loss after mini batch    60: 12.425\n",
      "Loss after mini batch    60: 11.698\n",
      "Loss after mini batch    60: 13.062\n",
      "Starting epoch 61\n",
      "Loss after mini batch    61: 11.209\n",
      "Loss after mini batch    61: 11.638\n",
      "Loss after mini batch    61: 14.341\n",
      "Loss after mini batch    61: 11.422\n",
      "Loss after mini batch    61: 13.260\n",
      "Loss after mini batch    61: 13.751\n",
      "Loss after mini batch    61: 12.399\n",
      "Loss after mini batch    61: 13.083\n",
      "Loss after mini batch    61: 12.324\n",
      "Loss after mini batch    61: 12.808\n",
      "Loss after mini batch    61: 12.197\n",
      "Starting epoch 62\n",
      "Loss after mini batch    62: 12.118\n",
      "Loss after mini batch    62: 12.503\n",
      "Loss after mini batch    62: 13.757\n",
      "Loss after mini batch    62: 11.867\n",
      "Loss after mini batch    62: 13.497\n",
      "Loss after mini batch    62: 12.182\n",
      "Loss after mini batch    62: 13.057\n",
      "Loss after mini batch    62: 13.104\n",
      "Loss after mini batch    62: 12.475\n",
      "Loss after mini batch    62: 12.475\n",
      "Loss after mini batch    62: 13.057\n",
      "Starting epoch 63\n",
      "Loss after mini batch    63: 11.543\n",
      "Loss after mini batch    63: 12.871\n",
      "Loss after mini batch    63: 12.457\n",
      "Loss after mini batch    63: 12.660\n",
      "Loss after mini batch    63: 13.735\n",
      "Loss after mini batch    63: 11.699\n",
      "Loss after mini batch    63: 13.461\n",
      "Loss after mini batch    63: 12.385\n",
      "Loss after mini batch    63: 12.473\n",
      "Loss after mini batch    63: 12.956\n",
      "Loss after mini batch    63: 12.478\n",
      "Starting epoch 64\n",
      "Loss after mini batch    64: 13.834\n",
      "Loss after mini batch    64: 11.460\n",
      "Loss after mini batch    64: 12.448\n",
      "Loss after mini batch    64: 12.142\n",
      "Loss after mini batch    64: 12.829\n",
      "Loss after mini batch    64: 13.002\n",
      "Loss after mini batch    64: 12.818\n",
      "Loss after mini batch    64: 13.999\n",
      "Loss after mini batch    64: 13.463\n",
      "Loss after mini batch    64: 10.781\n",
      "Loss after mini batch    64: 11.281\n",
      "Starting epoch 65\n",
      "Loss after mini batch    65: 13.335\n",
      "Loss after mini batch    65: 13.160\n",
      "Loss after mini batch    65: 11.653\n",
      "Loss after mini batch    65: 13.872\n",
      "Loss after mini batch    65: 12.686\n",
      "Loss after mini batch    65: 12.565\n",
      "Loss after mini batch    65: 12.989\n",
      "Loss after mini batch    65: 12.119\n",
      "Loss after mini batch    65: 12.124\n",
      "Loss after mini batch    65: 11.639\n",
      "Loss after mini batch    65: 11.473\n",
      "Starting epoch 66\n",
      "Loss after mini batch    66: 13.266\n",
      "Loss after mini batch    66: 11.870\n",
      "Loss after mini batch    66: 13.933\n",
      "Loss after mini batch    66: 12.242\n",
      "Loss after mini batch    66: 11.966\n",
      "Loss after mini batch    66: 12.901\n",
      "Loss after mini batch    66: 11.797\n",
      "Loss after mini batch    66: 12.098\n",
      "Loss after mini batch    66: 11.719\n",
      "Loss after mini batch    66: 13.102\n",
      "Loss after mini batch    66: 13.515\n",
      "Starting epoch 67\n",
      "Loss after mini batch    67: 11.229\n",
      "Loss after mini batch    67: 13.489\n",
      "Loss after mini batch    67: 13.039\n",
      "Loss after mini batch    67: 13.139\n",
      "Loss after mini batch    67: 12.324\n",
      "Loss after mini batch    67: 11.666\n",
      "Loss after mini batch    67: 13.792\n",
      "Loss after mini batch    67: 12.222\n",
      "Loss after mini batch    67: 12.314\n",
      "Loss after mini batch    67: 13.012\n",
      "Loss after mini batch    67: 13.099\n",
      "Starting epoch 68\n",
      "Loss after mini batch    68: 13.303\n",
      "Loss after mini batch    68: 13.345\n",
      "Loss after mini batch    68: 15.056\n",
      "Loss after mini batch    68: 12.790\n",
      "Loss after mini batch    68: 11.357\n",
      "Loss after mini batch    68: 12.002\n",
      "Loss after mini batch    68: 13.262\n",
      "Loss after mini batch    68: 11.888\n",
      "Loss after mini batch    68: 12.062\n",
      "Loss after mini batch    68: 13.077\n",
      "Loss after mini batch    68: 11.093\n",
      "Starting epoch 69\n",
      "Loss after mini batch    69: 11.547\n",
      "Loss after mini batch    69: 12.528\n",
      "Loss after mini batch    69: 12.931\n",
      "Loss after mini batch    69: 11.862\n",
      "Loss after mini batch    69: 12.491\n",
      "Loss after mini batch    69: 13.104\n",
      "Loss after mini batch    69: 13.062\n",
      "Loss after mini batch    69: 15.371\n",
      "Loss after mini batch    69: 13.174\n",
      "Loss after mini batch    69: 13.478\n",
      "Loss after mini batch    69: 12.246\n",
      "Starting epoch 70\n",
      "Loss after mini batch    70: 13.010\n",
      "Loss after mini batch    70: 12.752\n",
      "Loss after mini batch    70: 11.412\n",
      "Loss after mini batch    70: 13.101\n",
      "Loss after mini batch    70: 12.775\n",
      "Loss after mini batch    70: 13.108\n",
      "Loss after mini batch    70: 13.360\n",
      "Loss after mini batch    70: 12.205\n",
      "Loss after mini batch    70: 12.525\n",
      "Loss after mini batch    70: 13.063\n",
      "Loss after mini batch    70: 12.111\n",
      "Starting epoch 71\n",
      "Loss after mini batch    71: 12.161\n",
      "Loss after mini batch    71: 14.009\n",
      "Loss after mini batch    71: 12.534\n",
      "Loss after mini batch    71: 13.946\n",
      "Loss after mini batch    71: 13.145\n",
      "Loss after mini batch    71: 12.633\n",
      "Loss after mini batch    71: 11.058\n",
      "Loss after mini batch    71: 14.588\n",
      "Loss after mini batch    71: 11.047\n",
      "Loss after mini batch    71: 12.183\n",
      "Loss after mini batch    71: 13.218\n",
      "Starting epoch 72\n",
      "Loss after mini batch    72: 13.350\n",
      "Loss after mini batch    72: 11.099\n",
      "Loss after mini batch    72: 13.056\n",
      "Loss after mini batch    72: 12.118\n",
      "Loss after mini batch    72: 12.894\n",
      "Loss after mini batch    72: 11.762\n",
      "Loss after mini batch    72: 13.479\n",
      "Loss after mini batch    72: 12.063\n",
      "Loss after mini batch    72: 13.747\n",
      "Loss after mini batch    72: 12.086\n",
      "Loss after mini batch    72: 11.834\n",
      "Starting epoch 73\n",
      "Loss after mini batch    73: 14.219\n",
      "Loss after mini batch    73: 11.955\n",
      "Loss after mini batch    73: 12.817\n",
      "Loss after mini batch    73: 13.240\n",
      "Loss after mini batch    73: 12.059\n",
      "Loss after mini batch    73: 12.801\n",
      "Loss after mini batch    73: 11.785\n",
      "Loss after mini batch    73: 12.568\n",
      "Loss after mini batch    73: 11.666\n",
      "Loss after mini batch    73: 12.168\n",
      "Loss after mini batch    73: 12.660\n",
      "Starting epoch 74\n",
      "Loss after mini batch    74: 11.192\n",
      "Loss after mini batch    74: 12.536\n",
      "Loss after mini batch    74: 11.796\n",
      "Loss after mini batch    74: 12.197\n",
      "Loss after mini batch    74: 14.670\n",
      "Loss after mini batch    74: 13.409\n",
      "Loss after mini batch    74: 12.846\n",
      "Loss after mini batch    74: 13.075\n",
      "Loss after mini batch    74: 11.751\n",
      "Loss after mini batch    74: 12.829\n",
      "Loss after mini batch    74: 12.231\n",
      "Starting epoch 75\n",
      "Loss after mini batch    75: 13.692\n",
      "Loss after mini batch    75: 13.515\n",
      "Loss after mini batch    75: 11.882\n",
      "Loss after mini batch    75: 12.711\n",
      "Loss after mini batch    75: 12.125\n",
      "Loss after mini batch    75: 12.138\n",
      "Loss after mini batch    75: 11.921\n",
      "Loss after mini batch    75: 12.802\n",
      "Loss after mini batch    75: 12.811\n",
      "Loss after mini batch    75: 13.011\n",
      "Loss after mini batch    75: 11.688\n",
      "Starting epoch 76\n",
      "Loss after mini batch    76: 12.047\n",
      "Loss after mini batch    76: 12.065\n",
      "Loss after mini batch    76: 11.204\n",
      "Loss after mini batch    76: 12.061\n",
      "Loss after mini batch    76: 11.886\n",
      "Loss after mini batch    76: 13.904\n",
      "Loss after mini batch    76: 13.144\n",
      "Loss after mini batch    76: 12.102\n",
      "Loss after mini batch    76: 11.672\n",
      "Loss after mini batch    76: 12.258\n",
      "Loss after mini batch    76: 13.183\n",
      "Starting epoch 77\n",
      "Loss after mini batch    77: 12.750\n",
      "Loss after mini batch    77: 11.896\n",
      "Loss after mini batch    77: 13.252\n",
      "Loss after mini batch    77: 12.418\n",
      "Loss after mini batch    77: 11.866\n",
      "Loss after mini batch    77: 13.919\n",
      "Loss after mini batch    77: 11.712\n",
      "Loss after mini batch    77: 12.825\n",
      "Loss after mini batch    77: 12.423\n",
      "Loss after mini batch    77: 12.075\n",
      "Loss after mini batch    77: 12.299\n",
      "Starting epoch 78\n",
      "Loss after mini batch    78: 13.034\n",
      "Loss after mini batch    78: 12.926\n",
      "Loss after mini batch    78: 12.602\n",
      "Loss after mini batch    78: 12.295\n",
      "Loss after mini batch    78: 11.431\n",
      "Loss after mini batch    78: 13.368\n",
      "Loss after mini batch    78: 13.136\n",
      "Loss after mini batch    78: 12.235\n",
      "Loss after mini batch    78: 11.263\n",
      "Loss after mini batch    78: 12.472\n",
      "Loss after mini batch    78: 11.357\n",
      "Starting epoch 79\n",
      "Loss after mini batch    79: 11.949\n",
      "Loss after mini batch    79: 12.145\n",
      "Loss after mini batch    79: 12.369\n",
      "Loss after mini batch    79: 12.967\n",
      "Loss after mini batch    79: 13.093\n",
      "Loss after mini batch    79: 12.860\n",
      "Loss after mini batch    79: 12.312\n",
      "Loss after mini batch    79: 12.112\n",
      "Loss after mini batch    79: 12.764\n",
      "Loss after mini batch    79: 11.496\n",
      "Loss after mini batch    79: 12.311\n",
      "Starting epoch 80\n",
      "Loss after mini batch    80: 12.735\n",
      "Loss after mini batch    80: 13.154\n",
      "Loss after mini batch    80: 11.388\n",
      "Loss after mini batch    80: 11.682\n",
      "Loss after mini batch    80: 12.827\n",
      "Loss after mini batch    80: 12.757\n",
      "Loss after mini batch    80: 12.908\n",
      "Loss after mini batch    80: 12.268\n",
      "Loss after mini batch    80: 12.092\n",
      "Loss after mini batch    80: 12.371\n",
      "Loss after mini batch    80: 12.147\n",
      "Starting epoch 81\n",
      "Loss after mini batch    81: 12.258\n",
      "Loss after mini batch    81: 12.571\n",
      "Loss after mini batch    81: 13.155\n",
      "Loss after mini batch    81: 13.015\n",
      "Loss after mini batch    81: 12.771\n",
      "Loss after mini batch    81: 13.432\n",
      "Loss after mini batch    81: 12.352\n",
      "Loss after mini batch    81: 12.544\n",
      "Loss after mini batch    81: 11.465\n",
      "Loss after mini batch    81: 11.452\n",
      "Loss after mini batch    81: 12.104\n",
      "Starting epoch 82\n",
      "Loss after mini batch    82: 12.730\n",
      "Loss after mini batch    82: 12.437\n",
      "Loss after mini batch    82: 11.450\n",
      "Loss after mini batch    82: 12.542\n",
      "Loss after mini batch    82: 13.778\n",
      "Loss after mini batch    82: 13.716\n",
      "Loss after mini batch    82: 12.112\n",
      "Loss after mini batch    82: 11.511\n",
      "Loss after mini batch    82: 12.233\n",
      "Loss after mini batch    82: 13.034\n",
      "Loss after mini batch    82: 13.263\n",
      "Starting epoch 83\n",
      "Loss after mini batch    83: 12.007\n",
      "Loss after mini batch    83: 11.690\n",
      "Loss after mini batch    83: 13.350\n",
      "Loss after mini batch    83: 11.365\n",
      "Loss after mini batch    83: 12.562\n",
      "Loss after mini batch    83: 13.206\n",
      "Loss after mini batch    83: 11.529\n",
      "Loss after mini batch    83: 11.512\n",
      "Loss after mini batch    83: 12.117\n",
      "Loss after mini batch    83: 13.305\n",
      "Loss after mini batch    83: 13.304\n",
      "Starting epoch 84\n",
      "Loss after mini batch    84: 12.095\n",
      "Loss after mini batch    84: 13.042\n",
      "Loss after mini batch    84: 13.536\n",
      "Loss after mini batch    84: 11.520\n",
      "Loss after mini batch    84: 11.957\n",
      "Loss after mini batch    84: 11.713\n",
      "Loss after mini batch    84: 11.966\n",
      "Loss after mini batch    84: 12.814\n",
      "Loss after mini batch    84: 11.934\n",
      "Loss after mini batch    84: 12.809\n",
      "Loss after mini batch    84: 13.063\n",
      "Starting epoch 85\n",
      "Loss after mini batch    85: 11.666\n",
      "Loss after mini batch    85: 12.064\n",
      "Loss after mini batch    85: 11.833\n",
      "Loss after mini batch    85: 12.356\n",
      "Loss after mini batch    85: 12.615\n",
      "Loss after mini batch    85: 12.378\n",
      "Loss after mini batch    85: 11.902\n",
      "Loss after mini batch    85: 13.226\n",
      "Loss after mini batch    85: 11.068\n",
      "Loss after mini batch    85: 12.786\n",
      "Loss after mini batch    85: 12.764\n",
      "Starting epoch 86\n",
      "Loss after mini batch    86: 13.004\n",
      "Loss after mini batch    86: 11.832\n",
      "Loss after mini batch    86: 11.809\n",
      "Loss after mini batch    86: 14.383\n",
      "Loss after mini batch    86: 12.030\n",
      "Loss after mini batch    86: 11.557\n",
      "Loss after mini batch    86: 11.691\n",
      "Loss after mini batch    86: 12.361\n",
      "Loss after mini batch    86: 13.050\n",
      "Loss after mini batch    86: 12.384\n",
      "Loss after mini batch    86: 13.811\n",
      "Starting epoch 87\n",
      "Loss after mini batch    87: 12.451\n",
      "Loss after mini batch    87: 12.234\n",
      "Loss after mini batch    87: 13.354\n",
      "Loss after mini batch    87: 11.657\n",
      "Loss after mini batch    87: 12.610\n",
      "Loss after mini batch    87: 13.220\n",
      "Loss after mini batch    87: 12.822\n",
      "Loss after mini batch    87: 12.086\n",
      "Loss after mini batch    87: 11.024\n",
      "Loss after mini batch    87: 12.169\n",
      "Loss after mini batch    87: 12.818\n",
      "Starting epoch 88\n",
      "Loss after mini batch    88: 11.702\n",
      "Loss after mini batch    88: 11.375\n",
      "Loss after mini batch    88: 12.154\n",
      "Loss after mini batch    88: 13.318\n",
      "Loss after mini batch    88: 11.956\n",
      "Loss after mini batch    88: 13.282\n",
      "Loss after mini batch    88: 14.506\n",
      "Loss after mini batch    88: 11.677\n",
      "Loss after mini batch    88: 11.904\n",
      "Loss after mini batch    88: 11.987\n",
      "Loss after mini batch    88: 12.433\n",
      "Starting epoch 89\n",
      "Loss after mini batch    89: 11.983\n",
      "Loss after mini batch    89: 11.720\n",
      "Loss after mini batch    89: 13.879\n",
      "Loss after mini batch    89: 12.474\n",
      "Loss after mini batch    89: 12.959\n",
      "Loss after mini batch    89: 12.858\n",
      "Loss after mini batch    89: 11.988\n",
      "Loss after mini batch    89: 11.989\n",
      "Loss after mini batch    89: 13.488\n",
      "Loss after mini batch    89: 12.026\n",
      "Loss after mini batch    89: 13.487\n",
      "Starting epoch 90\n",
      "Loss after mini batch    90: 13.609\n",
      "Loss after mini batch    90: 12.992\n",
      "Loss after mini batch    90: 11.684\n",
      "Loss after mini batch    90: 13.411\n",
      "Loss after mini batch    90: 12.076\n",
      "Loss after mini batch    90: 11.772\n",
      "Loss after mini batch    90: 13.395\n",
      "Loss after mini batch    90: 12.288\n",
      "Loss after mini batch    90: 11.534\n",
      "Loss after mini batch    90: 12.952\n",
      "Loss after mini batch    90: 10.719\n",
      "Starting epoch 91\n",
      "Loss after mini batch    91: 12.584\n",
      "Loss after mini batch    91: 12.344\n",
      "Loss after mini batch    91: 12.586\n",
      "Loss after mini batch    91: 12.766\n",
      "Loss after mini batch    91: 11.444\n",
      "Loss after mini batch    91: 12.835\n",
      "Loss after mini batch    91: 13.274\n",
      "Loss after mini batch    91: 12.228\n",
      "Loss after mini batch    91: 12.637\n",
      "Loss after mini batch    91: 12.372\n",
      "Loss after mini batch    91: 12.206\n",
      "Starting epoch 92\n",
      "Loss after mini batch    92: 11.972\n",
      "Loss after mini batch    92: 13.037\n",
      "Loss after mini batch    92: 11.869\n",
      "Loss after mini batch    92: 12.655\n",
      "Loss after mini batch    92: 12.615\n",
      "Loss after mini batch    92: 12.067\n",
      "Loss after mini batch    92: 12.142\n",
      "Loss after mini batch    92: 11.501\n",
      "Loss after mini batch    92: 13.182\n",
      "Loss after mini batch    92: 11.324\n",
      "Loss after mini batch    92: 12.788\n",
      "Starting epoch 93\n",
      "Loss after mini batch    93: 12.028\n",
      "Loss after mini batch    93: 11.907\n",
      "Loss after mini batch    93: 13.167\n",
      "Loss after mini batch    93: 11.474\n",
      "Loss after mini batch    93: 12.620\n",
      "Loss after mini batch    93: 11.632\n",
      "Loss after mini batch    93: 11.689\n",
      "Loss after mini batch    93: 11.541\n",
      "Loss after mini batch    93: 12.445\n",
      "Loss after mini batch    93: 12.852\n",
      "Loss after mini batch    93: 12.107\n",
      "Starting epoch 94\n",
      "Loss after mini batch    94: 12.121\n",
      "Loss after mini batch    94: 11.961\n",
      "Loss after mini batch    94: 13.265\n",
      "Loss after mini batch    94: 13.036\n",
      "Loss after mini batch    94: 11.344\n",
      "Loss after mini batch    94: 12.443\n",
      "Loss after mini batch    94: 12.077\n",
      "Loss after mini batch    94: 13.426\n",
      "Loss after mini batch    94: 12.808\n",
      "Loss after mini batch    94: 13.059\n",
      "Loss after mini batch    94: 12.638\n",
      "Starting epoch 95\n",
      "Loss after mini batch    95: 11.943\n",
      "Loss after mini batch    95: 14.531\n",
      "Loss after mini batch    95: 10.947\n",
      "Loss after mini batch    95: 12.795\n",
      "Loss after mini batch    95: 11.151\n",
      "Loss after mini batch    95: 12.903\n",
      "Loss after mini batch    95: 11.666\n",
      "Loss after mini batch    95: 11.849\n",
      "Loss after mini batch    95: 11.477\n",
      "Loss after mini batch    95: 12.826\n",
      "Loss after mini batch    95: 13.181\n",
      "Starting epoch 96\n",
      "Loss after mini batch    96: 12.272\n",
      "Loss after mini batch    96: 12.592\n",
      "Loss after mini batch    96: 12.402\n",
      "Loss after mini batch    96: 11.700\n",
      "Loss after mini batch    96: 13.403\n",
      "Loss after mini batch    96: 12.209\n",
      "Loss after mini batch    96: 11.986\n",
      "Loss after mini batch    96: 14.123\n",
      "Loss after mini batch    96: 10.856\n",
      "Loss after mini batch    96: 12.480\n",
      "Loss after mini batch    96: 11.530\n",
      "Starting epoch 97\n",
      "Loss after mini batch    97: 13.804\n",
      "Loss after mini batch    97: 13.141\n",
      "Loss after mini batch    97: 12.935\n",
      "Loss after mini batch    97: 13.099\n",
      "Loss after mini batch    97: 12.457\n",
      "Loss after mini batch    97: 12.224\n",
      "Loss after mini batch    97: 12.403\n",
      "Loss after mini batch    97: 11.523\n",
      "Loss after mini batch    97: 13.100\n",
      "Loss after mini batch    97: 10.304\n",
      "Loss after mini batch    97: 12.215\n",
      "Starting epoch 98\n",
      "Loss after mini batch    98: 12.459\n",
      "Loss after mini batch    98: 12.851\n",
      "Loss after mini batch    98: 12.028\n",
      "Loss after mini batch    98: 12.522\n",
      "Loss after mini batch    98: 12.962\n",
      "Loss after mini batch    98: 11.756\n",
      "Loss after mini batch    98: 12.578\n",
      "Loss after mini batch    98: 11.028\n",
      "Loss after mini batch    98: 11.674\n",
      "Loss after mini batch    98: 14.235\n",
      "Loss after mini batch    98: 12.315\n",
      "Starting epoch 99\n",
      "Loss after mini batch    99: 12.332\n",
      "Loss after mini batch    99: 12.888\n",
      "Loss after mini batch    99: 11.425\n",
      "Loss after mini batch    99: 11.839\n",
      "Loss after mini batch    99: 13.082\n",
      "Loss after mini batch    99: 12.453\n",
      "Loss after mini batch    99: 13.186\n",
      "Loss after mini batch    99: 13.043\n",
      "Loss after mini batch    99: 11.397\n",
      "Loss after mini batch    99: 12.149\n",
      "Loss after mini batch    99: 12.394\n",
      "Starting epoch 100\n",
      "Loss after mini batch   100: 12.733\n",
      "Loss after mini batch   100: 13.378\n",
      "Loss after mini batch   100: 11.969\n",
      "Loss after mini batch   100: 12.024\n",
      "Loss after mini batch   100: 12.890\n",
      "Loss after mini batch   100: 12.558\n",
      "Loss after mini batch   100: 12.306\n",
      "Loss after mini batch   100: 11.650\n",
      "Loss after mini batch   100: 12.942\n",
      "Loss after mini batch   100: 12.192\n",
      "Loss after mini batch   100: 11.660\n",
      "Training Complete\n",
      "Starting Testing\n",
      "RMSE for fold 9: 3.7556572521016443\n",
      "rRMSE for fold 9: 0.07216009970658954\n",
      "r for fold 9: 0.9900039097132619\n",
      "Fast RMSE for fold 9: 3.454985242665817\n",
      "Fast rRMSE for fold 9: 0.06787983474391372\n",
      "Fast r for fold 9: 0.9810181020539598\n",
      "Slow RMSE for fold 9: 5.317782300334416\n",
      "Slow rRMSE for fold 9: 0.10289448166679008\n",
      "Slow r for fold 9: 0.9980223052981574\n",
      "Regular RMSE for fold 9: 2.079611110645689\n",
      "Regular rRMSE for fold 9: 0.038886573588191704\n",
      "Regular r for fold 9: 0.9972316425720893\n",
      "Fold 10\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini batch     1: 289.488\n",
      "Loss after mini batch     1: 20.032\n",
      "Loss after mini batch     1: 26.522\n",
      "Loss after mini batch     1: 21.303\n",
      "Loss after mini batch     1: 20.352\n",
      "Loss after mini batch     1: 22.793\n",
      "Loss after mini batch     1: 20.652\n",
      "Loss after mini batch     1: 18.851\n",
      "Loss after mini batch     1: 22.136\n",
      "Loss after mini batch     1: 20.387\n",
      "Loss after mini batch     1: 21.527\n",
      "Starting epoch 2\n",
      "Loss after mini batch     2: 20.289\n",
      "Loss after mini batch     2: 24.920\n",
      "Loss after mini batch     2: 25.477\n",
      "Loss after mini batch     2: 17.916\n",
      "Loss after mini batch     2: 19.546\n",
      "Loss after mini batch     2: 27.760\n",
      "Loss after mini batch     2: 18.899\n",
      "Loss after mini batch     2: 24.809\n",
      "Loss after mini batch     2: 17.513\n",
      "Loss after mini batch     2: 15.530\n",
      "Loss after mini batch     2: 20.269\n",
      "Starting epoch 3\n",
      "Loss after mini batch     3: 18.309\n",
      "Loss after mini batch     3: 16.756\n",
      "Loss after mini batch     3: 16.349\n",
      "Loss after mini batch     3: 20.318\n",
      "Loss after mini batch     3: 15.880\n",
      "Loss after mini batch     3: 25.797\n",
      "Loss after mini batch     3: 23.085\n",
      "Loss after mini batch     3: 16.910\n",
      "Loss after mini batch     3: 15.897\n",
      "Loss after mini batch     3: 17.485\n",
      "Loss after mini batch     3: 19.447\n",
      "Starting epoch 4\n",
      "Loss after mini batch     4: 20.641\n",
      "Loss after mini batch     4: 21.903\n",
      "Loss after mini batch     4: 14.566\n",
      "Loss after mini batch     4: 13.612\n",
      "Loss after mini batch     4: 15.819\n",
      "Loss after mini batch     4: 19.991\n",
      "Loss after mini batch     4: 16.754\n",
      "Loss after mini batch     4: 17.667\n",
      "Loss after mini batch     4: 16.190\n",
      "Loss after mini batch     4: 14.759\n",
      "Loss after mini batch     4: 21.018\n",
      "Starting epoch 5\n",
      "Loss after mini batch     5: 14.791\n",
      "Loss after mini batch     5: 13.918\n",
      "Loss after mini batch     5: 18.079\n",
      "Loss after mini batch     5: 27.116\n",
      "Loss after mini batch     5: 14.753\n",
      "Loss after mini batch     5: 16.833\n",
      "Loss after mini batch     5: 13.562\n",
      "Loss after mini batch     5: 17.208\n",
      "Loss after mini batch     5: 14.819\n",
      "Loss after mini batch     5: 14.408\n",
      "Loss after mini batch     5: 15.474\n",
      "Starting epoch 6\n",
      "Loss after mini batch     6: 15.935\n",
      "Loss after mini batch     6: 15.412\n",
      "Loss after mini batch     6: 14.696\n",
      "Loss after mini batch     6: 13.893\n",
      "Loss after mini batch     6: 14.348\n",
      "Loss after mini batch     6: 15.030\n",
      "Loss after mini batch     6: 16.567\n",
      "Loss after mini batch     6: 19.799\n",
      "Loss after mini batch     6: 13.936\n",
      "Loss after mini batch     6: 14.880\n",
      "Loss after mini batch     6: 14.221\n",
      "Starting epoch 7\n",
      "Loss after mini batch     7: 15.622\n",
      "Loss after mini batch     7: 14.735\n",
      "Loss after mini batch     7: 14.063\n",
      "Loss after mini batch     7: 14.325\n",
      "Loss after mini batch     7: 13.425\n",
      "Loss after mini batch     7: 18.256\n",
      "Loss after mini batch     7: 14.563\n",
      "Loss after mini batch     7: 16.213\n",
      "Loss after mini batch     7: 13.918\n",
      "Loss after mini batch     7: 12.684\n",
      "Loss after mini batch     7: 16.848\n",
      "Starting epoch 8\n",
      "Loss after mini batch     8: 13.365\n",
      "Loss after mini batch     8: 18.104\n",
      "Loss after mini batch     8: 15.072\n",
      "Loss after mini batch     8: 14.483\n",
      "Loss after mini batch     8: 11.971\n",
      "Loss after mini batch     8: 16.638\n",
      "Loss after mini batch     8: 18.599\n",
      "Loss after mini batch     8: 16.153\n",
      "Loss after mini batch     8: 16.858\n",
      "Loss after mini batch     8: 14.901\n",
      "Loss after mini batch     8: 15.283\n",
      "Starting epoch 9\n",
      "Loss after mini batch     9: 17.046\n",
      "Loss after mini batch     9: 15.712\n",
      "Loss after mini batch     9: 14.948\n",
      "Loss after mini batch     9: 11.369\n",
      "Loss after mini batch     9: 13.471\n",
      "Loss after mini batch     9: 14.166\n",
      "Loss after mini batch     9: 14.100\n",
      "Loss after mini batch     9: 15.309\n",
      "Loss after mini batch     9: 14.287\n",
      "Loss after mini batch     9: 13.780\n",
      "Loss after mini batch     9: 14.511\n",
      "Starting epoch 10\n",
      "Loss after mini batch    10: 14.469\n",
      "Loss after mini batch    10: 14.413\n",
      "Loss after mini batch    10: 14.591\n",
      "Loss after mini batch    10: 12.121\n",
      "Loss after mini batch    10: 16.008\n",
      "Loss after mini batch    10: 12.784\n",
      "Loss after mini batch    10: 14.349\n",
      "Loss after mini batch    10: 13.520\n",
      "Loss after mini batch    10: 13.884\n",
      "Loss after mini batch    10: 13.818\n",
      "Loss after mini batch    10: 14.447\n",
      "Starting epoch 11\n",
      "Loss after mini batch    11: 13.445\n",
      "Loss after mini batch    11: 16.260\n",
      "Loss after mini batch    11: 13.304\n",
      "Loss after mini batch    11: 13.390\n",
      "Loss after mini batch    11: 13.154\n",
      "Loss after mini batch    11: 13.008\n",
      "Loss after mini batch    11: 15.658\n",
      "Loss after mini batch    11: 17.955\n",
      "Loss after mini batch    11: 15.726\n",
      "Loss after mini batch    11: 13.550\n",
      "Loss after mini batch    11: 14.449\n",
      "Starting epoch 12\n",
      "Loss after mini batch    12: 15.256\n",
      "Loss after mini batch    12: 13.025\n",
      "Loss after mini batch    12: 12.540\n",
      "Loss after mini batch    12: 16.117\n",
      "Loss after mini batch    12: 13.302\n",
      "Loss after mini batch    12: 15.458\n",
      "Loss after mini batch    12: 13.600\n",
      "Loss after mini batch    12: 14.052\n",
      "Loss after mini batch    12: 13.686\n",
      "Loss after mini batch    12: 16.325\n",
      "Loss after mini batch    12: 14.750\n",
      "Starting epoch 13\n",
      "Loss after mini batch    13: 15.758\n",
      "Loss after mini batch    13: 14.232\n",
      "Loss after mini batch    13: 14.082\n",
      "Loss after mini batch    13: 13.656\n",
      "Loss after mini batch    13: 13.356\n",
      "Loss after mini batch    13: 13.931\n",
      "Loss after mini batch    13: 13.342\n",
      "Loss after mini batch    13: 14.864\n",
      "Loss after mini batch    13: 13.015\n",
      "Loss after mini batch    13: 15.153\n",
      "Loss after mini batch    13: 13.130\n",
      "Starting epoch 14\n",
      "Loss after mini batch    14: 13.888\n",
      "Loss after mini batch    14: 13.285\n",
      "Loss after mini batch    14: 15.225\n",
      "Loss after mini batch    14: 12.525\n",
      "Loss after mini batch    14: 12.341\n",
      "Loss after mini batch    14: 14.175\n",
      "Loss after mini batch    14: 14.472\n",
      "Loss after mini batch    14: 12.524\n",
      "Loss after mini batch    14: 14.834\n",
      "Loss after mini batch    14: 14.936\n",
      "Loss after mini batch    14: 14.491\n",
      "Starting epoch 15\n",
      "Loss after mini batch    15: 14.604\n",
      "Loss after mini batch    15: 13.693\n",
      "Loss after mini batch    15: 12.549\n",
      "Loss after mini batch    15: 13.511\n",
      "Loss after mini batch    15: 15.684\n",
      "Loss after mini batch    15: 13.656\n",
      "Loss after mini batch    15: 12.339\n",
      "Loss after mini batch    15: 14.816\n",
      "Loss after mini batch    15: 13.733\n",
      "Loss after mini batch    15: 15.127\n",
      "Loss after mini batch    15: 13.799\n",
      "Starting epoch 16\n",
      "Loss after mini batch    16: 13.706\n",
      "Loss after mini batch    16: 17.224\n",
      "Loss after mini batch    16: 13.346\n",
      "Loss after mini batch    16: 13.058\n",
      "Loss after mini batch    16: 13.413\n",
      "Loss after mini batch    16: 16.202\n",
      "Loss after mini batch    16: 17.879\n",
      "Loss after mini batch    16: 13.035\n",
      "Loss after mini batch    16: 17.210\n",
      "Loss after mini batch    16: 12.470\n",
      "Loss after mini batch    16: 13.507\n",
      "Starting epoch 17\n",
      "Loss after mini batch    17: 13.499\n",
      "Loss after mini batch    17: 13.153\n",
      "Loss after mini batch    17: 13.468\n",
      "Loss after mini batch    17: 14.091\n",
      "Loss after mini batch    17: 12.792\n",
      "Loss after mini batch    17: 12.890\n",
      "Loss after mini batch    17: 13.643\n",
      "Loss after mini batch    17: 14.482\n",
      "Loss after mini batch    17: 12.822\n",
      "Loss after mini batch    17: 13.192\n",
      "Loss after mini batch    17: 13.534\n",
      "Starting epoch 18\n",
      "Loss after mini batch    18: 13.471\n",
      "Loss after mini batch    18: 12.746\n",
      "Loss after mini batch    18: 12.954\n",
      "Loss after mini batch    18: 14.755\n",
      "Loss after mini batch    18: 12.465\n",
      "Loss after mini batch    18: 15.275\n",
      "Loss after mini batch    18: 12.945\n",
      "Loss after mini batch    18: 14.216\n",
      "Loss after mini batch    18: 15.484\n",
      "Loss after mini batch    18: 14.492\n",
      "Loss after mini batch    18: 14.363\n",
      "Starting epoch 19\n",
      "Loss after mini batch    19: 13.334\n",
      "Loss after mini batch    19: 13.667\n",
      "Loss after mini batch    19: 13.455\n",
      "Loss after mini batch    19: 11.926\n",
      "Loss after mini batch    19: 16.439\n",
      "Loss after mini batch    19: 12.164\n",
      "Loss after mini batch    19: 15.619\n",
      "Loss after mini batch    19: 13.369\n",
      "Loss after mini batch    19: 14.573\n",
      "Loss after mini batch    19: 12.580\n",
      "Loss after mini batch    19: 13.835\n",
      "Starting epoch 20\n",
      "Loss after mini batch    20: 12.009\n",
      "Loss after mini batch    20: 13.821\n",
      "Loss after mini batch    20: 14.335\n",
      "Loss after mini batch    20: 13.808\n",
      "Loss after mini batch    20: 15.163\n",
      "Loss after mini batch    20: 15.013\n",
      "Loss after mini batch    20: 14.508\n",
      "Loss after mini batch    20: 12.806\n",
      "Loss after mini batch    20: 13.490\n",
      "Loss after mini batch    20: 12.975\n",
      "Loss after mini batch    20: 13.333\n",
      "Starting epoch 21\n",
      "Loss after mini batch    21: 14.505\n",
      "Loss after mini batch    21: 15.687\n",
      "Loss after mini batch    21: 12.743\n",
      "Loss after mini batch    21: 12.979\n",
      "Loss after mini batch    21: 13.533\n",
      "Loss after mini batch    21: 13.786\n",
      "Loss after mini batch    21: 13.022\n",
      "Loss after mini batch    21: 14.085\n",
      "Loss after mini batch    21: 13.872\n",
      "Loss after mini batch    21: 12.897\n",
      "Loss after mini batch    21: 13.939\n",
      "Starting epoch 22\n",
      "Loss after mini batch    22: 14.089\n",
      "Loss after mini batch    22: 13.651\n",
      "Loss after mini batch    22: 14.472\n",
      "Loss after mini batch    22: 12.616\n",
      "Loss after mini batch    22: 12.931\n",
      "Loss after mini batch    22: 12.535\n",
      "Loss after mini batch    22: 12.937\n",
      "Loss after mini batch    22: 16.973\n",
      "Loss after mini batch    22: 13.103\n",
      "Loss after mini batch    22: 14.908\n",
      "Loss after mini batch    22: 11.888\n",
      "Starting epoch 23\n",
      "Loss after mini batch    23: 12.838\n",
      "Loss after mini batch    23: 13.041\n",
      "Loss after mini batch    23: 13.240\n",
      "Loss after mini batch    23: 12.562\n",
      "Loss after mini batch    23: 13.064\n",
      "Loss after mini batch    23: 12.720\n",
      "Loss after mini batch    23: 12.864\n",
      "Loss after mini batch    23: 14.363\n",
      "Loss after mini batch    23: 14.713\n",
      "Loss after mini batch    23: 13.463\n",
      "Loss after mini batch    23: 14.595\n",
      "Starting epoch 24\n",
      "Loss after mini batch    24: 14.095\n",
      "Loss after mini batch    24: 12.489\n",
      "Loss after mini batch    24: 12.424\n",
      "Loss after mini batch    24: 13.695\n",
      "Loss after mini batch    24: 14.547\n",
      "Loss after mini batch    24: 13.245\n",
      "Loss after mini batch    24: 13.386\n",
      "Loss after mini batch    24: 14.497\n",
      "Loss after mini batch    24: 14.714\n",
      "Loss after mini batch    24: 13.693\n",
      "Loss after mini batch    24: 13.096\n",
      "Starting epoch 25\n",
      "Loss after mini batch    25: 14.270\n",
      "Loss after mini batch    25: 13.475\n",
      "Loss after mini batch    25: 11.939\n",
      "Loss after mini batch    25: 12.631\n",
      "Loss after mini batch    25: 12.639\n",
      "Loss after mini batch    25: 13.013\n",
      "Loss after mini batch    25: 12.990\n",
      "Loss after mini batch    25: 13.381\n",
      "Loss after mini batch    25: 13.351\n",
      "Loss after mini batch    25: 12.742\n",
      "Loss after mini batch    25: 13.952\n",
      "Starting epoch 26\n",
      "Loss after mini batch    26: 14.977\n",
      "Loss after mini batch    26: 14.203\n",
      "Loss after mini batch    26: 13.648\n",
      "Loss after mini batch    26: 13.832\n",
      "Loss after mini batch    26: 12.788\n",
      "Loss after mini batch    26: 13.571\n",
      "Loss after mini batch    26: 15.818\n",
      "Loss after mini batch    26: 13.710\n",
      "Loss after mini batch    26: 14.280\n",
      "Loss after mini batch    26: 13.391\n",
      "Loss after mini batch    26: 11.209\n",
      "Starting epoch 27\n",
      "Loss after mini batch    27: 12.509\n",
      "Loss after mini batch    27: 13.842\n",
      "Loss after mini batch    27: 12.835\n",
      "Loss after mini batch    27: 12.824\n",
      "Loss after mini batch    27: 12.699\n",
      "Loss after mini batch    27: 13.713\n",
      "Loss after mini batch    27: 13.049\n",
      "Loss after mini batch    27: 13.215\n",
      "Loss after mini batch    27: 14.503\n",
      "Loss after mini batch    27: 13.599\n",
      "Loss after mini batch    27: 14.743\n",
      "Starting epoch 28\n",
      "Loss after mini batch    28: 13.031\n",
      "Loss after mini batch    28: 14.641\n",
      "Loss after mini batch    28: 13.505\n",
      "Loss after mini batch    28: 12.787\n",
      "Loss after mini batch    28: 13.247\n",
      "Loss after mini batch    28: 13.485\n",
      "Loss after mini batch    28: 12.878\n",
      "Loss after mini batch    28: 11.558\n",
      "Loss after mini batch    28: 14.151\n",
      "Loss after mini batch    28: 12.578\n",
      "Loss after mini batch    28: 14.009\n",
      "Starting epoch 29\n",
      "Loss after mini batch    29: 13.401\n",
      "Loss after mini batch    29: 13.838\n",
      "Loss after mini batch    29: 12.121\n",
      "Loss after mini batch    29: 13.121\n",
      "Loss after mini batch    29: 14.017\n",
      "Loss after mini batch    29: 12.423\n",
      "Loss after mini batch    29: 15.155\n",
      "Loss after mini batch    29: 13.110\n",
      "Loss after mini batch    29: 12.800\n",
      "Loss after mini batch    29: 12.306\n",
      "Loss after mini batch    29: 13.273\n",
      "Starting epoch 30\n",
      "Loss after mini batch    30: 14.820\n",
      "Loss after mini batch    30: 12.187\n",
      "Loss after mini batch    30: 13.766\n",
      "Loss after mini batch    30: 13.543\n",
      "Loss after mini batch    30: 13.196\n",
      "Loss after mini batch    30: 13.368\n",
      "Loss after mini batch    30: 12.958\n",
      "Loss after mini batch    30: 12.069\n",
      "Loss after mini batch    30: 15.515\n",
      "Loss after mini batch    30: 13.952\n",
      "Loss after mini batch    30: 13.264\n",
      "Starting epoch 31\n",
      "Loss after mini batch    31: 13.171\n",
      "Loss after mini batch    31: 13.524\n",
      "Loss after mini batch    31: 13.319\n",
      "Loss after mini batch    31: 12.475\n",
      "Loss after mini batch    31: 14.069\n",
      "Loss after mini batch    31: 13.845\n",
      "Loss after mini batch    31: 13.491\n",
      "Loss after mini batch    31: 12.766\n",
      "Loss after mini batch    31: 11.574\n",
      "Loss after mini batch    31: 13.800\n",
      "Loss after mini batch    31: 14.553\n",
      "Starting epoch 32\n",
      "Loss after mini batch    32: 12.996\n",
      "Loss after mini batch    32: 13.338\n",
      "Loss after mini batch    32: 12.761\n",
      "Loss after mini batch    32: 13.789\n",
      "Loss after mini batch    32: 12.348\n",
      "Loss after mini batch    32: 11.720\n",
      "Loss after mini batch    32: 13.295\n",
      "Loss after mini batch    32: 12.136\n",
      "Loss after mini batch    32: 13.541\n",
      "Loss after mini batch    32: 13.277\n",
      "Loss after mini batch    32: 13.366\n",
      "Starting epoch 33\n",
      "Loss after mini batch    33: 13.753\n",
      "Loss after mini batch    33: 13.082\n",
      "Loss after mini batch    33: 13.067\n",
      "Loss after mini batch    33: 12.870\n",
      "Loss after mini batch    33: 13.648\n",
      "Loss after mini batch    33: 12.243\n",
      "Loss after mini batch    33: 12.839\n",
      "Loss after mini batch    33: 13.169\n",
      "Loss after mini batch    33: 11.790\n",
      "Loss after mini batch    33: 13.592\n",
      "Loss after mini batch    33: 12.905\n",
      "Starting epoch 34\n",
      "Loss after mini batch    34: 13.653\n",
      "Loss after mini batch    34: 13.512\n",
      "Loss after mini batch    34: 12.505\n",
      "Loss after mini batch    34: 13.103\n",
      "Loss after mini batch    34: 12.649\n",
      "Loss after mini batch    34: 13.118\n",
      "Loss after mini batch    34: 13.486\n",
      "Loss after mini batch    34: 12.975\n",
      "Loss after mini batch    34: 12.633\n",
      "Loss after mini batch    34: 12.982\n",
      "Loss after mini batch    34: 11.633\n",
      "Starting epoch 35\n",
      "Loss after mini batch    35: 13.058\n",
      "Loss after mini batch    35: 13.211\n",
      "Loss after mini batch    35: 13.718\n",
      "Loss after mini batch    35: 11.536\n",
      "Loss after mini batch    35: 12.804\n",
      "Loss after mini batch    35: 12.881\n",
      "Loss after mini batch    35: 12.650\n",
      "Loss after mini batch    35: 12.366\n",
      "Loss after mini batch    35: 12.038\n",
      "Loss after mini batch    35: 12.310\n",
      "Loss after mini batch    35: 13.365\n",
      "Starting epoch 36\n",
      "Loss after mini batch    36: 12.500\n",
      "Loss after mini batch    36: 13.255\n",
      "Loss after mini batch    36: 12.079\n",
      "Loss after mini batch    36: 12.558\n",
      "Loss after mini batch    36: 12.403\n",
      "Loss after mini batch    36: 12.130\n",
      "Loss after mini batch    36: 13.201\n",
      "Loss after mini batch    36: 13.351\n",
      "Loss after mini batch    36: 14.728\n",
      "Loss after mini batch    36: 13.063\n",
      "Loss after mini batch    36: 13.168\n",
      "Starting epoch 37\n",
      "Loss after mini batch    37: 12.724\n",
      "Loss after mini batch    37: 12.463\n",
      "Loss after mini batch    37: 12.265\n",
      "Loss after mini batch    37: 13.425\n",
      "Loss after mini batch    37: 13.336\n",
      "Loss after mini batch    37: 12.316\n",
      "Loss after mini batch    37: 12.532\n",
      "Loss after mini batch    37: 12.330\n",
      "Loss after mini batch    37: 12.823\n",
      "Loss after mini batch    37: 13.563\n",
      "Loss after mini batch    37: 13.248\n",
      "Starting epoch 38\n",
      "Loss after mini batch    38: 11.973\n",
      "Loss after mini batch    38: 13.160\n",
      "Loss after mini batch    38: 13.496\n",
      "Loss after mini batch    38: 12.670\n",
      "Loss after mini batch    38: 14.429\n",
      "Loss after mini batch    38: 12.445\n",
      "Loss after mini batch    38: 13.890\n",
      "Loss after mini batch    38: 12.689\n",
      "Loss after mini batch    38: 12.999\n",
      "Loss after mini batch    38: 12.637\n",
      "Loss after mini batch    38: 12.424\n",
      "Starting epoch 39\n",
      "Loss after mini batch    39: 12.735\n",
      "Loss after mini batch    39: 13.055\n",
      "Loss after mini batch    39: 13.164\n",
      "Loss after mini batch    39: 12.421\n",
      "Loss after mini batch    39: 12.439\n",
      "Loss after mini batch    39: 13.026\n",
      "Loss after mini batch    39: 12.782\n",
      "Loss after mini batch    39: 13.153\n",
      "Loss after mini batch    39: 13.149\n",
      "Loss after mini batch    39: 14.131\n",
      "Loss after mini batch    39: 15.096\n",
      "Starting epoch 40\n",
      "Loss after mini batch    40: 12.770\n",
      "Loss after mini batch    40: 11.039\n",
      "Loss after mini batch    40: 12.495\n",
      "Loss after mini batch    40: 12.813\n",
      "Loss after mini batch    40: 13.267\n",
      "Loss after mini batch    40: 13.688\n",
      "Loss after mini batch    40: 12.528\n",
      "Loss after mini batch    40: 12.765\n",
      "Loss after mini batch    40: 12.347\n",
      "Loss after mini batch    40: 13.603\n",
      "Loss after mini batch    40: 12.664\n",
      "Starting epoch 41\n",
      "Loss after mini batch    41: 12.110\n",
      "Loss after mini batch    41: 11.892\n",
      "Loss after mini batch    41: 14.032\n",
      "Loss after mini batch    41: 13.259\n",
      "Loss after mini batch    41: 12.793\n",
      "Loss after mini batch    41: 12.909\n",
      "Loss after mini batch    41: 13.238\n",
      "Loss after mini batch    41: 14.050\n",
      "Loss after mini batch    41: 13.355\n",
      "Loss after mini batch    41: 12.243\n",
      "Loss after mini batch    41: 12.206\n",
      "Starting epoch 42\n",
      "Loss after mini batch    42: 12.103\n",
      "Loss after mini batch    42: 11.607\n",
      "Loss after mini batch    42: 12.148\n",
      "Loss after mini batch    42: 12.784\n",
      "Loss after mini batch    42: 12.245\n",
      "Loss after mini batch    42: 13.191\n",
      "Loss after mini batch    42: 12.748\n",
      "Loss after mini batch    42: 12.303\n",
      "Loss after mini batch    42: 12.058\n",
      "Loss after mini batch    42: 13.609\n",
      "Loss after mini batch    42: 13.995\n",
      "Starting epoch 43\n",
      "Loss after mini batch    43: 13.575\n",
      "Loss after mini batch    43: 13.048\n",
      "Loss after mini batch    43: 12.881\n",
      "Loss after mini batch    43: 12.005\n",
      "Loss after mini batch    43: 13.046\n",
      "Loss after mini batch    43: 13.815\n",
      "Loss after mini batch    43: 12.966\n",
      "Loss after mini batch    43: 12.327\n",
      "Loss after mini batch    43: 12.895\n",
      "Loss after mini batch    43: 11.594\n",
      "Loss after mini batch    43: 11.786\n",
      "Starting epoch 44\n",
      "Loss after mini batch    44: 14.393\n",
      "Loss after mini batch    44: 12.469\n",
      "Loss after mini batch    44: 11.949\n",
      "Loss after mini batch    44: 12.981\n",
      "Loss after mini batch    44: 13.756\n",
      "Loss after mini batch    44: 12.334\n",
      "Loss after mini batch    44: 12.039\n",
      "Loss after mini batch    44: 12.623\n",
      "Loss after mini batch    44: 14.153\n",
      "Loss after mini batch    44: 13.319\n",
      "Loss after mini batch    44: 12.410\n",
      "Starting epoch 45\n",
      "Loss after mini batch    45: 12.588\n",
      "Loss after mini batch    45: 13.299\n",
      "Loss after mini batch    45: 12.608\n",
      "Loss after mini batch    45: 13.026\n",
      "Loss after mini batch    45: 13.353\n",
      "Loss after mini batch    45: 10.298\n",
      "Loss after mini batch    45: 12.523\n",
      "Loss after mini batch    45: 13.327\n",
      "Loss after mini batch    45: 12.302\n",
      "Loss after mini batch    45: 13.215\n",
      "Loss after mini batch    45: 11.791\n",
      "Starting epoch 46\n",
      "Loss after mini batch    46: 11.788\n",
      "Loss after mini batch    46: 14.092\n",
      "Loss after mini batch    46: 13.074\n",
      "Loss after mini batch    46: 11.963\n",
      "Loss after mini batch    46: 12.540\n",
      "Loss after mini batch    46: 12.934\n",
      "Loss after mini batch    46: 12.667\n",
      "Loss after mini batch    46: 13.497\n",
      "Loss after mini batch    46: 12.473\n",
      "Loss after mini batch    46: 12.407\n",
      "Loss after mini batch    46: 12.103\n",
      "Starting epoch 47\n",
      "Loss after mini batch    47: 11.089\n",
      "Loss after mini batch    47: 13.286\n",
      "Loss after mini batch    47: 12.232\n",
      "Loss after mini batch    47: 13.577\n",
      "Loss after mini batch    47: 12.955\n",
      "Loss after mini batch    47: 11.270\n",
      "Loss after mini batch    47: 13.059\n",
      "Loss after mini batch    47: 12.423\n",
      "Loss after mini batch    47: 13.617\n",
      "Loss after mini batch    47: 12.496\n",
      "Loss after mini batch    47: 12.857\n",
      "Starting epoch 48\n",
      "Loss after mini batch    48: 13.309\n",
      "Loss after mini batch    48: 11.817\n",
      "Loss after mini batch    48: 12.898\n",
      "Loss after mini batch    48: 14.134\n",
      "Loss after mini batch    48: 13.761\n",
      "Loss after mini batch    48: 12.054\n",
      "Loss after mini batch    48: 12.161\n",
      "Loss after mini batch    48: 13.367\n",
      "Loss after mini batch    48: 13.345\n",
      "Loss after mini batch    48: 11.926\n",
      "Loss after mini batch    48: 12.513\n",
      "Starting epoch 49\n",
      "Loss after mini batch    49: 12.302\n",
      "Loss after mini batch    49: 13.141\n",
      "Loss after mini batch    49: 12.304\n",
      "Loss after mini batch    49: 11.987\n",
      "Loss after mini batch    49: 12.784\n",
      "Loss after mini batch    49: 13.210\n",
      "Loss after mini batch    49: 13.545\n",
      "Loss after mini batch    49: 12.341\n",
      "Loss after mini batch    49: 11.457\n",
      "Loss after mini batch    49: 11.756\n",
      "Loss after mini batch    49: 13.516\n",
      "Starting epoch 50\n",
      "Loss after mini batch    50: 12.638\n",
      "Loss after mini batch    50: 13.009\n",
      "Loss after mini batch    50: 12.225\n",
      "Loss after mini batch    50: 11.936\n",
      "Loss after mini batch    50: 13.498\n",
      "Loss after mini batch    50: 12.772\n",
      "Loss after mini batch    50: 13.221\n",
      "Loss after mini batch    50: 12.570\n",
      "Loss after mini batch    50: 12.906\n",
      "Loss after mini batch    50: 12.972\n",
      "Loss after mini batch    50: 12.954\n",
      "Starting epoch 51\n",
      "Loss after mini batch    51: 11.507\n",
      "Loss after mini batch    51: 12.526\n",
      "Loss after mini batch    51: 13.124\n",
      "Loss after mini batch    51: 13.224\n",
      "Loss after mini batch    51: 13.396\n",
      "Loss after mini batch    51: 12.248\n",
      "Loss after mini batch    51: 12.215\n",
      "Loss after mini batch    51: 12.752\n",
      "Loss after mini batch    51: 13.683\n",
      "Loss after mini batch    51: 13.259\n",
      "Loss after mini batch    51: 13.534\n",
      "Starting epoch 52\n",
      "Loss after mini batch    52: 13.973\n",
      "Loss after mini batch    52: 11.561\n",
      "Loss after mini batch    52: 12.435\n",
      "Loss after mini batch    52: 12.467\n",
      "Loss after mini batch    52: 13.241\n",
      "Loss after mini batch    52: 13.312\n",
      "Loss after mini batch    52: 11.965\n",
      "Loss after mini batch    52: 11.371\n",
      "Loss after mini batch    52: 12.267\n",
      "Loss after mini batch    52: 12.587\n",
      "Loss after mini batch    52: 11.680\n",
      "Starting epoch 53\n",
      "Loss after mini batch    53: 13.260\n",
      "Loss after mini batch    53: 11.708\n",
      "Loss after mini batch    53: 12.484\n",
      "Loss after mini batch    53: 12.164\n",
      "Loss after mini batch    53: 12.138\n",
      "Loss after mini batch    53: 12.959\n",
      "Loss after mini batch    53: 11.697\n",
      "Loss after mini batch    53: 12.754\n",
      "Loss after mini batch    53: 11.937\n",
      "Loss after mini batch    53: 14.100\n",
      "Loss after mini batch    53: 12.171\n",
      "Starting epoch 54\n",
      "Loss after mini batch    54: 11.445\n",
      "Loss after mini batch    54: 11.787\n",
      "Loss after mini batch    54: 12.524\n",
      "Loss after mini batch    54: 11.278\n",
      "Loss after mini batch    54: 12.728\n",
      "Loss after mini batch    54: 12.575\n",
      "Loss after mini batch    54: 12.875\n",
      "Loss after mini batch    54: 13.425\n",
      "Loss after mini batch    54: 13.833\n",
      "Loss after mini batch    54: 12.743\n",
      "Loss after mini batch    54: 13.096\n",
      "Starting epoch 55\n",
      "Loss after mini batch    55: 12.639\n",
      "Loss after mini batch    55: 11.882\n",
      "Loss after mini batch    55: 14.036\n",
      "Loss after mini batch    55: 12.999\n",
      "Loss after mini batch    55: 12.386\n",
      "Loss after mini batch    55: 13.462\n",
      "Loss after mini batch    55: 12.850\n",
      "Loss after mini batch    55: 12.695\n",
      "Loss after mini batch    55: 14.089\n",
      "Loss after mini batch    55: 11.171\n",
      "Loss after mini batch    55: 11.529\n",
      "Starting epoch 56\n",
      "Loss after mini batch    56: 12.732\n",
      "Loss after mini batch    56: 12.724\n",
      "Loss after mini batch    56: 12.820\n",
      "Loss after mini batch    56: 12.836\n",
      "Loss after mini batch    56: 12.805\n",
      "Loss after mini batch    56: 10.896\n",
      "Loss after mini batch    56: 11.959\n",
      "Loss after mini batch    56: 12.936\n",
      "Loss after mini batch    56: 12.749\n",
      "Loss after mini batch    56: 13.820\n",
      "Loss after mini batch    56: 11.075\n",
      "Starting epoch 57\n",
      "Loss after mini batch    57: 13.251\n",
      "Loss after mini batch    57: 11.879\n",
      "Loss after mini batch    57: 14.283\n",
      "Loss after mini batch    57: 12.728\n",
      "Loss after mini batch    57: 11.997\n",
      "Loss after mini batch    57: 12.399\n",
      "Loss after mini batch    57: 12.804\n",
      "Loss after mini batch    57: 11.536\n",
      "Loss after mini batch    57: 12.097\n",
      "Loss after mini batch    57: 13.196\n",
      "Loss after mini batch    57: 12.854\n",
      "Starting epoch 58\n",
      "Loss after mini batch    58: 12.305\n",
      "Loss after mini batch    58: 12.317\n",
      "Loss after mini batch    58: 13.447\n",
      "Loss after mini batch    58: 12.015\n",
      "Loss after mini batch    58: 12.815\n",
      "Loss after mini batch    58: 13.897\n",
      "Loss after mini batch    58: 11.758\n",
      "Loss after mini batch    58: 11.096\n",
      "Loss after mini batch    58: 12.447\n",
      "Loss after mini batch    58: 11.952\n",
      "Loss after mini batch    58: 13.477\n",
      "Starting epoch 59\n",
      "Loss after mini batch    59: 11.826\n",
      "Loss after mini batch    59: 12.808\n",
      "Loss after mini batch    59: 12.276\n",
      "Loss after mini batch    59: 11.565\n",
      "Loss after mini batch    59: 12.118\n",
      "Loss after mini batch    59: 12.102\n",
      "Loss after mini batch    59: 13.609\n",
      "Loss after mini batch    59: 12.711\n",
      "Loss after mini batch    59: 12.091\n",
      "Loss after mini batch    59: 12.499\n",
      "Loss after mini batch    59: 12.629\n",
      "Starting epoch 60\n",
      "Loss after mini batch    60: 11.936\n",
      "Loss after mini batch    60: 11.959\n",
      "Loss after mini batch    60: 12.754\n",
      "Loss after mini batch    60: 13.167\n",
      "Loss after mini batch    60: 13.095\n",
      "Loss after mini batch    60: 13.080\n",
      "Loss after mini batch    60: 11.583\n",
      "Loss after mini batch    60: 12.570\n",
      "Loss after mini batch    60: 12.225\n",
      "Loss after mini batch    60: 12.539\n",
      "Loss after mini batch    60: 12.827\n",
      "Starting epoch 61\n",
      "Loss after mini batch    61: 13.158\n",
      "Loss after mini batch    61: 12.701\n",
      "Loss after mini batch    61: 12.418\n",
      "Loss after mini batch    61: 12.685\n",
      "Loss after mini batch    61: 12.183\n",
      "Loss after mini batch    61: 11.490\n",
      "Loss after mini batch    61: 12.613\n",
      "Loss after mini batch    61: 13.218\n",
      "Loss after mini batch    61: 12.501\n",
      "Loss after mini batch    61: 11.800\n",
      "Loss after mini batch    61: 11.515\n",
      "Starting epoch 62\n",
      "Loss after mini batch    62: 13.141\n",
      "Loss after mini batch    62: 11.609\n",
      "Loss after mini batch    62: 11.404\n",
      "Loss after mini batch    62: 11.771\n",
      "Loss after mini batch    62: 13.470\n",
      "Loss after mini batch    62: 12.787\n",
      "Loss after mini batch    62: 12.920\n",
      "Loss after mini batch    62: 13.028\n",
      "Loss after mini batch    62: 12.776\n",
      "Loss after mini batch    62: 12.073\n",
      "Loss after mini batch    62: 12.328\n",
      "Starting epoch 63\n",
      "Loss after mini batch    63: 13.276\n",
      "Loss after mini batch    63: 12.571\n",
      "Loss after mini batch    63: 12.696\n",
      "Loss after mini batch    63: 12.565\n",
      "Loss after mini batch    63: 11.367\n",
      "Loss after mini batch    63: 12.432\n",
      "Loss after mini batch    63: 12.131\n",
      "Loss after mini batch    63: 12.452\n",
      "Loss after mini batch    63: 12.440\n",
      "Loss after mini batch    63: 11.642\n",
      "Loss after mini batch    63: 11.462\n",
      "Starting epoch 64\n",
      "Loss after mini batch    64: 11.244\n",
      "Loss after mini batch    64: 11.207\n",
      "Loss after mini batch    64: 11.795\n",
      "Loss after mini batch    64: 12.961\n",
      "Loss after mini batch    64: 13.442\n",
      "Loss after mini batch    64: 13.972\n",
      "Loss after mini batch    64: 13.253\n",
      "Loss after mini batch    64: 12.121\n",
      "Loss after mini batch    64: 12.524\n",
      "Loss after mini batch    64: 11.725\n",
      "Loss after mini batch    64: 12.342\n",
      "Starting epoch 65\n",
      "Loss after mini batch    65: 12.945\n",
      "Loss after mini batch    65: 12.192\n",
      "Loss after mini batch    65: 13.561\n",
      "Loss after mini batch    65: 11.890\n",
      "Loss after mini batch    65: 12.567\n",
      "Loss after mini batch    65: 11.751\n",
      "Loss after mini batch    65: 13.372\n",
      "Loss after mini batch    65: 11.197\n",
      "Loss after mini batch    65: 11.193\n",
      "Loss after mini batch    65: 12.198\n",
      "Loss after mini batch    65: 12.583\n",
      "Starting epoch 66\n",
      "Loss after mini batch    66: 13.408\n",
      "Loss after mini batch    66: 13.373\n",
      "Loss after mini batch    66: 12.468\n",
      "Loss after mini batch    66: 13.465\n",
      "Loss after mini batch    66: 13.037\n",
      "Loss after mini batch    66: 12.087\n",
      "Loss after mini batch    66: 12.397\n",
      "Loss after mini batch    66: 12.918\n",
      "Loss after mini batch    66: 12.817\n",
      "Loss after mini batch    66: 11.560\n",
      "Loss after mini batch    66: 12.226\n",
      "Starting epoch 67\n",
      "Loss after mini batch    67: 15.125\n",
      "Loss after mini batch    67: 12.045\n",
      "Loss after mini batch    67: 13.171\n",
      "Loss after mini batch    67: 12.086\n",
      "Loss after mini batch    67: 10.749\n",
      "Loss after mini batch    67: 11.824\n",
      "Loss after mini batch    67: 12.495\n",
      "Loss after mini batch    67: 13.575\n",
      "Loss after mini batch    67: 12.655\n",
      "Loss after mini batch    67: 12.439\n",
      "Loss after mini batch    67: 12.043\n",
      "Starting epoch 68\n",
      "Loss after mini batch    68: 11.119\n",
      "Loss after mini batch    68: 11.405\n",
      "Loss after mini batch    68: 14.129\n",
      "Loss after mini batch    68: 12.117\n",
      "Loss after mini batch    68: 12.157\n",
      "Loss after mini batch    68: 11.709\n",
      "Loss after mini batch    68: 12.930\n",
      "Loss after mini batch    68: 12.170\n",
      "Loss after mini batch    68: 11.521\n",
      "Loss after mini batch    68: 13.133\n",
      "Loss after mini batch    68: 12.259\n",
      "Starting epoch 69\n",
      "Loss after mini batch    69: 11.514\n",
      "Loss after mini batch    69: 12.272\n",
      "Loss after mini batch    69: 12.024\n",
      "Loss after mini batch    69: 12.177\n",
      "Loss after mini batch    69: 13.457\n",
      "Loss after mini batch    69: 12.175\n",
      "Loss after mini batch    69: 12.775\n",
      "Loss after mini batch    69: 12.241\n",
      "Loss after mini batch    69: 12.851\n",
      "Loss after mini batch    69: 12.827\n",
      "Loss after mini batch    69: 11.532\n",
      "Starting epoch 70\n",
      "Loss after mini batch    70: 11.701\n",
      "Loss after mini batch    70: 12.364\n",
      "Loss after mini batch    70: 11.715\n",
      "Loss after mini batch    70: 12.748\n",
      "Loss after mini batch    70: 13.263\n",
      "Loss after mini batch    70: 11.777\n",
      "Loss after mini batch    70: 12.367\n",
      "Loss after mini batch    70: 12.164\n",
      "Loss after mini batch    70: 12.372\n",
      "Loss after mini batch    70: 11.194\n",
      "Loss after mini batch    70: 12.868\n",
      "Starting epoch 71\n",
      "Loss after mini batch    71: 13.534\n",
      "Loss after mini batch    71: 12.549\n",
      "Loss after mini batch    71: 12.009\n",
      "Loss after mini batch    71: 12.387\n",
      "Loss after mini batch    71: 12.588\n",
      "Loss after mini batch    71: 12.640\n",
      "Loss after mini batch    71: 12.329\n",
      "Loss after mini batch    71: 11.474\n",
      "Loss after mini batch    71: 11.387\n",
      "Loss after mini batch    71: 12.663\n",
      "Loss after mini batch    71: 11.719\n",
      "Starting epoch 72\n",
      "Loss after mini batch    72: 12.700\n",
      "Loss after mini batch    72: 11.427\n",
      "Loss after mini batch    72: 12.465\n",
      "Loss after mini batch    72: 12.563\n",
      "Loss after mini batch    72: 12.052\n",
      "Loss after mini batch    72: 12.351\n",
      "Loss after mini batch    72: 12.152\n",
      "Loss after mini batch    72: 12.860\n",
      "Loss after mini batch    72: 12.311\n",
      "Loss after mini batch    72: 13.134\n",
      "Loss after mini batch    72: 12.310\n",
      "Starting epoch 73\n",
      "Loss after mini batch    73: 11.718\n",
      "Loss after mini batch    73: 13.119\n",
      "Loss after mini batch    73: 11.909\n",
      "Loss after mini batch    73: 12.313\n",
      "Loss after mini batch    73: 13.574\n",
      "Loss after mini batch    73: 12.239\n",
      "Loss after mini batch    73: 12.222\n",
      "Loss after mini batch    73: 12.550\n",
      "Loss after mini batch    73: 12.755\n",
      "Loss after mini batch    73: 11.695\n",
      "Loss after mini batch    73: 12.122\n",
      "Starting epoch 74\n",
      "Loss after mini batch    74: 12.626\n",
      "Loss after mini batch    74: 11.625\n",
      "Loss after mini batch    74: 11.842\n",
      "Loss after mini batch    74: 14.034\n",
      "Loss after mini batch    74: 12.939\n",
      "Loss after mini batch    74: 11.331\n",
      "Loss after mini batch    74: 11.482\n",
      "Loss after mini batch    74: 13.464\n",
      "Loss after mini batch    74: 11.795\n",
      "Loss after mini batch    74: 12.448\n",
      "Loss after mini batch    74: 11.225\n",
      "Starting epoch 75\n",
      "Loss after mini batch    75: 11.969\n",
      "Loss after mini batch    75: 13.928\n",
      "Loss after mini batch    75: 11.027\n",
      "Loss after mini batch    75: 11.562\n",
      "Loss after mini batch    75: 12.375\n",
      "Loss after mini batch    75: 13.362\n",
      "Loss after mini batch    75: 12.909\n",
      "Loss after mini batch    75: 12.863\n",
      "Loss after mini batch    75: 12.837\n",
      "Loss after mini batch    75: 12.917\n",
      "Loss after mini batch    75: 11.567\n",
      "Starting epoch 76\n",
      "Loss after mini batch    76: 12.022\n",
      "Loss after mini batch    76: 11.255\n",
      "Loss after mini batch    76: 11.821\n",
      "Loss after mini batch    76: 10.873\n",
      "Loss after mini batch    76: 13.082\n",
      "Loss after mini batch    76: 12.960\n",
      "Loss after mini batch    76: 13.735\n",
      "Loss after mini batch    76: 12.629\n",
      "Loss after mini batch    76: 12.123\n",
      "Loss after mini batch    76: 12.651\n",
      "Loss after mini batch    76: 12.910\n",
      "Starting epoch 77\n",
      "Loss after mini batch    77: 11.595\n",
      "Loss after mini batch    77: 12.415\n",
      "Loss after mini batch    77: 12.758\n",
      "Loss after mini batch    77: 12.842\n",
      "Loss after mini batch    77: 13.080\n",
      "Loss after mini batch    77: 12.051\n",
      "Loss after mini batch    77: 12.155\n",
      "Loss after mini batch    77: 12.851\n",
      "Loss after mini batch    77: 12.481\n",
      "Loss after mini batch    77: 11.548\n",
      "Loss after mini batch    77: 11.310\n",
      "Starting epoch 78\n",
      "Loss after mini batch    78: 13.413\n",
      "Loss after mini batch    78: 12.455\n",
      "Loss after mini batch    78: 11.391\n",
      "Loss after mini batch    78: 12.567\n",
      "Loss after mini batch    78: 12.714\n",
      "Loss after mini batch    78: 11.618\n",
      "Loss after mini batch    78: 11.069\n",
      "Loss after mini batch    78: 13.017\n",
      "Loss after mini batch    78: 13.969\n",
      "Loss after mini batch    78: 11.595\n",
      "Loss after mini batch    78: 12.231\n",
      "Starting epoch 79\n",
      "Loss after mini batch    79: 11.913\n",
      "Loss after mini batch    79: 12.010\n",
      "Loss after mini batch    79: 12.008\n",
      "Loss after mini batch    79: 13.386\n",
      "Loss after mini batch    79: 12.796\n",
      "Loss after mini batch    79: 11.725\n",
      "Loss after mini batch    79: 11.851\n",
      "Loss after mini batch    79: 11.903\n",
      "Loss after mini batch    79: 14.051\n",
      "Loss after mini batch    79: 11.423\n",
      "Loss after mini batch    79: 11.057\n",
      "Starting epoch 80\n",
      "Loss after mini batch    80: 11.310\n",
      "Loss after mini batch    80: 12.590\n",
      "Loss after mini batch    80: 11.569\n",
      "Loss after mini batch    80: 12.686\n",
      "Loss after mini batch    80: 13.033\n",
      "Loss after mini batch    80: 11.617\n",
      "Loss after mini batch    80: 11.774\n",
      "Loss after mini batch    80: 12.135\n",
      "Loss after mini batch    80: 12.818\n",
      "Loss after mini batch    80: 12.459\n",
      "Loss after mini batch    80: 12.150\n",
      "Starting epoch 81\n",
      "Loss after mini batch    81: 11.870\n",
      "Loss after mini batch    81: 12.496\n",
      "Loss after mini batch    81: 13.517\n",
      "Loss after mini batch    81: 12.795\n",
      "Loss after mini batch    81: 10.660\n",
      "Loss after mini batch    81: 11.661\n",
      "Loss after mini batch    81: 12.677\n",
      "Loss after mini batch    81: 13.729\n",
      "Loss after mini batch    81: 13.524\n",
      "Loss after mini batch    81: 11.243\n",
      "Loss after mini batch    81: 12.723\n",
      "Starting epoch 82\n",
      "Loss after mini batch    82: 11.987\n",
      "Loss after mini batch    82: 12.241\n",
      "Loss after mini batch    82: 11.104\n",
      "Loss after mini batch    82: 12.706\n",
      "Loss after mini batch    82: 11.892\n",
      "Loss after mini batch    82: 13.717\n",
      "Loss after mini batch    82: 12.909\n",
      "Loss after mini batch    82: 12.685\n",
      "Loss after mini batch    82: 11.698\n",
      "Loss after mini batch    82: 11.591\n",
      "Loss after mini batch    82: 12.403\n",
      "Starting epoch 83\n",
      "Loss after mini batch    83: 11.681\n",
      "Loss after mini batch    83: 12.407\n",
      "Loss after mini batch    83: 12.736\n",
      "Loss after mini batch    83: 12.772\n",
      "Loss after mini batch    83: 12.217\n",
      "Loss after mini batch    83: 12.146\n",
      "Loss after mini batch    83: 12.338\n",
      "Loss after mini batch    83: 13.507\n",
      "Loss after mini batch    83: 12.862\n",
      "Loss after mini batch    83: 11.039\n",
      "Loss after mini batch    83: 13.438\n",
      "Starting epoch 84\n",
      "Loss after mini batch    84: 13.861\n",
      "Loss after mini batch    84: 12.112\n",
      "Loss after mini batch    84: 12.739\n",
      "Loss after mini batch    84: 14.315\n",
      "Loss after mini batch    84: 12.198\n",
      "Loss after mini batch    84: 11.018\n",
      "Loss after mini batch    84: 12.102\n",
      "Loss after mini batch    84: 11.930\n",
      "Loss after mini batch    84: 11.785\n",
      "Loss after mini batch    84: 12.403\n",
      "Loss after mini batch    84: 11.232\n",
      "Starting epoch 85\n",
      "Loss after mini batch    85: 12.873\n",
      "Loss after mini batch    85: 12.716\n",
      "Loss after mini batch    85: 12.822\n",
      "Loss after mini batch    85: 11.298\n",
      "Loss after mini batch    85: 12.464\n",
      "Loss after mini batch    85: 12.095\n",
      "Loss after mini batch    85: 12.592\n",
      "Loss after mini batch    85: 11.561\n",
      "Loss after mini batch    85: 12.160\n",
      "Loss after mini batch    85: 12.420\n",
      "Loss after mini batch    85: 12.058\n",
      "Starting epoch 86\n",
      "Loss after mini batch    86: 12.297\n",
      "Loss after mini batch    86: 12.043\n",
      "Loss after mini batch    86: 13.631\n",
      "Loss after mini batch    86: 13.578\n",
      "Loss after mini batch    86: 12.080\n",
      "Loss after mini batch    86: 12.641\n",
      "Loss after mini batch    86: 12.420\n",
      "Loss after mini batch    86: 11.349\n",
      "Loss after mini batch    86: 11.122\n",
      "Loss after mini batch    86: 12.481\n",
      "Loss after mini batch    86: 11.687\n",
      "Starting epoch 87\n",
      "Loss after mini batch    87: 12.461\n",
      "Loss after mini batch    87: 11.779\n",
      "Loss after mini batch    87: 12.386\n",
      "Loss after mini batch    87: 11.699\n",
      "Loss after mini batch    87: 12.462\n",
      "Loss after mini batch    87: 12.346\n",
      "Loss after mini batch    87: 12.920\n",
      "Loss after mini batch    87: 12.765\n",
      "Loss after mini batch    87: 11.090\n",
      "Loss after mini batch    87: 12.554\n",
      "Loss after mini batch    87: 12.627\n",
      "Starting epoch 88\n",
      "Loss after mini batch    88: 12.089\n",
      "Loss after mini batch    88: 12.903\n",
      "Loss after mini batch    88: 12.521\n",
      "Loss after mini batch    88: 12.558\n",
      "Loss after mini batch    88: 12.194\n",
      "Loss after mini batch    88: 12.238\n",
      "Loss after mini batch    88: 12.901\n",
      "Loss after mini batch    88: 12.946\n",
      "Loss after mini batch    88: 11.954\n",
      "Loss after mini batch    88: 12.240\n",
      "Loss after mini batch    88: 11.212\n",
      "Starting epoch 89\n",
      "Loss after mini batch    89: 12.673\n",
      "Loss after mini batch    89: 11.426\n",
      "Loss after mini batch    89: 13.458\n",
      "Loss after mini batch    89: 11.997\n",
      "Loss after mini batch    89: 13.507\n",
      "Loss after mini batch    89: 12.023\n",
      "Loss after mini batch    89: 11.467\n",
      "Loss after mini batch    89: 12.015\n",
      "Loss after mini batch    89: 12.593\n",
      "Loss after mini batch    89: 12.132\n",
      "Loss after mini batch    89: 13.050\n",
      "Starting epoch 90\n",
      "Loss after mini batch    90: 12.967\n",
      "Loss after mini batch    90: 13.796\n",
      "Loss after mini batch    90: 12.008\n",
      "Loss after mini batch    90: 13.423\n",
      "Loss after mini batch    90: 12.673\n",
      "Loss after mini batch    90: 13.561\n",
      "Loss after mini batch    90: 11.776\n",
      "Loss after mini batch    90: 11.768\n",
      "Loss after mini batch    90: 11.258\n",
      "Loss after mini batch    90: 11.070\n",
      "Loss after mini batch    90: 12.148\n",
      "Starting epoch 91\n",
      "Loss after mini batch    91: 12.985\n",
      "Loss after mini batch    91: 11.600\n",
      "Loss after mini batch    91: 12.998\n",
      "Loss after mini batch    91: 13.156\n",
      "Loss after mini batch    91: 11.055\n",
      "Loss after mini batch    91: 13.282\n",
      "Loss after mini batch    91: 13.216\n",
      "Loss after mini batch    91: 12.315\n",
      "Loss after mini batch    91: 10.542\n",
      "Loss after mini batch    91: 11.791\n",
      "Loss after mini batch    91: 12.320\n",
      "Starting epoch 92\n",
      "Loss after mini batch    92: 11.432\n",
      "Loss after mini batch    92: 12.503\n",
      "Loss after mini batch    92: 12.353\n",
      "Loss after mini batch    92: 12.237\n",
      "Loss after mini batch    92: 12.142\n",
      "Loss after mini batch    92: 11.936\n",
      "Loss after mini batch    92: 12.215\n",
      "Loss after mini batch    92: 12.353\n",
      "Loss after mini batch    92: 11.513\n",
      "Loss after mini batch    92: 12.825\n",
      "Loss after mini batch    92: 12.261\n",
      "Starting epoch 93\n",
      "Loss after mini batch    93: 12.273\n",
      "Loss after mini batch    93: 12.806\n",
      "Loss after mini batch    93: 12.401\n",
      "Loss after mini batch    93: 12.820\n",
      "Loss after mini batch    93: 11.638\n",
      "Loss after mini batch    93: 12.239\n",
      "Loss after mini batch    93: 12.012\n",
      "Loss after mini batch    93: 12.319\n",
      "Loss after mini batch    93: 12.474\n",
      "Loss after mini batch    93: 11.403\n",
      "Loss after mini batch    93: 10.786\n",
      "Starting epoch 94\n",
      "Loss after mini batch    94: 11.179\n",
      "Loss after mini batch    94: 10.598\n",
      "Loss after mini batch    94: 11.514\n",
      "Loss after mini batch    94: 12.710\n",
      "Loss after mini batch    94: 13.222\n",
      "Loss after mini batch    94: 11.026\n",
      "Loss after mini batch    94: 12.335\n",
      "Loss after mini batch    94: 12.239\n",
      "Loss after mini batch    94: 12.367\n",
      "Loss after mini batch    94: 13.160\n",
      "Loss after mini batch    94: 12.608\n",
      "Starting epoch 95\n",
      "Loss after mini batch    95: 11.778\n",
      "Loss after mini batch    95: 12.506\n",
      "Loss after mini batch    95: 13.266\n",
      "Loss after mini batch    95: 11.566\n",
      "Loss after mini batch    95: 11.979\n",
      "Loss after mini batch    95: 13.151\n",
      "Loss after mini batch    95: 13.083\n",
      "Loss after mini batch    95: 11.543\n",
      "Loss after mini batch    95: 11.064\n",
      "Loss after mini batch    95: 12.464\n",
      "Loss after mini batch    95: 12.234\n",
      "Starting epoch 96\n",
      "Loss after mini batch    96: 11.655\n",
      "Loss after mini batch    96: 11.704\n",
      "Loss after mini batch    96: 12.716\n",
      "Loss after mini batch    96: 13.330\n",
      "Loss after mini batch    96: 12.073\n",
      "Loss after mini batch    96: 12.146\n",
      "Loss after mini batch    96: 10.783\n",
      "Loss after mini batch    96: 12.708\n",
      "Loss after mini batch    96: 12.906\n",
      "Loss after mini batch    96: 11.916\n",
      "Loss after mini batch    96: 13.122\n",
      "Starting epoch 97\n",
      "Loss after mini batch    97: 11.397\n",
      "Loss after mini batch    97: 11.748\n",
      "Loss after mini batch    97: 11.914\n",
      "Loss after mini batch    97: 12.698\n",
      "Loss after mini batch    97: 13.234\n",
      "Loss after mini batch    97: 13.079\n",
      "Loss after mini batch    97: 12.516\n",
      "Loss after mini batch    97: 12.077\n",
      "Loss after mini batch    97: 11.979\n",
      "Loss after mini batch    97: 12.107\n",
      "Loss after mini batch    97: 11.253\n",
      "Starting epoch 98\n",
      "Loss after mini batch    98: 11.613\n",
      "Loss after mini batch    98: 12.448\n",
      "Loss after mini batch    98: 12.302\n",
      "Loss after mini batch    98: 11.215\n",
      "Loss after mini batch    98: 11.794\n",
      "Loss after mini batch    98: 12.211\n",
      "Loss after mini batch    98: 12.310\n",
      "Loss after mini batch    98: 12.195\n",
      "Loss after mini batch    98: 12.038\n",
      "Loss after mini batch    98: 11.924\n",
      "Loss after mini batch    98: 11.933\n",
      "Starting epoch 99\n",
      "Loss after mini batch    99: 13.729\n",
      "Loss after mini batch    99: 11.424\n",
      "Loss after mini batch    99: 11.578\n",
      "Loss after mini batch    99: 11.555\n",
      "Loss after mini batch    99: 12.164\n",
      "Loss after mini batch    99: 12.048\n",
      "Loss after mini batch    99: 12.629\n",
      "Loss after mini batch    99: 13.198\n",
      "Loss after mini batch    99: 13.666\n",
      "Loss after mini batch    99: 12.016\n",
      "Loss after mini batch    99: 11.663\n",
      "Starting epoch 100\n",
      "Loss after mini batch   100: 11.604\n",
      "Loss after mini batch   100: 11.115\n",
      "Loss after mini batch   100: 10.992\n",
      "Loss after mini batch   100: 13.654\n",
      "Loss after mini batch   100: 12.108\n",
      "Loss after mini batch   100: 11.863\n",
      "Loss after mini batch   100: 13.419\n",
      "Loss after mini batch   100: 13.133\n",
      "Loss after mini batch   100: 12.992\n",
      "Loss after mini batch   100: 11.182\n",
      "Loss after mini batch   100: 11.511\n",
      "Training Complete\n",
      "Starting Testing\n",
      "RMSE for fold 10: 3.4623338625054143\n",
      "rRMSE for fold 10: 0.06979926263829242\n",
      "r for fold 10: 0.9904821718745922\n",
      "Fast RMSE for fold 10: 2.9113989717622557\n",
      "Fast rRMSE for fold 10: 0.05774106222346305\n",
      "Fast r for fold 10: 0.9871581699986847\n",
      "Slow RMSE for fold 10: 4.494834902641248\n",
      "Slow rRMSE for fold 10: 0.09232549299694953\n",
      "Slow r for fold 10: 0.9979450442523751\n",
      "Regular RMSE for fold 10: 3.009527812179631\n",
      "Regular rRMSE for fold 10: 0.06078444940503963\n",
      "Regular r for fold 10: 0.996197246972671\n",
      "K-Fold Cross Validation Results for 10 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 3.5177407912832637\n",
      "Fold 1: 3.499257962721933\n",
      "Fold 2: 3.531483060184505\n",
      "Fold 3: 3.3627152271238296\n",
      "Fold 4: 3.3468918344909544\n",
      "Fold 5: 3.5172663269293074\n",
      "Fold 6: 3.462193383918484\n",
      "Fold 7: 3.4751733640608955\n",
      "Fold 8: 3.7556572521016443\n",
      "Fold 9: 3.4623338625054143\n",
      "Average: 3.4930713065320234 %\n",
      "Standard Deviation: 0.10594282289701228\n"
     ]
    }
   ],
   "source": [
    "print('--------------------------------')\n",
    "\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(splits.split(dataset)):\n",
    "\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size = batch_size, sampler = train_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size = batch_size, sampler = test_sampler)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    network = Network()\n",
    "    network.to(device)\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr = .001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Starting epoch {epoch + 1}')\n",
    "        current_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, targets = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = network(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            current_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('Loss after mini batch %5d: %.3f' % (epoch + 1, current_loss / 100))\n",
    "                current_loss = 0.0\n",
    "    \n",
    "    print('Training Complete')\n",
    "    print('Starting Testing')\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        predictions, actuals = list(), list()\n",
    "        meanObserved = 0\n",
    "        numObserved = 0\n",
    "\n",
    "        fast = dict()\n",
    "        regular = dict()\n",
    "        slow = dict()\n",
    "\n",
    "        fast['predictions'] = list()\n",
    "        fast['actuals'] = list()\n",
    "        fast['meanObserved'] = 0\n",
    "        fast['numObserved'] = 0\n",
    "\n",
    "        regular['predictions'] = list()\n",
    "        regular['actuals'] = list()\n",
    "        regular['meanObserved'] = 0\n",
    "        regular['numObserved'] = 0\n",
    "\n",
    "        slow['predictions'] = list()\n",
    "        slow['actuals'] = list()\n",
    "        slow['meanObserved'] = 0\n",
    "        slow['numObserved'] = 0\n",
    "\n",
    "        \n",
    "        for index, data in enumerate(test_loader, 0):\n",
    "            inputs, targets = data\n",
    "            output = network(inputs)\n",
    "        \n",
    "            output = output.detach().numpy()\n",
    "            actual = targets.numpy()\n",
    "\n",
    "            predictions.append(output)\n",
    "            actuals.append(actual)\n",
    "\n",
    "            for i in range(len(actual)):\n",
    "\n",
    "                if tuple(np.array(actual[i])) in regularVector:\n",
    "                    regular['predictions'].append(output[i])\n",
    "                    regular['actuals'].append(actual[i])\n",
    "                    for scalar in actual[i]:\n",
    "                        meanObserved += scalar\n",
    "                        numObserved += 1\n",
    "                        regular['meanObserved'] += scalar\n",
    "                        regular['numObserved'] += 1\n",
    "                elif tuple(np.array(actual[i])) in slowVector:\n",
    "                    slow['predictions'].append(output[i])\n",
    "                    slow['actuals'].append(actual[i])\n",
    "                    for scalar in actual[i]:\n",
    "                        meanObserved += scalar\n",
    "                        numObserved += 1\n",
    "                        slow['meanObserved'] += scalar\n",
    "                        slow['numObserved'] += 1\n",
    "                else:\n",
    "                    fast['predictions'].append(output[i])\n",
    "                    fast['actuals'].append(actual[i])\n",
    "                    for scalar in actual[i]:\n",
    "                        meanObserved += scalar\n",
    "                        numObserved += 1\n",
    "                        fast['meanObserved'] += scalar\n",
    "                        fast['numObserved'] += 1\n",
    "                \n",
    "    meanObserved /= numObserved\n",
    "    fast['meanObserved'] /= fast['numObserved']\n",
    "    slow['meanObserved'] /= slow['numObserved']\n",
    "    regular['meanObserved'] /= regular['numObserved']\n",
    "\n",
    "\n",
    "    \n",
    "    predictions, actuals = np.array(predictions, dtype = object), np.array(actuals, dtype = object)\n",
    "    fast['predictions'], fast['actuals'] = np.array(fast['predictions']), np.array(fast['actuals'])\n",
    "    slow['predictions'], slow['actuals'] = np.array(slow['predictions']), np.array(slow['actuals'])\n",
    "    regular['predictions'], regular['actuals'] = np.array(regular['predictions']), np.array(regular['actuals'])\n",
    "\n",
    "\n",
    "    predictions, actuals = np.vstack(predictions), np.vstack(actuals)\n",
    "    fast['predictions'], fast['actuals'] = np.vstack(fast['predictions']), np.vstack(fast['actuals'])\n",
    "    slow['predictions'], slow['actuals'] = np.vstack(slow['predictions']), np.vstack(slow['actuals'])\n",
    "    regular['predictions'], regular['actuals'] = np.vstack(regular['predictions']), np.vstack(regular['actuals'])\n",
    "\n",
    "    mseRealAngle = mean_squared_error(actuals, predictions)\n",
    "    fastMSERealAngle = mean_squared_error(fast['actuals'], fast['predictions'])\n",
    "    regularMSERealAngle = mean_squared_error(regular['actuals'], regular['predictions'])\n",
    "    slowMSERealAngle = mean_squared_error(slow['actuals'], slow['predictions'])\n",
    "\n",
    "\n",
    "\n",
    "    predictions, actuals = np.ndarray.flatten(predictions), np.ndarray.flatten(actuals)\n",
    "    fast['predictions'], fast['actuals'] = np.ndarray.flatten(fast['predictions']), np.ndarray.flatten(fast['actuals'])\n",
    "    slow['predictions'], slow['actuals'] = np.ndarray.flatten(slow['predictions']), np.ndarray.flatten(slow['actuals'])\n",
    "    regular['predictions'], regular['actuals'] = np.ndarray.flatten(regular['predictions']), np.ndarray.flatten(regular['actuals'])\n",
    "\n",
    "\n",
    "\n",
    "    r = np.corrcoef(predictions, actuals)[0][1]\n",
    "    rFast = np.corrcoef(fast['predictions'], fast['actuals'])[0][1]\n",
    "    rRegular = np.corrcoef(regular['predictions'], regular['actuals'])[0][1]\n",
    "    rSlow = np.corrcoef(slow['predictions'], slow['actuals'])[0][1]\n",
    "\n",
    "    print(f'RMSE for fold {fold + 1}: {math.sqrt(mseRealAngle)}')\n",
    "    print(f'rRMSE for fold {fold + 1}: {math.sqrt(mseRealAngle) / meanObserved}')\n",
    "    print(f'r for fold {fold + 1}: {r}')\n",
    "\n",
    "    fastMeanObserved = fast['meanObserved']\n",
    "    print(f'Fast RMSE for fold {fold + 1}: {math.sqrt(fastMSERealAngle)}')\n",
    "    print(f'Fast rRMSE for fold {fold + 1}: {math.sqrt(fastMSERealAngle) / fastMeanObserved}')\n",
    "    print(f'Fast r for fold {fold + 1}: {rFast}')\n",
    "\n",
    "    slowMeanObserved = slow['meanObserved']\n",
    "    print(f'Slow RMSE for fold {fold + 1}: {math.sqrt(slowMSERealAngle)}')\n",
    "    print(f'Slow rRMSE for fold {fold + 1}: {math.sqrt(slowMSERealAngle) / slowMeanObserved}')\n",
    "    print(f'Slow r for fold {fold + 1}: {rSlow}')\n",
    "\n",
    "    regularMeanObserved = regular['meanObserved']\n",
    "    print(f'Regular RMSE for fold {fold + 1}: {math.sqrt(regularMSERealAngle)}')\n",
    "    print(f'Regular rRMSE for fold {fold + 1}: {math.sqrt(regularMSERealAngle) / regularMeanObserved}')\n",
    "    print(f'Regular r for fold {fold + 1}: {rRegular}')\n",
    "    \n",
    "    results[fold] = (math.sqrt(mseRealAngle), math.sqrt(mseRealAngle) / meanObserved, r)\n",
    "    regularResults[fold] = (math.sqrt(regularMSERealAngle), math.sqrt(regularMSERealAngle) / regularMeanObserved, rRegular)\n",
    "    slowResults[fold] = (math.sqrt(slowMSERealAngle), math.sqrt(slowMSERealAngle) / slowMeanObserved, rSlow)\n",
    "    fastResults[fold] = (math.sqrt(fastMSERealAngle), math.sqrt(fastMSERealAngle) / fastMeanObserved, rFast)\n",
    "\n",
    "print('K-Fold Cross Validation Results for 10 FOLDS')\n",
    "print('--------------------------------')\n",
    "sum = 0.0\n",
    "scoreArr = []\n",
    "for key, value in results.items():\n",
    "    print(f'Fold {key}: {value[0]}')\n",
    "    sum += value[0]\n",
    "    scoreArr.append(value[0])\n",
    "print(f'Average: {sum / len(scoreArr)}')\n",
    "print(f'Standard Deviation: {np.std(scoreArr)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE:  3.4930713065320234 +- 0.10594282289701228\n",
      "Mean rRMSE:  0.0681072477482915 +- 0.001952770870385403\n",
      "Mean r:  0.9903408284001302 +- 0.00038371178555326427\n"
     ]
    }
   ],
   "source": [
    "results\n",
    "\n",
    "rmseAll = [element[1][0] for element in results.items()]\n",
    "rrmseAll = [element[1][1] for element in results.items()]\n",
    "rAll = [element[1][2] for element in results.items()]\n",
    "\n",
    "print('Mean RMSE: ', np.mean(rmseAll) , '+-', np.std(rmseAll))\n",
    "print('Mean rRMSE: ', np.mean(rrmseAll) , '+-', np.std(rrmseAll))\n",
    "print('Mean r: ', np.mean(rAll) , '+-', np.std(rAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE:  2.611506437540097 +- 0.40575624176574054\n",
      "Mean rRMSE:  0.051843385221528225 +- 0.00834382539283731\n",
      "Mean r:  0.996798470050431 +- 0.0004774547209944931\n"
     ]
    }
   ],
   "source": [
    "regularResults\n",
    "\n",
    "rmseRegular = [element[1][0] for element in regularResults.items()]\n",
    "rrmseRegular = [element[1][1] for element in regularResults.items()]\n",
    "rRegular = [element[1][2] for element in regularResults.items()]\n",
    "\n",
    "print('Mean RMSE: ', np.mean(rmseRegular) , '+-', np.std(rmseRegular))\n",
    "print('Mean rRMSE: ', np.mean(rrmseRegular) , '+-', np.std(rrmseRegular))\n",
    "print('Mean r: ', np.mean(rRegular) , '+-', np.std(rRegular))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE:  4.5632028199285175 +- 0.4103402687220703\n",
      "Mean rRMSE:  0.08704333606729006 +- 0.007835388415749437\n",
      "Mean r:  0.9979441342200109 +- 0.0002469458517429239\n"
     ]
    }
   ],
   "source": [
    "slowResults\n",
    "\n",
    "rmseSlow = [element[1][0] for element in slowResults.items()]\n",
    "rrmseSlow = [element[1][1] for element in slowResults.items()]\n",
    "rSlow = [element[1][2] for element in slowResults.items()]\n",
    "\n",
    "print('Mean RMSE: ', np.mean(rmseSlow) , '+-', np.std(rmseSlow))\n",
    "print('Mean rRMSE: ', np.mean(rrmseSlow) , '+-', np.std(rrmseSlow))\n",
    "print('Mean r: ', np.mean(rSlow) , '+-', np.std(rSlow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE:  3.1597620589283704 +- 0.1824326717387349\n",
      "Mean rRMSE:  0.06188097884413161 +- 0.003747189730778715\n",
      "Mean r:  0.9836832856471212 +- 0.0026315530871670514\n"
     ]
    }
   ],
   "source": [
    "fastResults\n",
    "\n",
    "rmseFast = [element[1][0] for element in fastResults.items()]\n",
    "rrmseFast = [element[1][1] for element in fastResults.items()]\n",
    "rFast = [element[1][2] for element in fastResults.items()]\n",
    "\n",
    "print('Mean RMSE: ', np.mean(rmseFast) , '+-', np.std(rmseFast))\n",
    "print('Mean rRMSE: ', np.mean(rrmseFast) , '+-', np.std(rrmseFast))\n",
    "print('Mean r: ', np.mean(rFast) , '+-', np.std(rFast))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83c74610afe6bee9fbcc55841ea020c0a6c60e56f7a0fcf5ee8c50098a1a8d79"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
